<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java多线程并发]]></title>
    <url>%2F2019%2F05%2F06%2F2019-05-06-concurrency-and-multithreading%2F</url>
    <content type="text"><![CDATA[Java并发知识库 Java线程实现/创建方式继承Thread类Thread 类本质上是实现了Runnable 接口的一个实例，代表一个线程的实例。启动线程的唯一方法就是通过Thread 类的start()实例方法。start()方法是一个native 方法，它将启动一个新线程，并执行run()方法。1234567891011public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println("MyThread.run()"); &#125; public static void main(String[] args) &#123; MyThread myThread = new MyThread(); myThread.start(); &#125;&#125; 实现Runnable接口如果自己的类已经extends 另一个类，就无法直接extends Thread，此时，可以实现一个Runnable 接口。 Runnable接口对比Callable接口：Runnable不能抛出异常，不能带返回值。12345678910111213public class MyThread extends OtherClass implements Runnable &#123; @Override public void run() &#123; System.out.println("MyThread.run()"); &#125; public static void main(String[] args) &#123; //启动MyThread，需要首先实例化一个Thread，并传入自己的MyThread 实例： MyThread myThread = new MyThread(); Thread thread = new Thread(myThread); thread.start(); &#125;&#125; ExecutorService、Callable、Future有返回值线程有返回值的任务必须实现Callable 接口，类似的，无返回值的任务必须实现Runnable 接口。执行Callable 任务后，可以获取一个Future 的对象，在该对象上调用get 就可以获取到Callable 任务返回的Object 了，再结合线程池接口ExecutorService 就可以实现传说中有返回结果的多线程了。1234567891011121314151617//创建一个线程池ExecutorService pool = Executors.new FixedThreadPool(taskSize);// 创建多个有返回值的任务List&lt;Future&gt; list = new ArrayList&lt;Future&gt;();for (int i = 0; i &lt; taskSize; i++) &#123; Callable c = new MyCallable(i + " "); // 执行任务并获取Future 对象 Future f = pool.submit(c); list.add(f);&#125;// 关闭线程池pool.shutdown();// 获取所有并发任务的运行结果for (Future f : list) &#123; // 从Future 对象上获取任务的返回值，并输出到控制台 System.out.println("res：" + f.get().toString());&#125; 基于线程池的方式线程和数据库连接这些资源都是非常宝贵的资源。那么每次需要的时候创建，不需要的时候销毁，是非常浪费资源的。那么我们就可以使用缓存的策略，也就是使用线程池。 1234567891011121314151617181920212223242526272829303132333435363738public class ThreadPool &#123; /** * 线程池的基本大小 */ private static int corePoolSize = 10; /** * 线程池最大数量 */ private static int maximumPoolSizeSize = 100; /** * 线程活动保持时间 */ private static long keepAliveTime = 1; /** * 任务队列 */ private static ArrayBlockingQueue workQueue = new ArrayBlockingQueue(10); /** * 线程池拒绝策略 */ private static RejectedExecutionHandler handler = new ThreadPoolExecutor.AbortPolicy(); public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor( corePoolSize, maximumPoolSizeSize, keepAliveTime, TimeUnit.SECONDS, workQueue, // Google guava new ThreadFactoryBuilder().setNameFormat("XX-task-%d").build(), // 线程池拒绝策略 handler); //提交一个任务 executor.execute(() -&gt; System.out.println("ok")); &#125;&#125; 序号 名称 类型 含义 1 corePoolSize int 核心线程池大小 2 maximumPoolSize int 最大线程池大小 3 keepAliveTime long 线程最大空闲时间 4 unit TimeUnit 时间单位 5 workQueue BlockingQueue 线程等待队列 6 threadFactory ThreadFactory 线程创建工厂 7 handler RejectedExecutionHandler 拒绝策略 4种线程池Java 里面线程池的顶级接口是Executor，但是严格意义上讲Executor 并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。 newCachedThreadPool创建一个可根据需要创建新线程的线程池，但是在以前构造的线程可用时将重用它们。对于执行很多短期异步任务的程序而言，这些线程池通常可提高程序性能。调用 execute 将重用以前构造的线程（如果线程可用）。如果现有线程没有可用的，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程。因此，长时间保持空闲的线程池不会使用任何资源。 newFixedThreadPool创建一个可重用固定线程数的线程池，以共享的无界队列方式(可能会导致OOM)来运行这些线程。在任意点，在大多数 nThreads 线程会处于处理任务的活动状态。如果在所有线程处于活动状态时提交附加任务，则在有可用线程之前，附加任务将在队列中等待。如果在关闭前的执行期间由于失败而导致任何线程终止，那么一个新线程将代替它执行后续的任务（如果需要）。在某个线程被显式地关闭之前，池中的线程将一直存在。 newScheduledThreadPool创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。123456789101112131415161718192021public class MyThreadPool &#123; private static int initialDelay = 1; private static int period = 3; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(3); scheduledThreadPool.schedule(new Runnable() &#123; @Override public void run() &#123; System.out.println("延迟三秒"); &#125; &#125;, 3, TimeUnit.SECONDS); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; System.out.println("延迟1 秒后每三秒执行一次"); &#125; &#125;, initialDelay, period, TimeUnit.SECONDS); &#125;&#125; newSingleThreadExecutorExecutors.newSingleThreadExecutor()返回一个线程池（这个线程池只有一个线程）,这个线程池可以在线程死后（或发生异常时）重新启动一个线程来替代原来的线程继续执行下去。 线程生命周期(状态)当线程被创建并启动以后，它既不是一启动就进入了执行状态，也不是一直处于执行状态。在线程的生命周期中，它要经过新建(New)、就绪（Runnable）、运行（Running）、阻塞(Blocked)和死亡(Dead)5 种状态。尤其是当线程启动以后，它不可能一直”霸占”着CPU 独自运行，所以CPU 需要在多条线程之间切换，于是线程状态也会多次在运行、阻塞之间切换。 新建状态(new)当程序使用new 关键字创建了一个线程之后，该线程就处于新建状态，此时仅由JVM 为其分配内存，并初始化其成员变量的值 就绪状态(runnable)当线程对象调用了start()方法之后，该线程处于就绪状态。Java 虚拟机会为其创建方法调用栈和程序计数器，等待调度运行。 运行状态(running)如果处于就绪状态的线程获得了CPU，开始执行run()方法的线程执行体，则该线程处于运行状态。 阻塞状态(blocked)阻塞状态是指线程因为某种原因放弃了cpu 使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得cpu timeslice 转到运行(running)状态。阻塞的情况分三种：等待阻塞(o.wait-&gt;等待队列)运行(running)的线程执行o.wait()方法，JVM 会把该线程放入等待队列(waitting queue)中。同步阻塞(lock-&gt;锁池)运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM 会把该线程放入锁池(lock pool)中。其他阻塞(sleep/join)运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O 请求时，JVM 会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。 线程死亡(dead)线程会以下面三种方式结束，结束后就是死亡状态。正常结束1.run()或call()方法执行完成，线程正常结束。异常结束2.线程抛出一个未捕获的Exception 或Error。调用stop3.直接调用该线程的stop()方法来结束该线程—该方法通常容易导致死锁，不推荐使用。 终止线程的4种方式正常运行结束程序运行结束，线程自动结束。 使用退出标志退出线程一般run()方法执行完，线程就会正常结束，然而，常常有些线程是伺服线程。它们需要长时间的运行，只有在外部某些条件满足的情况下，才能关闭这些线程。使用一个变量来控制循环，例如：最直接的方法就是设一个boolean 类型的标志，并通过设置这个标志为true 或false 来控制while循环是否退出，代码示例：123456789public class ThreadSafe extends Thread &#123; public volatile boolean exit = false; @Override public void run() &#123; while(!exit) &#123; // do something &#125; &#125;&#125; 定义了一个退出标志exit，当exit 为true 时，while 循环退出，exit 的默认值为false.在定义exit时，使用了一个Java 关键字volatile，这个关键字的目的是使exit 同步，也就是说在同一时刻只能由一个线程来修改exit 的值。 Interrupt方法结束线程使用interrupt()方法来中断线程有两种情况：1. 线程处于阻塞状态如使用了sleep,同步锁的wait,socket 中的receiver,accept 等方法时，会使线程处于阻塞状态。当调用线程的interrupt()方法时，会抛出InterruptException 异常。阻塞中的那个方法抛出这个异常，通过代码捕获该异常，然后break 跳出循环状态，从而让我们有机会结束这个线程的执行。通常很多人认为只要调用interrupt 方法线程就会结束，实际上是错的， 一定要先捕获InterruptedException 异常之后通过break 来跳出循环，才能正常结束run 方法。2. 线程未处于阻塞状态使用isInterrupted()判断线程的中断标志来退出循环。当使用interrupt()方法时，中断标志就会置true，和使用自定义的标志来控制循环是一样的道理。12345678910111213141516public class ThreadSafe extends Thread &#123; @Override public void run() &#123; // 非阻塞过程中通过判断中断标志来退出 while (!isInterrupted()) &#123; try &#123; // 阻塞过程捕获中断异常来退出 Thread.sleep(5 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); // 捕获到异常之后，执行break 跳出循环 break; &#125; &#125; &#125;&#125; stop方法终止线程(线程不安全）程序中可以直接使用thread.stop()来强行终止线程，但是stop 方法是很危险的，就像突然关闭计算机电源，而不是按正常程序关机一样，可能会产生不可预料的结果，不安全主要是：thread.stop()调用之后，创建子线程的线程就会抛出ThreadDeatherror 的错误，并且会释放子线程所持有的所有锁。一般任何进行加锁的代码块，都是为了保护数据的一致性，如果在调用thread.stop()后导致了该线程所持有的所有锁的突然释放(不可控制)，那么被保护数据就有可能呈现不一致性，其他线程在使用这些被破坏的数据时，有可能导致一些很奇怪的应用程序错误。因此，并不推荐使用stop 方法来终止线程。 sleep与wait区别 对于sleep()方法，我们首先要知道该方法是属于Thread 类中的。而wait()方法，则是属于Object 类中的。 sleep()方法导致了程序暂停执行指定的时间，让出cpu给其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态。 在调用sleep()方法的过程中，线程不会释放对象锁。 而当调用wait()方法的时候，线程会放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象调用notify()方法后本线程才进入对象锁定池准备获取对象锁进入运行状态。 start与run区别 start（）方法来启动线程，真正实现了多线程运行。这时无需等待run 方法体代码执行完毕，可以直接继续执行下面的代码。 通过调用Thread 类的start()方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行，需要有可用的CPU资源才运行。 方法run()称为线程体，它包含了要执行的这个线程的内容，线程就进入了运行状态，开始运行run 函数当中的代码。 Run 方法运行结束， 此线程终止。然后CPU 再调度其它线程。 Java后台线程 定义：守护线程–也称“服务线程”，他是后台线程，它有一个特性，即为用户线程提供公共服务，在没有用户线程可服务时会自动离开。 优先级：守护线程的优先级比较低，用于为系统中的其它对象和线程提供服务。 设置：通过setDaemon(true)来设置线程为“守护线程”；将一个用户线程设置为守护线程的方式是在 线程对象创建 之前 用线程对象的setDaemon 方法。 在Daemon 线程中产生的新线程也是Daemon 的。 线程则是JVM 级别的，以Tomcat 为例，如果你在Web 应用中启动一个线程，这个线程的生命周期并不会和Web 应用程序保持同步。也就是说，即使你停止了Web 应用，这个线程依旧是活跃的。 example: 垃圾回收线程就是一个经典的守护线程，当我们的程序中不再有任何运行的Thread,程序就不会再产生垃圾，垃圾回收器也就无事可做，所以当垃圾回收线程是JVM 上仅剩的线程时，垃圾回收线程会自动离开。它始终在低级别的状态中运行，用于实时监控和管理系统中的可回收资源。 生命周期：守护进程（Daemon）是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。也就是说守护线程不依赖于终端，但是依赖于系统，与系统“同生共死”。当JVM 中所有的线程都是守护线程的时候，JVM 就可以退出了；如果还有一个或以上的非守护线程则JVM 不会退出。 Java锁乐观锁乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作（比较跟上一次的版本号，如果一样则更新），如果失败则要重复读-比较-写的操作。java 中的乐观锁基本都是通过CAS 操作实现的，CAS 是一种更新的原子操作，比较当前值跟传入值是否一样，一样则更新，否则失败。 悲观锁悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会block 直到拿到锁。java 中的悲观锁就是Synchronized,AQS 框架下的锁则是先尝试cas 乐观锁去获取锁，获取不到，才会转换为悲观锁，如RetreenLock。 自旋锁自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。线程自旋是需要消耗cup 的，说白了就是让cup 在做无用功，如果一直获取不到锁，那线程也不能一直占用cup 自旋做无用功，所以需要设定一个自旋等待的最大时间。如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。自旋锁的优缺点自旋锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗，这些操作会导致线程发生两次上下文切换！但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适合使用自旋锁了，因为自旋锁在获取锁前一直都是占用cpu 做无用功，占着XX 不XX，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要cup 的线程又不能获取到cpu，造成cpu 的浪费。所以这种情况下我们要关闭自旋锁；自旋锁时间阀值(1.6引入量适应性自旋锁)自旋锁的目的是为了占着CPU 的资源不释放，等到获取到锁立即进行处理。但是如何去选择自旋的执行时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用CPU 资源，进而会影响整体系统的性能。因此自旋的周期选的额外重要！JVM 对于自旋周期的选择，jdk1.5 这个限度是一定的写死的，在1.6 引入了适应性自旋锁，适应性自旋锁意味着自旋的时间不在是固定的了，而是由前一次在同一个锁上的自旋时间以及锁的拥有者的状态来决定，基本认为一个线程上下文切换的时间是最佳的一个时间，同时JVM 还针对当前CPU 的负荷情况做了较多的优化，如果平均负载小于CPUs 则一直自旋，如果有超过(CPUs/2)个线程正在自旋，则后来线程直接阻塞，如果正在自旋的线程发现Owner 发生了变化则延迟自旋时间（自旋计数）或进入阻塞，如果CPU 处于节电模式则停止自旋，自旋时间的最坏情况是CPU的存储延迟（CPU A 存储了一个数据，到CPU B 得知这个数据直接的时间差），自旋时会适当放弃线程优先级之间的差异。自旋锁的开启JDK1.6 中-XX:+UseSpinning 开启；-XX:PreBlockSpin=10 为自旋次数；JDK1.7 后，去掉此参数，由jvm 控制； synchronized同步锁synchronized 它可以把任意一个非NULL 的对象当作锁。他属于独占式的悲观锁、非公平锁，同时属于可重入锁。Synchronized 作用范围 作用于方法时，锁住的是当前对象的实例(this)； 当作用于静态方法时，锁住的是Class 实例，又因为Class 的相关数据存储在永久代PermGen（jdk1.8 则是metaspace），永久代是全局共享的，因此静态方法锁相当于类的一个全局锁，会锁所有调用该方法的线程； synchronized 作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中。 Synchronized 核心组件 Wait Set：哪些调用wait 方法被阻塞的线程被放置在这里； Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：Contention List 中那些有资格成为候选资源的线程被移动到Entry List 中； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到锁资源的线程被称为Owner； !Owner：当前释放锁的线程。 Synchronized 实现 JVM 每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList 会被大量的并发线程进行CAS 访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList 中作为候选竞争线程。 Owner 线程会在unlock 时，将ContentionList 中的部分线程迁移到EntryList 中，并指定EntryList 中的某个线程为OnDeck 线程（一般是最先进去的那个线程）。 Owner 线程并不直接把锁传递给OnDeck 线程，而是把锁竞争的权利交给OnDeck，OnDeck 需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM 中，也把这种选择行为称之为“竞争切换”。 OnDeck 线程获取到锁资源后会变为Owner 线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner 线程被wait 方法阻塞，则转移到WaitSet 队列中，直到某个时刻通过notify或者notifyAll 唤醒，会重新进去EntryList 中。 处于ContentionList、EntryList、WaitSet 中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux 内核下采用pthread_mutex_lock 内核函数实现的）。 Synchronized 是非公平锁。 Synchronized 在线程进入ContentionList 时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占OnDeck 线程的锁资源。 每个对象都有个monitor 对象，加锁就是在竞争monitor 对象，代码块加锁是在前后分别加上monitorenter 和monitorexit 指令来实现的，方法加锁是通过一个标记位来判断的 synchronized 是一个重量级操作，需要调用操作系统相关接口，性能是低效的，有可能给线程加锁消耗的时间比有用操作消耗的时间更多。 Java1.6，synchronized 进行了很多的优化，有适应自旋、锁消除、锁粗化、轻量级锁及偏向锁等，效率有了本质上的提高。在之后推出的Java1.7 与1.8 中，均对该关键字的实现机理做了优化。引入了偏向锁和轻量级锁。都是在对象头中有标记位，不需要经过操作系统加锁。 锁可以从偏向锁升级到轻量级锁，再升级到重量级锁。这种升级过程叫做锁膨胀； JDK 1.6 中默认是开启偏向锁和轻量级锁，可以通过-XX:-UseBiasedLocking 来禁用偏向锁。 ReentrantLockReentantLock 继承接口Lock 并实现了接口中定义的方法，他是一种可重入锁，除了能完成synchronized 所能完成的所有工作外，还提供了诸如可响应中断锁、可轮询锁请求、定时锁等避免多线程死锁的方法。Lock 接口的主要方法 void lock(): 执行此方法时, 如果锁处于空闲状态, 当前线程将获取到锁. 相反, 如果锁已经被其他线程持有, 将禁用当前线程, 直到当前线程获取到锁. boolean tryLock()：如果锁可用, 则获取锁, 并立即返回true, 否则返回false. 该方法和lock()的区别在于, tryLock()只是”试图”获取锁, 如果锁不可用, 不会导致当前线程被禁用,当前线程仍然继续往下执行代码. 而lock()方法则是一定要获取到锁, 如果锁不可用, 就一直等待, 在未获得锁之前,当前线程并不继续向下执行. void unlock()：执行此方法时, 当前线程将释放持有的锁. 锁只能由持有者释放, 如果线程并不持有锁, 却执行该方法, 可能导致异常的发生. Condition newCondition()：条件对象，获取等待通知组件。该组件和当前的锁绑定，当前线程只有获取了锁，才能调用该组件的await()方法，而调用后，当前线程将释放锁。 getHoldCount() ：查询当前线程保持此锁的次数，也就是执行此线程执行lock 方法的次数。 getQueueLength（）：返回正等待获取此锁的线程估计数，比如启动10 个线程，1 个线程获得锁，此时返回的是9 getWaitQueueLength：（Condition condition）返回等待与此锁相关的给定条件的线程计数。比如10 个线程，用同一个condition 对象，并且此时这10 个线程都执行了condition 对的await 方法，那么此时执行此方法返回10 hasWaiters(Condition condition) ： 查询是否有线程等待与此锁有关的给定条件(condition)，对于指定contidion 对象，有多少线程执行了condition.await 方法 hasQueuedThread(Thread thread)：查询给定线程是否等待获取此锁 hasQueuedThreads()：是否有线程等待此锁 isFair()：该锁是否公平锁 isHeldByCurrentThread()： 当前线程是否保持锁锁定，线程的执行lock 方法的前后分别是false 和true isLock()：此锁是否有任意线程占用 lockInterruptibly（）：如果当前线程未被中断，获取锁 tryLock（）：尝试获得锁，仅在调用时锁未被线程占用，获得锁 tryLock(long timeout TimeUnit unit)：如果锁在给定等待时间内没有被另一个线程保持，则获取该锁。 非公平锁JVM 按随机、就近原则分配锁的机制则称为不公平锁，ReentrantLock 在构造函数中提供了是否公平锁的初始化方式，默认为非公平锁。非公平锁实际执行的效率要远远超出公平锁，除非程序有特殊需要，否则最常用非公平锁的分配机制。公平锁公平锁指的是锁的分配机制是公平的，通常先对锁提出获取请求的线程会先被分配到锁，ReentrantLock 在构造函数中提供了是否公平锁的初始化方式来定义公平锁。ReentrantLock 与 synchronized ReentrantLock 通过方法lock()与unlock()来进行加锁与解锁操作，与synchronized 会被JVM 自动解锁机制不同，ReentrantLock 加锁后需要手动进行解锁。为了避免程序出现异常而无法正常解锁的情况，使用ReentrantLock 必须在finally 控制块中进行解锁操作。 ReentrantLock 相比synchronized 的优势是可中断、公平锁、多个锁。这种情况下需要使用ReentrantLock。 ReentrantLock1234567891011121314151617181920212223242526public class MyService &#123; // Lock lock=new ReentrantLock(true);//公平锁 private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void testMethod() &#123; try &#123; // lock 加锁 lock.lock(); // 1：wait 方法等待： //通过创建Condition 对象来使线程wait，必须先执行lock.lock 方法获得锁 condition.await(); // 2：condition 对象的signal 方法可以唤醒wait 线程 condition.signal(); for (int i = 0; i &lt; 5; i++) &#123; System.out.println("ThreadName=" + Thread.currentThread().getName() + (" " + (i + 1))); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; Condition 类和 Object 类锁方法区别 Condition 类的awiat 方法和Object 类的wait 方法等效 Condition 类的signal 方法和Object 类的notify 方法等效 Condition 类的signalAll 方法和Object 类的notifyAll 方法等效 ReentrantLock 类可以唤醒指定条件的线程，而object 的唤醒是随机的 tryLock 和 lock 和 lockInterruptibly 的区别 tryLock 能获得锁就返回true，不能就立即返回false，tryLock(long timeout,TimeUnitunit)，可以增加时间限制，如果超过该时间段还没获得锁，返回false lock 能获得锁就返回true，不能的话一直等待获得锁 lock 和lockInterruptibly，如果两个线程分别执行这两个方法，但此时中断这两个线程，lock 不会抛出异常，而lockInterruptibly 会抛出异常。 Semaphore信号量Semaphore 是一种基于计数的信号量。它可以设定一个阈值，基于此，多个线程竞争获取许可信号，做完自己的申请后归还，超过阈值后，线程申请许可信号将会被阻塞。Semaphore 可以用来构建一些对象池，资源池之类的，比如数据库连接池实现互斥锁(计数器为1)我们也可以创建计数为1 的Semaphore，将其作为一种类似互斥锁的机制，这也叫二元信号量，表示两种互斥状态。它的用法如下：12345678910111213141516public void testMethod() &#123; // 创建一个计数阈值为5 的信号量对象 // 只能5 个线程同时访问 Semaphore semp = new Semaphore(5); try &#123; // 申请许可 semp.acquire(); try &#123; // 业务逻辑 &#125; catch (Exception e) &#123; &#125; finally &#123; // 释放许可 semp.release(); &#125; &#125; catch (InterruptedException e) &#123; &#125;&#125; Semaphore 与 ReentrantLockSemaphore 基本能完成ReentrantLock 的所有工作，使用方法也与之类似，通过acquire()与release()方法来获得和释放临界资源。经实测，Semaphone.acquire()方法默认为可响应中断锁，与ReentrantLock.lockInterruptibly()作用效果一致，也就是说在等待临界资源的过程中可以被Thread.interrupt()方法中断。此外，Semaphore 也实现了可轮询的锁请求与定时锁的功能，除了方法名tryAcquire 与tryLock不同，其使用方法与ReentrantLock 几乎一致。Semaphore 也提供了公平与非公平锁的机制，也可在构造函数中进行设定。Semaphore 的锁释放操作也由手动进行，因此与ReentrantLock 一样，为避免线程因抛出异常而无法正常释放锁的情况发生，释放锁的操作也必须在finally 代码块中完成。 AtomicInteger首先说明， 此处AtomicInteger ， 一个提供原子操作的Integer 的类， 常见的还有AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference 等，他们的实现原理相同，区别在与运算对象类型的不同。令人兴奋地，还可以通过AtomicReference将一个对象的所有操作转化成原子操作。我们知道，在多线程程序中，诸如++i 或 i++等运算不具有原子性，是不安全的线程操作之一。通常我们会使用synchronized 将该操作变成一个原子操作，但JVM 为此类操作特意提供了一些同步类，使得使用更方便，且使程序运行效率变得更高。通过相关资料显示，通常AtomicInteger的性能是ReentantLock 的好几倍。 可重入锁(递归锁)本文里面讲的是广义上的可重入锁，而不是单指JAVA 下的ReentrantLock。可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。在JAVA 环境下 ReentrantLock 和synchronized 都是 可重入锁。 公平锁与非公平锁公平锁(Fair)加锁前检查是否有排队等待的线程，优先排队等待的线程，先来先得非公平锁(Nonfair)加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待 非公平锁性能比公平锁高5~10 倍，因为公平锁需要在多核的情况下维护一个队列 Java 中的synchronized 是非公平锁，ReentrantLock 默认的lock()方法采用的是非公平锁。 ReadWriteLock读写锁为了提高性能，Java 提供了读写锁，在读的地方使用读锁，在写的地方使用写锁，灵活控制，如果没有写锁的情况下，读是无阻塞的,在一定程度上提高了程序的执行效率。读写锁分为读锁和写锁，多个读锁不互斥，读锁与写锁互斥，这是由jvm 自己控制的，你只要上好相应的锁即可。读锁如果你的代码只读数据，可以很多人同时读，但不能同时写，那就上读锁写锁如果你的代码修改数据，只能有一个人在写，且不能同时读取，那就上写锁。总之，读的时候上读锁，写的时候上写锁！Java 中读写锁有个接口java.util.concurrent.locks.ReadWriteLock ， 也有具体的实现ReentrantReadWriteLock。 共享锁和独占锁java 并发包提供的加锁模式分为独占锁和共享锁。独占锁独占锁模式下，每次只能有一个线程能持有锁，ReentrantLock 就是以独占方式实现的互斥锁。独占锁是一种悲观保守的加锁策略，它避免了读/读冲突，如果某个只读线程获取锁，则其他读线程都只能等待，这种情况下就限制了不必要的并发性，因为读操作并不会影响数据的一致性。共享锁共享锁则允许多个线程同时获取锁，并发访问 共享资源，如：ReadWriteLock。共享锁则是一种乐观锁，它放宽了加锁策略，允许多个执行读操作的线程同时访问共享资源。 AQS 的内部类Node 定义了两个常量SHARED 和EXCLUSIVE，他们分别标识 AQS 队列中等待线程的锁获取模式。 java 的并发包中提供了ReadWriteLock，读-写锁。它允许一个资源可以被多个读操作访问，或者被一个 写操作访问，但两者不能同时进行。 重量级锁(Mutex Lock)Synchronized 是通过对象内部的一个叫做监视器锁（monitor）来实现的。但是监视器锁本质又是依赖于底层的操作系统的Mutex Lock 来实现的。而操作系统实现线程之间的切换这就需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized 效率低的原因。因此，这种依赖于操作系统Mutex Lock 所实现的锁我们称之为“重量级锁”。JDK 中对Synchronized 做的种种优化，其核心都是为了减少这种重量级锁的使用。JDK1.6 以后，为了减少获得锁和释放锁所带来的性能消耗，提高性能，引入了“轻量级锁”和“偏向锁”。 轻量级锁锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。锁升级随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的。但是，首先需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。 偏向锁Hotspot 的作者经过以往的研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得。偏向锁的目的是在某个线程获得锁之后，消除这个线程锁重入（CAS）的开销，看起来让这个线程得到了偏护。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS 原子指令，而偏向锁只需要在置换ThreadID 的时候依赖一次CAS 原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的CAS 原子指令的性能消耗）。上面说过，轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。 分段锁分段锁也并非一种实际的锁，而是一种思想ConcurrentHashMap 是学习分段锁的最好实践 锁优化减少持有时间只用在有线程安全要求的程序上加锁减小粒度将大对象（这个对象可能会被很多线程访问），拆成小对象，大大增加并行度，降低锁竞争。降低了锁的竞争，偏向锁，轻量级锁成功率才会提高。最最典型的减小锁粒度的案例就是ConcurrentHashMap。锁分离最常见的锁分离就是读写锁ReadWriteLock，根据功能进行分离成读锁和写锁，这样读读不互斥，读写互斥，写写互斥，即保证了线程安全，又提高了性能，具体也请查看[高并发Java 五]JDK 并发包1。读写分离思想可以延伸，只要操作互不影响，锁就可以分离。比如LinkedBlockingQueue 从头部取出，从尾部放数据锁粗化通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽量短，即在使用完公共资源后，应该立即释放锁。但是，凡事都有一个度，如果对同一个锁不停的进行请求、同步和释放，其本身也会消耗系统宝贵的资源，反而不利于性能的优化 。锁消除锁消除是在编译器级别的事情。在即时编译器时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作，多数是因为程序员编码不规范引起。 线程基本方法线程相关的基本方法有wait，notify，notifyAll，sleep，join，yield 等。 线程等待(wait)调用该方法的线程进入WAITING 状态，只有等待另外线程的通知或被中断才会返回，需要注意的是调用wait()方法后，会释放对象的锁。因此，wait 方法一般用在同步方法或同步代码块中。 线程睡眠(sleep)sleep 导致当前线程休眠，与wait 方法不同的是sleep 不会释放当前占有的锁,sleep(long)会导致线程进入TIMED-WATING 状态，而wait()方法会导致当前线程进入WATING 状态 线程让步(yield)yield 会使当前线程让出CPU 执行时间片，与其他线程一起重新竞争CPU 时间片。一般情况下，优先级高的线程有更大的可能性成功竞争得到CPU 时间片，但这又不是绝对的，有的操作系统对线程优先级并不敏感。 线程中断(interrupt)中断一个线程，其本意是给这个线程一个通知信号，会影响这个线程内部的一个中断标识位。这个线程本身并不会因此而改变状态(如阻塞，终止等)。 调用interrupt()方法并不会中断一个正在运行的线程。也就是说处于Running 状态的线程并不会因为被中断而被终止，仅仅改变了内部维护的中断标识位而已。 若调用sleep()而使线程处于TIMED-WATING 状态，这时调用interrupt()方法，会抛出InterruptedException,从而使线程提前结束TIMED-WATING 状态。 许多声明抛出InterruptedException 的方法(如Thread.sleep(long mills 方法))，抛出异常前，都会清除中断标识位，所以抛出异常后，调用isInterrupted()方法将会返回false。 中断状态是线程固有的一个标识位，可以通过此标识位安全的终止线程。比如,你想终止一个线程thread 的时候，可以调用thread.interrupt()方法，在线程的run 方法内部可以根据thread.isInterrupted()的值来优雅的终止线程。 join等待其他线程终止join() 方法，等待其他线程终止，在当前线程中调用一个线程的 join() 方法，则当前线程转为阻塞状态，回到另一个线程结束，当前线程再由阻塞状态变为就绪状态，等待 cpu 的宠幸。 为什么要用join()方法很多情况下，主线程生成并启动了子线程，需要用到子线程返回的结果，也就是需要主线程需要在子线程结束后再结束，这时候就要用到 join() 方法。12345System.out.println(Thread.currentThread().getName() + "线程运行开始!");Thread thread1 = new Thread();thread1.setName("线程B");thread1.join();System.out.println("这时thread1 执行完毕之后才能执行主线程"); 线程唤醒(notify)Object 类中的 notify() 方法，唤醒在此对象监视器上等待的单个线程，如果所有线程都在此对象上等待，则会选择唤醒其中一个线程，选择是任意的，并在对实现做出决定时发生，线程通过调用其中一个 wait() 方法，在对象的监视器上等待，直到当前的线程放弃此对象上的锁定，才能继续执行被唤醒的线程，被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。类似的方法还有 notifyAll() ，唤醒再次监视器上等待的所有线程。 其他方法 sleep()：强迫一个线程睡眠Ｎ毫秒。 isAlive()： 判断一个线程是否存活。 join()： 等待线程终止。 activeCount()： 程序中活跃的线程数。 enumerate()： 枚举程序中的线程。 currentThread()： 得到当前线程。 isDaemon()： 一个线程是否为守护线程。 setDaemon()： 设置一个线程为守护线程。(用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束) setName()： 为线程设置一个名称。 wait()： 强迫一个线程等待。 notify()： 通知一个线程继续运行。 setPriority()： 设置一个线程的优先级。 getPriority():：获得一个线程的优先级。 线程上下文切换巧妙地利用了时间片轮转的方式, CPU 给每个任务都服务一定的时间，然后把当前任务的状态保存下来，在加载下一任务的状态后，继续服务下一任务，任务的状态保存及再加载, 这段过程就叫做上下文切换。时间片轮转的方式使多个任务在同一颗CPU 上执行变成了可能。 进程（有时候也称做任务）是指一个程序运行的实例。在Linux 系统中，线程就是能并行运行并且与他们的父进程（创建他们的进程）共享同一地址空间（一段内存区域）和其他资源的轻量级的进程。 上下文是指某一时间点 CPU 寄存器和程序计数器的内容。 寄存器是 CPU 内部的数量较少但是速度很快的内存（与之对应的是 CPU 外部相对较慢的 RAM 主内存）。寄存器通过对常用值（通常是运算的中间值）的快速访问来提高计算机程序运行的速度。 程序计数器是一个专用的寄存器，用于表明指令序列中 CPU 正在执行的位置，存的值为正在执行的指令的位置或者下一个将要被执行的指令的位置，具体依赖于特定的系统。 PCB-“切换帧”上下文切换可以认为是内核（操作系统的核心）在 CPU 上对于进程（包括线程）进行切换，上下文切换过程中的信息是保存在进程控制块（PCB, process control block）中的。PCB 还经常被称作“切换桢”（switchframe）。信息会一直保存到CPU 的内存中，直到他们被再次使用。 上下文切换的活动 挂起一个进程，将这个进程在 CPU 中的状态（上下文）存储于内存中的某处。 在内存中检索下一个进程的上下文并将其在 CPU 的寄存器中恢复。 跳转到程序计数器所指向的位置（即跳转到进程被中断时的代码行），以恢复该进程在程序中。 引起线程上下文切换的原因 当前执行任务的时间片用完之后，系统CPU 正常调度下一个任务； 当前执行任务碰到IO 阻塞，调度器将此任务挂起，继续下一任务； 多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一任务； 用户代码挂起当前任务，让出CPU 时间； 硬件中断； 同步锁与死锁同步锁当多个线程同时访问同一个数据时，很容易出现问题。为了避免这种情况出现，我们要保证线程同步互斥，就是指并发执行的多个线程，在同一时间内只允许一个线程访问共享数据。 Java 中可以使用synchronized 关键字来取得一个对象的同步锁。 死锁何为死锁，就是多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author Shado * @date 2017-06-03 14:21 */public class DeadLock implements Runnable &#123; private int flag = 0; private static Object o1 = new Object(); private static Object o2 = new Object(); @Override public void run() &#123; if (flag == 1) &#123; method1(); &#125; else if (flag == 2) &#123; method2(); &#125; &#125; private void method1() &#123; synchronized (o1) &#123; System.out.println(Thread.currentThread().getName() + "\t" + "method1" + "\t lock o1"); synchronized (o2) &#123; System.out.println(Thread.currentThread().getName() + "\t" + "method1" + "\t lock o2"); &#125; &#125; &#125; private void method2() &#123; synchronized (o2) &#123; System.out.println(Thread.currentThread().getName() + "\t" + "method2" + "\t lock o2"); synchronized (o1) &#123; System.out.println(Thread.currentThread().getName() + "\t" + "method2" + "\t lock o1"); &#125; &#125; &#125; public static void main(String[] args) &#123; DeadLock deadLock1 = new DeadLock(); DeadLock deadLock2 = new DeadLock(); deadLock1.flag = 1; deadLock2.flag = 2; Thread thread1 = new Thread(deadLock1,"thread1"); Thread thread2 = new Thread(deadLock2, "thread2"); thread1.start(); thread2.start(); &#125;&#125; 执行结果，程序不终止，输出内容如下:12thread1 method1 lock o1thread2 method2 lock o2 线程池原理线程池做的工作主要是控制运行的线程的数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量超出数量的线程排队等候，等其它线程执行完毕，再从队列中取出任务来执行。他的主要特点为：线程复用；控制最大并发数；管理线程。 线程复用每一个 Thread 的类都有一个 start 方法。 当调用start 启动线程时Java 虚拟机会调用该类的 run方法。 那么该类的 run() 方法中就是调用了 Runnable 对象的 run() 方法。 我们可以继承重写Thread 类，在其 start 方法中添加不断循环调用传递过来的 Runnable 对象。 这就是线程池的实现原理。循环方法中不断获取 Runnable 是用 Queue 实现的，在获取下一个 Runnable 之前可以是阻塞的。 线程池的组成一般的线程池主要分为以下4 个组成部分： 线程池管理器：用于创建并管理线程池 工作线程：线程池中的线程 任务接口：每个任务必须实现的接口，用于工作线程调度其运行 任务队列：用于存放待处理的任务，提供一种缓冲机制 Java 中的线程池是通过Executor 框架实现的，该框架中用到了Executor，Executors，ExecutorService，ThreadPoolExecutor ，Callable 和Future、FutureTask 这几个类。 ThreadPoolExecutor的构造方法如下:12345678910111213141516171819202122232425262728293031323334/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters, the default thread factory and the default rejected * execution handler. * * &lt;p&gt;It may be more convenient to use one of the &#123;@link Executors&#125; * factory methods instead of this general purpose constructor. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125; corePoolSize：指定了线程池中的线程数量。 maximumPoolSize：指定了线程池中的最大线程数量。 keepAliveTime：当前线程池数量超过corePoolSize 时，多余的空闲线程的存活时间，即多次时间内会被销毁。 unit：keepAliveTime 的单位。 workQueue：任务队列，被提交但尚未被执行的任务。 threadFactory：线程工厂，用于创建线程，一般用默认的即可。 handler：拒绝策略，当任务太多来不及处理，如何拒绝任务。 拒绝策略线程池中的线程已经用完了，无法继续为新任务服务，同时，等待队列也已经排满了，再也塞不下新任务了。这时候我们就需要拒绝策略机制合理的处理这个问题。JDK 内置的拒绝策略如下： AbortPolicy ： 直接抛出异常，阻止系统正常运行。 CallerRunsPolicy ： 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。显然这样做不会真的丢弃任务，但是，任务提交线程的性能极有可能会急剧下降。 DiscardOldestPolicy ： 丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。 DiscardPolicy ： 该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，这是最好的一种方案。 以上内置拒绝策略均实现了RejectedExecutionHandler 接口，若以上策略仍无法满足实际需要，完全可以自己扩展RejectedExecutionHandler 接口。 Java 线程池工作过程 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。 当调用 execute() 方法添加一个任务时，线程池会做如下判断：a) 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务；b) 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列；c) 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务；d) 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常RejectExecutionException。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。 Java阻塞队列原理阻塞队列，关键字是阻塞，先理解阻塞的含义，在阻塞队列中，线程阻塞有这样的两种情况： 当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列。 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒。阻塞队列的主要方法 抛出异常：抛出一个异常； 特殊值：返回一个特殊值（null 或false,视情况而定） 则塞：在成功操作之前，一直阻塞线程 超时：放弃前只在最大的时间内阻塞 插入操作: public abstract boolean add(E paramE)：将指定元素插入此队列中（如果立即可行且不会违反容量限制），成功时返回 true，如果当前没有可用的空间，则抛出 IllegalStateException。如果该元素是NULL，则会抛出NullPointerException 异常。 public abstract boolean offer(E paramE)：将指定元素插入此队列中（如果立即可行且不会违反容量限制），成功时返回 true，如果当前没有可用的空间，则返回 false。 public abstract void put(E paramE) throws InterruptedException： 将指定元素插入此队列中，将等待可用的空间（如果有必要） offer(E o, long timeout, TimeUnit unit)：可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败。 12345678910111213141516171819/** * Inserts the specified element at the tail of this queue, waiting * for space to become available if the queue is full. * * @throws InterruptedException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; */public void put(E e) throws InterruptedException &#123; Objects.requireNonNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == items.length) notFull.await(); enqueue(e); &#125; finally &#123; lock.unlock(); &#125;&#125; 获取数据操作: poll(time):取走BlockingQueue 里排在首位的对象,若不能立即取出,则可以等time 参数规定的时间,取不到时返回null; poll(long timeout, TimeUnit unit)：从BlockingQueue 取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则直到时间超时还没有数据可取，返回失败。 take():取走BlockingQueue 里排在首位的对象,若BlockingQueue 为空,阻断进入等待状态直到BlockingQueue 有新的数据被加入。 drainTo():一次性从BlockingQueue 获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。 Java中的阻塞队列 ArrayBlockingQueue ：由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：支持优先级排序的无界阻塞队列。 DelayQueue：使用优先级队列实现的无界阻塞队列。 SynchronousQueue：不存储元素的阻塞队列。 LinkedTransferQueue：由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：由链表结构组成的双向阻塞队列。 ArrayBlockingQueue（公平、非公平）用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序。默认情况下不保证访问者公平的访问队列，所谓公平访问队列是指阻塞的所有生产者线程或消费者线程，当队列可用时，可以按照阻塞的先后顺序访问队列，即先阻塞的生产者线程，可以先往队列里插入元素，先阻塞的消费者线程，可以先从队列里获取元素。通常情况下为了保证公平性会降低吞吐量。我们可以使用以下代码创建一个公平的阻塞队列：1ArrayBlockingQueue fairQueue = new ArrayBlockingQueue(1000,true); LinkedBlockingQueue（两个独立锁提高并发）基于链表的阻塞队列，同ArrayListBlockingQueue 类似，此队列按照先进先出（FIFO）的原则对元素进行排序。而LinkedBlockingQueue 之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。LinkedBlockingQueue 会默认一个类似无限大小的容量（Integer.MAX_VALUE）。 PriorityBlockingQueue（compareTo 排序实现优先）是一个支持优先级的无界队列。默认情况下元素采取自然顺序升序排列。可以自定义实现compareTo()方法来指定元素进行排序规则，或者初始化PriorityBlockingQueue 时，指定构造参数Comparator 来对元素进行排序。需要注意的是不能保证同优先级元素的顺序。 DelayQueue（缓存失效、定时任务 ）是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue 来实现。队列中的元素必须实现Delayed 接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。我们可以将DelayQueue 运用在以下应用场景： 缓存系统的设计：可以用DelayQueue 保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue 中获取元素时，表示缓存有效期到了。 定时任务调度： 使用DelayQueue 保存当天将会执行的任务和执行时间， 一旦从DelayQueue 中获取到任务就开始执行，从比如TimerQueue 就是使用DelayQueue 实现的。 SynchronousQueue（不存储数据、可用于传递数据）是一个不存储元素的阻塞队列。每一个put 操作必须等待一个take 操作，否则不能继续添加元素。SynchronousQueue 可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合于传递性场景,比如在一个线程中使用的数据，传递给另外一个线程使用， SynchronousQueue 的吞吐量高于LinkedBlockingQueue 和ArrayBlockingQueue。 LinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue 队列。相对于其他阻塞队列，LinkedTransferQueue 多了tryTransfer 和transfer 方法。 transfer 方法：如果当前有消费者正在等待接收元素（消费者使用take()方法或带时间限制的poll()方法时），transfer 方法可以把生产者传入的元素立刻transfer（传输）给消费者。如果没有消费者在等待接收元素，transfer 方法会将元素存放在队列的tail 节点，并等到该元素被消费者消费了才返回。 tryTransfer 方法。则是用来试探下生产者传入的元素是否能直接传给消费者。如果没有消费者等待接收元素，则返回false。和transfer 方法的区别是tryTransfer 方法无论消费者是否接收，方法立即返回。而transfer 方法是必须等到消费者消费了才返回。 对于带有时间限制的tryTransfer(E e, long timeout, TimeUnit unit)方法，则是试图把生产者传入的元素直接传给消费者，但是如果没有消费者消费该元素则等待指定的时间再返回，如果超时还没消费元素，则返回false，如果在超时时间内消费了元素，则返回true。 LinkedBlockingDeque是一个由链表结构组成的双向阻塞队列。所谓双向队列指的你可以从队列的两端插入和移出元素。双端队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。相比其他的阻塞队列， LinkedBlockingDeque 多了addFirst ， addLast ， offerFirst ， offerLast ，peekFirst，peekLast 等方法，以First 单词结尾的方法，表示插入，获取（peek）或移除双端队列的第一个元素。以Last 单词结尾的方法，表示插入，获取或移除双端队列的最后一个元素。另外插入方法add 等同于addLast，移除方法remove 等效于removeFirst。但是take 方法却等同于takeFirst，不知道是不是Jdk 的bug，使用时还是用带有First 和Last 后缀的方法更清楚。在初始化LinkedBlockingDeque 时可以设置容量防止其过渡膨胀。另外双向阻塞队列可以运用在“工作窃取”模式中。 CyclicBarrier、CountDownLatch、Semaphore的用法CountDownLatch(线程计数器)CountDownLatch 类位于java.util.concurrent 包下，利用它可以实现类似计数器的功能。比如有一个任务A，它要等待其他4 个任务执行完毕之后才能执行，此时就可以利用CountDownLatch来实现这种功能了。1234567891011121314151617181920212223242526272829303132333435public void doSth() &#123; final CountDownLatch latch = new CountDownLatch(2); new Thread() &#123; public void run() &#123; System.out.println("子线程" + Thread.currentThread().getName() + "正在执行"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("子线程" + Thread.currentThread().getName() + "执行完毕"); latch.countDown(); &#125; &#125;.start(); new Thread() &#123; public void run() &#123; System.out.println("子线程" + Thread.currentThread().getName() + "正在执行"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("子线程" + Thread.currentThread().getName() + "执行完毕"); latch.countDown(); &#125; &#125;.start(); System.out.println("等待2 个子线程执行完毕..."); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("2 个子线程已经执行完毕"); System.out.println("继续执行主线程"); &#125; CyclicBarrier（回环栅栏-等待至barrier 状态再全部同时执行）字面意思回环栅栏，通过它可以实现让一组线程等待至某个状态之后再全部同时执行。叫做回环是因为当所有等待线程都被释放以后，CyclicBarrier 可以被重用。我们暂且把这个状态就叫做barrier，当调用await()方法之后，线程就处于barrier 了。CyclicBarrier 中最重要的方法就是await 方法，它有2 个重载版本： public int await()：用来挂起当前线程，直至所有线程都到达barrier 状态再同时执行后续任务。 public int await(long timeout, TimeUnit unit)：让这些线程等待至一定的时间，如果还有线程没有到达barrier 状态就直接让到达barrier 的线程执行后续任务。具体使用如下，另外CyclicBarrier 是可以重用的。123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; int N = 4; CyclicBarrier barrier = new CyclicBarrier(N); for (int i = 0; i &lt; N; i++) &#123; new Writer(barrier).start(); &#125; &#125; static class Writer extends Thread &#123; private CyclicBarrier cyclicBarrier; public Writer(CyclicBarrier cyclicBarrier) &#123; this.cyclicBarrier = cyclicBarrier; &#125; @Override public void run() &#123; try &#123; // 模拟线程写入数据操作 Thread.sleep(5000); System.out.println(" 线程" + Thread.currentThread().getName() + "写入数据完毕,等待其他线程写入完毕"); cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println("所有线程写入完毕，继续处理其他任务，比如数据操作"); &#125; &#125;&#125; Semaphore（信号量-控制同时访问的线程个数）Semaphore 翻译成字面意思为 信号量，Semaphore 可以控制同时访问的线程个数，通过acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。Semaphore 类中比较重要的几个方法： public void acquire(): 用来获取一个许可，若无许可能够获得，则会一直等待，直到获得许可。 public void acquire(int permits):获取permits 个许可 public void release() { } :释放许可。注意，在释放许可之前，必须先获获得许可。 public void release(int permits) { }:释放permits 个许可上面4 个方法都会被阻塞，如果想立即得到执行结果，可以使用下面几个方法 public boolean tryAcquire():尝试获取一个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire(long timeout, TimeUnit unit):尝试获取一个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false public boolean tryAcquire(int permits):尝试获取permits 个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire(int permits, long timeout, TimeUnit unit): 尝试获取permits个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false 还可以通过availablePermits()方法得到可用的许可数目。例子：若一个工厂有5 台机器，但是有8 个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过Semaphore 来实现：1234567891011121314151617181920212223242526272829303132333435public class Test &#123; public static void main(String[] args) &#123; // 工人数 int N = 8; // 机器数目 Semaphore semaphore = new Semaphore(5); for (int i = 0; i &lt; N; i++) &#123; new Worker(i, semaphore).start(); &#125; &#125; static class Worker extends Thread &#123; private int num; private Semaphore semaphore; public Worker(int num, Semaphore semaphore) &#123; this.num = num; this.semaphore = semaphore; &#125; @Override public void run() &#123; try &#123; semaphore.acquire(); System.out.println("工人" + this.num + "占用一个机器在生产..."); Thread.sleep(2000); System.out.println("工人" + this.num + "释放出机器"); semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; CountDownLatch 和CyclicBarrier 都能够实现线程之间的等待，只不过它们侧重点不同；CountDownLatch 一般用于某个线程A 等待若干个其他线程执行完任务之后，它才执行；而CyclicBarrier 一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行；另外，CountDownLatch 是不能够重用的，而CyclicBarrier 是可以重用的。 Semaphore 其实和锁有点类似，它一般用于控制对某组资源的访问权限。volatile关键字的作用(变量可见性、禁止重排序)Java 语言提供了一种稍弱的同步机制，即volatile 变量，用来确保将变量的更新操作通知到其他线程。volatile 变量具备两种特性，volatile 变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile 类型的变量时总会返回最新写入的值。变量可见性其一是保证该变量对所有线程可见，这里的可见性指的是当一个线程修改了变量的值，那么新的值对于其他线程是可以立即获取的。禁止重排序volatile 禁止了指令重排。比 synchronized 更轻量级在访问volatile 变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile 变量是一种比synchronized 关键字更轻量级的同步机制。volatile 适合这种场景：一个变量被多个线程共享，线程直接给这个变量赋值。当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU 缓存中。如果计算机有多个CPU，每个线程可能在不同的CPU 上被处理，这意味着每个线程可以拷贝到不同的 CPUcache 中。而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache这一步。适用场景值得说明的是对volatile 变量的单次读/写操作可以保证原子性的，如long 和double 类型变量，但是并不能保证i++这种操作的原子性，因为本质上i++是读、写两次操作。在某些场景下可以代替Synchronized。但是,volatile 的不能完全取代Synchronized 的位置，只有在一些特殊的场景下，才能适用volatile。总的来说，必须同时满足下面两个条件才能保证在并发环境的线程安全：（1）对变量的写操作不依赖于当前值（比如 i++），或者说是单纯的变量赋值（boolean flag = true）。（2）该变量没有包含在具有其他变量的不变式中，也就是说，不同的volatile 变量之间，不能互相依赖。只有在状态真正独立于程序内其他内容时才能使用 volatile。 如何在两个线程之间共享数据Java 里面进行多线程通信的主要方式就是共享内存的方式，共享内存主要的关注点有两个：可见性和有序性原子性。Java 内存模型（JMM）解决了可见性和有序性的问题，而锁解决了原子性的问题，理想情况下我们希望做到“同步”和“互斥”。有以下常规实现方法：将数据抽象成一个类，并将数据的操作作为这个类的方法 将数据抽象成一个类，并将对这个数据的操作作为这个类的方法，这么设计可以和容易做到同步，只要在方法上加”synchronized“ 123456789101112131415161718192021222324252627public class MyData &#123; private int j = 0; public static void main(String[] args) &#123; MyData data = new MyData(); Runnable add = new AddRunnable(data); Runnable dec = new DecRunnable(data); for (int i = 0; i &lt; 2; i++) &#123; new Thread(add).start(); new Thread(dec).start(); &#125; &#125; public synchronized void add() &#123; j++; System.out.println("线程" + Thread.currentThread().getName() + "j 为：" + j); &#125; public synchronized void dec() &#123; j--; System.out.println("线程" + Thread.currentThread().getName() + "j 为：" + j); &#125; public int getData() &#123; return j; &#125;&#125; 1234567891011public class AddRunnable implements Runnable &#123; MyData data; public AddRunnable(MyData data) &#123; this.data = data; &#125; public void run() &#123; data.add(); &#125;&#125; 123456789101112public class DecRunnable implements Runnable &#123; MyData data; public DecRunnable(MyData data) &#123; this.data = data; &#125; @Override public void run() &#123; data.dec(); &#125;&#125; Runnable 对象作为一个类的内部类 将Runnable 对象作为一个类的内部类，共享数据作为这个类的成员变量，每个线程对共享数据的操作方法也封装在外部类，以便实现对数据的各个操作的同步和互斥，作为内部类的各个Runnable 对象调用外部类的这些方法。 12345678910111213141516171819202122232425262728293031323334353637public class MyData &#123; private int j = 0; public synchronized void add() &#123; j++; System.out.println("线程" + Thread.currentThread().getName() + "j 为：" + j); &#125; public synchronized void dec() &#123; j--; System.out.println("线程" + Thread.currentThread().getName() + "j 为：" + j); &#125; public int getData() &#123; return j; &#125;&#125;class TestThread &#123; public static void main(String[] args) &#123; final MyData data = new MyData(); for (int i = 0; i &lt; 2; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; data.add(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; data.dec(); &#125; &#125;).start(); &#125; &#125;&#125; ThreadLocal作用(线程本地存储)ThreadLocal，很多地方叫做线程本地变量，也有些地方叫做线程本地存储，ThreadLocal 的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。ThreadLocalMap(线程的一个属性) 每个线程中都有一个自己的ThreadLocalMap 类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。 将一个共用的ThreadLocal 静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap 中，然后在线程执行的各处通过这个静态ThreadLocal 实例的get()方法取得自己线程保存的那个对象，避免了将这个对象作为参数传递的麻烦。 ThreadLocalMap 其实就是线程里面的一个属性，它在Thread 类中定义1ThreadLocal.ThreadLocalMap threadLocals = null; 适用场景最常见的ThreadLocal 使用场景为 用来解决 数据库连接、Session 管理等。123456789101112131415public class Test&#123; private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s; &#125;&#125; synchronized和reentrantLock的区别两者的共同点： 都是用来协调多线程对共享对象、变量的访问 都是可重入锁，同一线程可以多次获得同一个锁 都保证了可见性和互斥性 两者的不同点： ReentrantLock 显示的获得、释放锁，synchronized 隐式获得释放锁 ReentrantLock 可响应中断、可轮回，synchronized 是不可以响应中断的，为处理锁的不可用性提供了更高的灵活性 ReentrantLock 是API 级别的，synchronized 是JVM级别的 ReentrantLock 可以实现公平锁 ReentrantLock 通过Condition 可以绑定多个条件 底层实现不一样， synchronized 是同步阻塞，使用的是悲观并发策略，lock 是同步非阻塞，采用的是乐观并发策略 Lock 是一个接口，而synchronized 是Java 中的关键字，synchronized 是内置的语言实现。 synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock 在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock 时需要在finally 块中释放锁。 Lock 可以让等待锁的线程响应中断，而synchronized 却不行，使用synchronized 时，等待的线程会一直等待下去，不能够响应中断。 通过Lock 可以知道有没有成功获取锁，而synchronized 却无法办到。 Lock 可以提高多个线程进行读操作的效率，既就是实现读写锁等。 ConcurrentHashMap并发减小锁粒度减小锁粒度是指缩小锁定对象的范围，从而减小锁冲突的可能性，从而提高系统的并发能力。减小锁粒度是一种削弱多线程锁竞争的有效手段，这种技术典型的应用是ConcurrentHashMap(高性能的HashMap)类的实现。对于HashMap 而言，最重要的两个方法是get 与set 方法，如果我们对整个HashMap 加锁，可以得到线程安全的对象，但是加锁粒度太大。Segment 的大小也被称为ConcurrentHashMap 的并发度。 ConcurrentHashMap 分段锁ConcurrentHashMap，它内部细分了若干个小的HashMap，称之为段(Segment)。默认情况下一个ConcurrentHashMap 被进一步细分为16 个段，既就是锁的并发度。如果需要在ConcurrentHashMap 中添加一个新的表项，并不是将整个HashMap 加锁，而是首先根据hashcode 得到该表项应该存放在哪个段中，然后对该段加锁，并完成put 操作。在多线程环境中，如果多个线程同时进行put 操作，只要被加入的表项不存放在同一个段中，则线程间可以做到真正的并行。 ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成ConcurrentHashMap 是由Segment 数组结构和HashEntry 数组结构组成。Segment 是一种可重入锁ReentrantLock，在ConcurrentHashMap 里扮演锁的角色，HashEntry 则用于存储键值对数据。一个ConcurrentHashMap 里包含一个Segment 数组，Segment 的结构和HashMap类似，是一种数组和链表结构， 一个Segment 里包含一个HashEntry 数组，每个HashEntry 是一个链表结构的元素， 每个Segment 守护一个HashEntry 数组里的元素,当对HashEntry 数组的数据进行修改时，必须首先获得它对应的Segment 锁。 Java中用到的线程调度抢占式调度：抢占式调度指的是每条线程执行的时间、线程的切换都由系统控制，系统控制指的是在系统某种运行机制下，可能每条线程都分同样的执行时间片，也可能是某些线程执行的时间片较长，甚至某些线程得不到执行的时间片。在这种机制下，一个线程的堵塞不会导致整个进程堵塞。 协同式调度：协同式调度指某一线程执行完后主动通知系统切换到另一线程上执行，这种模式就像接力赛一样，一个人跑完自己的路程就把接力棒交接给下一个人，下个人继续往下跑。线程的执行时间由线程本身控制，线程切换可以预知，不存在多线程同步问题，但它有一个致命弱点：如果一个线程编写有问题，运行到一半就一直堵塞，那么可能导致整个系统崩溃。 JVM 的线程调度实现（抢占式调度）java 使用的线程调使用抢占式调度，Java 中线程会按优先级分配CPU 时间片运行，且优先级越高越优先执行，但优先级高并不代表能独自占用执行时间片，可能是优先级高得到越多的执行时间片，反之，优先级低的分到的执行时间少但不会分配不到执行时间。 线程让出cpu 的情况： 当前运行线程主动放弃CPU，JVM 暂时放弃CPU 操作（基于时间片轮转调度的JVM 操作系统不会让线程永久放弃CPU，或者说放弃本次时间片的执行权），例如调用yield()方法。 当前运行线程因为某些原因进入阻塞状态，例如阻塞在I/O 上。 当前运行线程结束，即运行完run()方法里面的任务。 进程调度算法优先调度算法1. 先来先服务调度算法（FCFS）当在作业调度中采用该算法时，每次调度都是从后备作业队列中选择一个或多个最先进入该队列的作业，将它们调入内存，为它们分配资源、创建进程，然后放入就绪队列。在进程调度中采用FCFS 算法时，则每次调度是从就绪队列中选择一个最先进入该队列的进程，为之分配处理机，使之投入运行。该进程一直运行到完成或发生某事件而阻塞后才放弃处理机，特点是：算法比较简单，可以实现基本上的公平。2. 短作业(进程)优先调度算法短作业优先(SJF)的调度算法是从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。而短进程优先(SPF)调度算法则是从就绪队列中选出一个估计运行时间最短的进程，将处理机分配给它，使它立即执行并一直执行到完成，或发生某事件而被阻塞放弃处理机时再重新调度。该算法未照顾紧迫型作业。 高优先权优先调度算法为了照顾紧迫型作业，使之在进入系统后便获得优先处理，引入了最高优先权优先(FPF)调度算法。当把该算法用于作业调度时，系统将从后备队列中选择若干个优先权最高的作业装入内存。当用于进程调度时，该算法是把处理机分配给就绪队列中优先权最高的进程。1. 非抢占式优先权算法在这种方式下，系统一旦把处理机分配给就绪队列中优先权最高的进程后，该进程便一直执行下去，直至完成；或因发生某事件使该进程放弃处理机时。这种调度算法主要用于批处理系统中；也可用于某些对实时性要求不严的实时系统中。2. 抢占式优先权调度算法在这种方式下，系统同样是把处理机分配给优先权最高的进程，使之执行。但在其执行期间，只要又出现了另一个其优先权更高的进程，进程调度程序就立即停止当前进程(原优先权最高的进程)的执行，重新将处理机分配给新到的优先权最高的进程。显然，这种抢占式的优先权调度算法能更好地满足紧迫作业的要求，故而常用于要求比较严格的实时系统中，以及对性能要求较高的批处理和分时系统中。3. 高响应比优先调度算法在批处理系统中，短作业优先算法是一种比较好的算法，其主要的不足之处是长作业的运行得不到保证。如果我们能为每个作业引入前面所述的动态优先权，并使作业的优先级随着等待时间的增加而以速率a 提高，则长作业在等待一定的时间后，必然有机会分配到处理机。该优先权的变化规律可描述为： (1) 如果作业的等待时间相同，则要求服务的时间愈短，其优先权愈高，因而该算法有利于短作业。(2) 当要求服务的时间相同时，作业的优先权决定于其等待时间，等待时间愈长，其优先权愈高，因而它实现的是先来先服务。(3) 对于长作业，作业的优先级可以随等待时间的增加而提高，当其等待时间足够长时，其优先级便可升到很高，从而也可获得处理机。简言之，该算法既照顾了短作业，又考虑了作业到达的先后次序，不会使长作业长期得不到服务。因此，该算法实现了一种较好的折衷。当然，在利用该算法时，每要进行调度之前，都须先做响应比的计算，这会增加系统开销。 基于时间片的轮转调度算法1. 时间片轮转法在早期的时间片轮转法中，系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把CPU 分配给队首进程，并令其执行一个时间片。时间片的大小从几ms 到几百ms。当执行的时间片用完时，由一个计时器发出时钟中断请求，调度程序便据此信号来停止该进程的执行，并将它送往就绪队列的末尾；然后，再把处理机分配给就绪队列中新的队首进程，同时也让它执行一个时间片。这样就可以保证就绪队列中的所有进程在一给定的时间内均能获得一时间片的处理机执行时间。2. 多级反馈队列调度算法(1) 应设置多个就绪队列，并为各个队列赋予不同的优先级。第一个队列的优先级最高，第二个队列次之，其余各队列的优先权逐个降低。该算法赋予各个队列中进程执行时间片的大小也各不相同，在优先权愈高的队列中，为每个进程所规定的执行时间片就愈小。例如，第二个队列的时间片要比第一个队列的时间片长一倍，……，第i+1 个队列的时间片要比第i 个队列的时间片长一倍。(2) 当一个新进程进入内存后，首先将它放入第一队列的末尾，按FCFS 原则排队等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可准备撤离系统；如果它在一个时间片结束时尚未完成，调度程序便将该进程转入第二队列的末尾，再同样地按FCFS 原则等待调度执行；如果它在第二队列中运行一个时间片后仍未完成，再依次将它放入第三队列，……，如此下去，当一个长作业(进程)从第一队列依次降到第n 队列后，在第n 队列便采取按时间片轮转的方式运行。(3) 仅当第一队列空闲时，调度程序才调度第二队列中的进程运行；仅当第1～(i-1)队列均空时，才会调度第i 队列中的进程运行。如果处理机正在第i 队列中为某进程服务时，又有新进程进入优先权较高的队列(第1～(i-1)中的任何一个队列)，则此时新进程将抢占正在运行进程的处理机，即由调度程序把正在运行的进程放回到第i 队列的末尾，把处理机分配给新到的高优先权进程。在多级反馈队列调度算法中，如果规定第一个队列的时间片略大于多数人机交互所需之处理时间时，便能够较好的满足各种类型用户的需要。 什么是CAS(乐观锁-自旋锁-比较并交换 compare and swap)概念及特性CAS（Compare And Swap/Set）比较并交换，CAS 算法的过程是这样：它包含3 个参数CAS(V,E,N)。V 表示要更新的变量(内存值)，E 表示预期值(旧的)，N 表示新值。当且仅当V 值等于E 值时，才会将V 的值设为N，如果V 值和E 值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，CAS 返回当前V 的真实值。CAS 操作是抱着乐观的态度进行的(乐观锁)，它总是认为自己可以成功完成操作。当多个线程同时使用CAS 操作一个变量时，只有一个会胜出，并成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理，CAS 操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。 原子包 java.util.concurrent.atomic（锁自旋）JDK1.5 的原子包：java.util.concurrent.atomic 这个包里面提供了一组原子类。其基本的特性就是在多线程环境下，当有多个线程同时执行这些类的实例包含的方法时，具有排他性，即当某个线程进入方法，执行其中的指令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等到该方法执行完成，才由JVM 从等待队列中选择一个另一个线程进入，这只是一种逻辑上的理解。相对于对于synchronized 这种阻塞算法，CAS 是非阻塞算法的一种常见实现。由于一般CPU 切换时间比CPU 指令集操作更加长， 所以J.U.C 在性能上有了很大的提升。如下代码：1234567891011121314151617public class AtomicInteger extends Number implements java.io.Serializable &#123; private volatile int value; public final int get() &#123; return value; &#125; public final int getAndIncrement() &#123; for (;;) &#123; //CAS 自旋，一直尝试，直达成功 int current = get(); int next = current + 1; if (compareAndSet(current, next)) return current; &#125; &#125; public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125;&#125; getAndIncrement 采用了CAS 操作，每次从内存中读取数据然后将此数据和+1 后的结果进行CAS 操作，如果成功就返回结果，否则重试直到成功为止。而compareAndSet 利用JNI 来完成CPU 指令的操作。12345678910111213141516cmpxchg/** * accumulator = AL,AX,or EAX,depending on whether * a byte,word,or doubleword comparison is being performed */ // 更新的变量和旧的预期值是否相等 if(accumulator == Destination) &#123; // 设置跳转标识 ZF = 1; Destination = Source; &#125; else &#123; // 不设置值类 ZF = 0; accumulator = Destination; &#125; ABA 问题CAS 会导致“ABA 问题”。CAS 算法实现一个重要前提需要取出内存中某时刻的数据，而在下时刻比较并替换，那么在这个时间差类会导致数据的变化。比如说一个线程one 从内存位置V 中取出A，这时候另一个线程two 也从内存中取出A，并且two 进行了一些操作变成了B，然后two 又将V 位置的数据变成A，这时候线程one 进行CAS 操作发现内存中仍然是A，然后one 操作成功。尽管线程one 的CAS 操作成功，但是不代表这个过程就是没有问题的。部分乐观锁的实现是通过版本号（version）的方式来解决ABA 问题，乐观锁每次在执行数据的修改操作时，都会带上一个版本号，一旦版本号和数据的版本号一致就可以执行修改操作并对版本号执行+1 操作，否则就执行失败。因为每次操作的版本号都会随之增加，所以不会出现ABA 问题，因为版本号只会增加不会减少。 什么是AQS(抽象的队列同步器AbstractQueuedSynchronizer)AbstractQueuedSynchronizer 类如其名，抽象的队列式的同步器，AQS 定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch。它维护了一个volatile int state（代表共享资源）和一个FIFO 线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里volatile 是核心关键词，具体volatile 的语义，在此不述。state 的访问方式有三种: getState() setState() compareAndSetState() AQS 定义两种资源共享方式Exclusive 独占资源-ReentrantLockExclusive（独占，只有一个线程能执行，如ReentrantLock）Share 共享资源-Semaphore/CountDownLatchShare（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。AQS 只是一个框架，具体资源的获取/释放方式交由自定义同步器去实现，AQS 这里只定义了一个接口，具体资源的获取交由自定义同步器去实现了（通过state 的get/set/CAS)之所以没有定义成abstract ， 是因为独占模式下只用实现tryAcquire-tryRelease ， 而共享模式下只用实现tryAcquireShared-tryReleaseShared。如果都定义成abstract，那么每个模式也要去实现另一模式下的接口。不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition 才需要去实现它。 tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0 表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 同步器的实现是ABS核心(state资源状态计数)同步器的实现是ABS 核心，以ReentrantLock 为例，state 初始化为0，表示未锁定状态。A 线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A 线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state 是能回到零态的。以CountDownLatch 以例，任务分为N 个子线程去执行，state 也初始化为N（注意N 要与线程个数一致）。这N 个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS 减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 ReentrantReadWriteLock 实现独占和共享两种方式一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquiretryRelease、tryAcquireShared-tryReleaseShared 中的一种即可。但AQS 也支持自定义同步器,同时实现独占和共享两种方式，如ReentrantReadWriteLock。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件MQ总结]]></title>
    <url>%2F2019%2F04%2F16%2F2019-04-16-message-queue%2F</url>
    <content type="text"><![CDATA[前言消息队列提供 异步 通信协议，这意味着消息的发送方和接收方不需要同时与消息队列交互。放置在队列中的消息将被存储，直到收件人检索它们。消息队列对可以在单个消息中传输的数据大小以及可能在队列中保持未完成的消息数具有隐式或显式限制。 为什么在项目中使用MQ使用MQ的优点解藕现场画个图来说明一下，A系统发送个数据到BCD三个系统，接口调用发送，那如果E系统也要这个数据呢？那如果C系统现在不需要了呢？现在A系统又要发送第二种数据了呢？A系统负责人濒临崩溃中。。。再来点更加崩溃的事儿，A系统要时时刻刻考虑BCDE四个系统如果挂了咋办？我要不要重发？我要不要把消息存起来？头发都白了啊。。。 异步现场画个图来说明一下，A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要3ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，用户感觉搞个什么东西，太慢了。 削峰每天0点到11点，A系统风平浪静，每秒并发请求数量就100个。结果每次一到11点~1点，每秒并发请求数量突然会暴增到1万条。但是系统最大的处理能力就只能是每秒钟处理1000个请求啊。。。尴尬了，系统会崩溃。。。 使用MQ的缺点系统可用性降低：MQ故障，生产者无法生产消息，消费者无法消费消息 系统复杂性变高消息丢失，消息重复，消息乱序，消息积压，消息一致性等问题 如何保证消息队列的高可用RabbitMQ的高可用性RabbitMQ有三种模式：单机模式，普通集群模式，镜像集群模式 单机模式单机模式，几乎没有高可用性，一旦故障就不可用 普通集群多台机器上启动多个rabbitmq实例，每个机器启动一个。但是创建的queue，只会放在某一个rabbtimq实例上。消费数据的时候，如果连接到了不是存放queue的那个实例，那么连接的实例会从queue所在实例上拉取数据过来。 如果存放queue的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果开启了消息持久化，让rabbitmq落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个queue拉取数据。 这方案主要是提高吞吐量的，基本没有高可用性。 镜像集群模式这种模式，才是rabbitmq的高可用模式。 跟普通集群模式不一样的是，创建的queue，无论元数据还是queue里的消息都会同步保存到多个实例上，然后每次写消息到queue的时候，都会自动把消息同步到多个实例的queue里。 好处在于，任何一个机器宕机了，别的机器都可以用，不会导致MQ系统直接故障。 坏处在于，第一，性能开销大，消息同步所有机器，导致网络带宽压力、内存压力大。第二，扩展性低，如果某个queue负载很重，要减轻这个queue的负载，新增机器，但是新增的机器也同步这个queue的所以数据，并没有减轻这个queue的负载。 Kafka的高可用性kafka一个最基本的架构认识：多个broker组成，每个broker是一个节点；创建一个topic，这个topic可以划分为多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据。 replica副本机制。每个partition的数据都会同步其他机器上，形成自己的多个replica副本。然后所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower。写的时候，leader会负责把数据同步到所有follower上去，读的时候就直接读leader上数据即可。只能读写leader？很简单，要是可以随意读写每个follower，那么就要care数据一致性的问题，系统复杂度太高，很容易出问题。kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个broker宕机了，没事儿，那个broker上面的partition在其他机器上都有副本的，如果这上面有某个partition的leader，那么此时会重新选举一个新的leader出来，大家继续读写那个新的leader即可。这就有所谓的高可用性了。 写数据的时候，生产者就写leader，然后leader将数据落地写本地磁盘，接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了，就会发送ack给leader，leader收到所有follower的ack之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从leader去读，但是只有一个消息已经被所有follower都同步成功返回ack的时候，这个消息才会被消费者读到。 如何保证消息不被重复消费（消息幂等性）rabbitmq、rocketmq、kafka，都有可能会出现消费重复消费的问题。 （1）比如拿个数据要写库，先根据主键查一下，如果这数据都有了，就别插入了，update一下 （2）比如是写redis，那没问题了，反正每次都是set，天然幂等性 （3）比如不是上面两个场景，那做的稍微复杂一点，需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，就处理，然后这个id写redis。如果消费过了，那就别处理了，保证别重复处理相同的消息即可。 还有比如基于数据库的唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会有重复，因为kafka消费者还没来得及提交offset，重复数据拿到了以后我们插入的时候，因为有唯一键约束了，所以重复数据只会插入报错，不会导致数据库中出现脏数据 如何保证MQ的消费是幂等性的，需要结合具体的业务来看 如何保证消息的可靠性传输（如何处理消息丢失的问题）RabbitMQ解决消息丢失问题生产者弄丢了数据生产者将数据发送到rabbitmq的时候，可能数据就在半路给搞丢了，因为网络啥的问题，都有可能。 使用rabbitmq的事务机制用rabbitmq提供的事务功能，就是生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。 使用rabbitmq的confirm机制在生产者那里设置开启confirm模式之后，每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会回传一个ack消息，告诉你这个消息ok了。如果rabbitmq没能处理这个消息，会回调一个nack接口，告诉你这个消息接收失败，你可以重试。而且可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么可以重发。 事务机制和cnofirm机制最大的不同在于，事务机制是同步的，提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 RabbitMQ弄丢了数据rabbitmq自己弄丢了数据，这个必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。 设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，也是可以自己重发的。 哪怕是给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。 消费者弄丢了数据rabbitmq如果丢失了数据，主要是因为消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，rabbitmq认为你都消费了，这数据就丢了。 这个时候得用rabbitmq提供的ack机制，简单来说，就是关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次自己代码里确保处理完的时候，再程序里ack一把。这样的话，如果还没处理完，不就没有ack？那rabbitmq就认为你还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。 Kafka解决消息丢失问题Kafka消费者弄丢了数据唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实刚准备处理这个消息，还没处理，自己就挂了，此时这条消息就丢咯。 这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 Kafka弄丢了数据这块比较常见的一个场景，就是kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前kafka的leader机器宕机了，将follower切换为leader之后，就会发现说这个数据就丢了 所以此时一般是要求起码设置如下4个参数： 给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本 在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧 在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了 在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了 我们生产环境就是按照上述要求配置的，这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失 生产者会不会弄丢数据如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 如何保证消息的顺序RabbitMQ消息乱序场景一个queue，多个consumer，比如依次发送3条消息到queue，分配到不同消费者处理，消费者处理的速度各异，很可能导致消息的顺序错乱。 RabbitMQ解决方案拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理 Kafka消息乱序场景一个topic，一个partition，一个consumer，内部多线程，这不也明显乱了 Kafka解决方案一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可 消息队列的消息积压、队列快满了、消息快过期的解决方案临时扩容消费者1）先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉2）新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量3）然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue4）接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据5）这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据6）等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息 消息快速消费修改消费者的操作，对消息先不做处理，快速消费掉所有的消息，等过了高峰期以后，再重新将消费者修改成原来的逻辑操作，补回之前没有处理的消息。 ActiveMQ、RabbitMQ、Kafka、RocketMQ有什么区别 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，吞吐量比RocketMQ和Kafka要低了一个数量级 万级，吞吐量比RocketMQ和Kafka要低了一个数量级 10万级，RocketMQ也是可以支撑高吞吐的一种MQ 10万级别，这是kafka最大的优点，就是吞吐量高。一般配合大数据类的系统来进行实时数据计算、日志采集等场景 时效性 毫秒级 微秒级，这是rabbitmq的一大特点，延迟是最低的 毫秒级 毫秒级 topic数量对吞吐量的影响 ActiveMQ RabbitMQ topic可以达到几百，几千个的级别，吞吐量会有较小幅度的下降，这是RocketMQ的一大优势，在同等机器下，可以支撑大量的topic topic从几十个到几百个的时候，吞吐量会大幅度下降，所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源 可用性 高，基于主从架构实现高可用性 高，基于主从架构实现高可用性 非常高，分布式架构 非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 RabbitMQ 经过参数优化配置，可以做到0丢失 经过参数优化配置，消息可以做到0丢失 功能支持 MQ领域的功能极其完备 基于erlang开发，所以并发能力很强，性能极其好，延时很低 MQ功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 优劣势总结 非常成熟，功能强大。偶尔会有较低概率丢失消息。主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用。现在社区以及国内应用都越来越少，官方社区对ActiveMQ维护越来越少。 erlang语言开发，性能极其好，延时很低；吞吐量到万级，MQ功能比较完备，开源提供的管理界面很方便监测，社区相对比较活跃，在国内互联网公司近几年用RabbitMQ也比较多。但是问题也是显而易见的，RabbitMQ确实吞吐量会低一些，因为实现机制比较重。而且erlang开发，源码级别的研究和定制比较困难，基本只能依赖于开源社区的快速维护和修复bug，并且RabbitMQ集群动态扩展会很麻烦。 日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是ok的，还可以支撑大规模的topic数量，支持复杂MQ业务场景。阿里出品、java实现，我们可以自己阅读源码，定制自己公司的MQ，可以掌控。社区活跃度一般，文档相对来说简单一些，接口这块不是按照标准JMS规范有些系统要迁移需要修改大量代码。 kafka的特点很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时kafka最好是支撑较少的topic数量即可，保证其超高吞吐量。而且kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略。这个特性天然适合大数据实时计算以及日志收集 综上所述，得出以下结论： 一般的业务系统要引入MQ，最早的公司都用ActiveMQ，但是现在确实用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以渐渐被抛弃。 后来开始用RabbitMQ，但是确实erlang语言阻止了大量的java工程师去深入研究和掌控他，但是是开源的，比较稳定的支持，活跃度也高，图形化界面易于维护； 现在确实越来越多的公司，会去用RocketMQ，确实很不错，还支持MQ事务，大型项目首选。 所以中小型公司项目，技术实力较为一般，技术挑战不是特别高，用RabbitMQ是不错的选择；大型公司，基础架构研发实力较强，用RocketMQ是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用Kafka是业内标准的，绝对没问题，社区活跃度很高，何况几乎是全世界大数据领域的事实性规范]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ArrayList源码分析]]></title>
    <url>%2F2019%2F03%2F02%2F2019-03-02-arraylist-source%2F</url>
    <content type="text"><![CDATA[前言ArrayList是可调整大小的数组,线程不安全。实现了List,RandomAccess,Cloneable,Serializable接口,可插入空数据。size, isEmpty, get, set, iterator,listIterator操作可以在常数时间(O(1))内完成, add元素操作是amortized constant time, add n个元素需要花费O(n)时间。所有操作都是线性时间关系（粗略地说）。与LinkedList实现相比，常数因子较低。 每个ArrayList实例都有一个容量capacity，容量是用于存储列表中元素的数组的大小，它始终至少与列表大小一样大。当元素添加到ArrayList时，其容量会自动扩容。扩容的具体策略不是特定的，添加一个元素花费constant amortized constant time。 程序在添加大量元素之前，应该进行ensureCapacity(int minCapacity)操作，可以增加ArrayList实例的容量capacity，这可能会减少增量重新分配的次数。 值得注意的是ArrayList的实现不是同步的。如果多线程并发操作一个ArrayList，并且至少有一个线程修改了ArrayList的结构，必须在操作外部实现同步。（结构修改是添加或删除一个或多个元素或显式调整后备数组大小的任何操作;仅设置元素的值不是结构修改。）通常可以通过对包裹list的对象进行同步，或者可以使用Collections.synchronizedList方法来对list同步，如List list = Collections.synchronizedList(new ArrayList(...)); 迭代器iterator具有fail-fast(快速失败)机制：如果list的结构在iterator创建后的任意时刻发生改变，除了通过iterator实现的方法remove()或add()操作的，iterator都会抛出ConcurrentModificationException。因此，在并发修改的情况下，迭代器快速而干净地失败，而不是在未来的未确定时间冒着任意的、非确定性行为的风险。请注意，迭代器的快速失败行为无法得到保证，因为一般来说，在存在不同步的并发修改时，不可能做出任何硬性保证。 fail-fast iterator会尽最大努力抛出ConcurrentModificationException。因此，程序不能依靠iterator的 fail-fast 抛出异常来确保正确性，iterator 的 fail-fast 行为应该用来检测bug。 ArrayList底层数据结构是数组，主要的两个属性是elementData数组和size集合大小。 add(E e)在ArrayList末尾添加元素add(E e)方法的操作步骤: modCount++ 修改次数自增 进行扩容校验，如果size等于数组长度，会进行数组复制，使得数组size+1，前size个元素数据不变 将值放到尾部 size+1 下面是源码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public boolean add(E e) &#123; modCount++; add(e, elementData, size); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; size = s + 1;&#125;private Object[] grow() &#123; return grow(size + 1);&#125;private Object[] grow(int minCapacity) &#123; return elementData = Arrays.copyOf(elementData, newCapacity(minCapacity));&#125;/** * Returns a capacity at least as large as the given minimum capacity. * Returns the current capacity increased by 50% if that suffices. * Will not return a capacity greater than MAX_ARRAY_SIZE unless * the given minimum capacity is greater than MAX_ARRAY_SIZE. * * @param minCapacity the desired minimum capacity * @throws OutOfMemoryError if minCapacity is less than zero */private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; add(int index, E element)在指定位置添加元素add(int index, E element)在指定位置添加元素的步骤: 对index的界限判断 modCount++ 修改次数自增 进行扩容校验，如果size等于数组长度，会进行数组复制，使得数组size+1 数组复制，将 index 后面的数据都向后移动一个位置，目的是把 index 位置空出来插入的数据 将数据插入到 index 位置 下面是源码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); System.arraycopy(elementData, index, elementData, index + 1, s - index); elementData[index] = element; size = s + 1;&#125;private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private Object[] grow() &#123; return grow(size + 1);&#125;/** * Returns a capacity at least as large as the given minimum capacity. * Returns the current capacity increased by 50% if that suffices. * Will not return a capacity greater than MAX_ARRAY_SIZE unless * the given minimum capacity is greater than MAX_ARRAY_SIZE. * * @param minCapacity the desired minimum capacity * @throws OutOfMemoryError if minCapacity is less than zero */private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 通过两个增加元素方法源码可以看出ArrayList在数组扩容和数组移位的操作消耗较大，在创建ArrayList时最好通过new ArrayList(int initialCapacity)定义初始化容量，减少数组扩容的性能消耗。 get(int index)获取指定位置的元素get(int index)获取指定位置的元素的操作步骤： index的合法性校验 返回指定位置的数组元素 下面是源码实现：123456789101112131415161718192021222324252627/** * Returns the element at the specified position in this list. * * @param index index of the element to return * @return the element at the specified position in this list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123; Objects.checkIndex(index, size); return elementData(index);&#125;public static int checkIndex(int index, int length) &#123; return Preconditions.checkIndex(index, length, null);&#125;public static &lt;X extends RuntimeException&gt;int checkIndex(int index, int length, BiFunction&lt;String, List&lt;Integer&gt;, X&gt; oobef) &#123; if (index &lt; 0 || index &gt;= length) throw outOfBoundsCheckIndex(oobef, index, length); return index;&#125;E elementData(int index) &#123; return (E) elementData[index];&#125; remove(int index)删除元素 检查 index 合法性 将数组size - 1，判断 index 的位置，在数组中间则数组移位复制，在数组末尾则将elementData[index] = null 下面是源码实现：123456789101112131415161718192021public E remove(int index) &#123; Objects.checkIndex(index, size); final Object[] es = elementData; @SuppressWarnings("unchecked") E oldValue = (E) es[index]; fastRemove(es, index); return oldValue;&#125;/** * Private remove method that skips bounds checking and does not * return the value removed. */private void fastRemove(Object[] es, int i) &#123; modCount++; final int newSize; if ((newSize = size - 1) &gt; i) System.arraycopy(es, i + 1, es, i, newSize - i); es[size = newSize] = null;&#125; remove(Object o)删除元素 判断 o 是否为空，为空则遍历数组查找元素为空的位置 o 不为空，遍历数组，用o.equals(es[i])判断是否相等 找到第一个相等的元素，调用fastRemove(es, i)删除 没有找到相等的元素则不做改变 下面是源码实现：1234567891011121314151617181920212223242526272829303132/** * Removes the first occurrence of the specified element from this list, * if it is present. If the list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &#123;@code i&#125; such that * &#123;@code Objects.equals(o, get(i))&#125; * (if such an element exists). Returns &#123;@code true&#125; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &#123;@code true&#125; if this list contained the specified element */public boolean remove(Object o) &#123; final Object[] es = elementData; final int size = this.size; int i = 0; found: &#123; if (o == null) &#123; for (; i &lt; size; i++) if (es[i] == null) break found; &#125; else &#123; for (; i &lt; size; i++) if (o.equals(es[i])) break found; &#125; return false; &#125; fastRemove(es, i); return true;&#125; Vector与ArrayList类似，也是实现List，底层数据结构也是动态数组。不同的是Vector线程安全的。在add()方法和remove()方法加了 synchronized 进行同步操作，但是加锁使得Vector性能较低，Vector 是一个同步容器并不是一个并发容器。 add()下面是源码实现：12345public synchronized boolean add(E e) &#123; modCount++; add(e, elementData, elementCount); return true;&#125; remove()下面是源码实现：1234567891011121314public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置]]></title>
    <url>%2F2019%2F01%2F21%2F2019-01-21-nginx%2F</url>
    <content type="text"><![CDATA[前言Nginx是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 Nginx 安装参考：菜鸟教程Nginx安装 安装完成之后将Nginx目录添加到环境变量中 执行命令，编辑系统的环境变量1[root@root nginx]# vim /etc/profile 在文件末尾追加nginx的目录123#nginxexport NGINX_HOME=/usr/local/webserver/nginxexport PATH=$PATH:$NGINX_HOME/sbin 刷新系统环境变量，令之前的修改立即生效1[root@root conf]# source /etc/profile 查看nginx版本，输入如下命令显示出nginx版本即安装成功12[root@root nginx]# nginx -vnginx version: nginx/1.6.2 启动nginx之后，可以通过日志来查看进程的相关信息1234[root@root nginx]# ps -ef | grep nginxroot 627 29830 0 15:46 pts/1 00:00:00 grep --color=auto nginxroot 18474 1 0 May17 ? 00:00:00 nginx: master process /usr/local/webserver/nginx/sbin/nginxnobody 29997 18474 0 14:44 ? 00:00:00 nginx: worker process nginx: master process 主要是负责日志的更新，热装载 主进程nginx: worker process 工作进程 处理客户端的连接，处理请求 Nginx 常用命令1234567nginx # 默认方式启动nginxnginx -s reload # 重新载入配置文件，热装载nginx -s reopen # 重启 Nginxnginx -s stop # 快速停止 Nginxnginx -s quit # 当前请求处理完停止 Nginxnginx -c /tmp/nginx.conf # 指定配置文件启动nginx -t # 测试配置是否正确 Nginx 常用配置Nginx服务器的基础配置，默认的配置存放在nginx/nginx.conf。 在 nginx.conf 的注释符号为： # 默认的 nginx 配置文件 nginx.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] "$request" ' # '$status $body_bytes_sent "$http_referer" ' # '"$http_user_agent" "$http_x_forwarded_for"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; Nginx的文件结构123456789101112131415161718192021222324252627... #全局块events &#123; #events块 ...&#125;http #http块&#123; ... #http全局块 server #server块 &#123; ... #server全局块 location [PATTERN] #location块 &#123; ... &#125; location [PATTERN] &#123; ... &#125; &#125; server &#123; ... &#125; ... #http全局块&#125; 1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。 2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。 3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。 4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。 5、location块：配置请求的路由，以及各种页面的处理情况。 配置文件详解123456789101112131415161718192021222324252627282930313233343536373839########### 每个指令必须有分号结束。##################user administrator administrators; #配置用户或者组，默认为nobody nobody。#worker_processes 2; #允许生成的进程数，默认为1#pid /nginx/pid/nginx.pid; #指定nginx进程运行文件存放地址error_log log/error.log debug; #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emergevents &#123; accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off #use epoll; #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport worker_connections 1024; #最大连接数，默认为512&#125;http &#123; include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型，默认为text/plain #access_log off; #取消服务日志 log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式 access_log log/access.log myFormat; #combined为日志格式的默认值 sendfile on; #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 keepalive_timeout 65; #连接超时时间，默认为75s，可以在http，server，location块。 upstream mysvr &#123; server 127.0.0.1:7878; server 192.168.10.121:3333 backup; #热备 &#125; error_page 404 https://www.baidu.com; #错误页 server &#123; keepalive_requests 120; #单连接请求上限次数。 listen 4545; #监听端口 server_name 127.0.0.1; #监听地址 location ~*^.+$ &#123; #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。 #root path; #根目录 #index vv.txt; #设置默认页 proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip &#125; &#125;&#125; 上面是nginx的基本配置，需要注意的有以下几点： 1、几个常见配置项：1.$remote_addr 与 $http_x_forwarded_for 用以记录客户端的ip地址；2.$remote_user ：用来记录客户端用户名称；3.$time_local ： 用来记录访问时间与时区；4.$request ： 用来记录请求的url与http协议；5.$status ： 用来记录请求状态；成功是200；6.$body_bytes_s ent ：记录发送给客户端文件主体内容大小；7.$http_referer ：用来记录从那个页面链接访问过来的；8.$http_user_agent ：记录客户端浏览器的相关信息； 2、惊群现象：一个网路连接到来，多个睡眠的进程被同事叫醒，但只有一个进程能获得链接，这样会影响系统性能。 3、每个指令必须有分号结束。原文地址：https://www.cnblogs.com/knowledgesea/p/5175711.html 配置动静分离所谓的动静分离实质上是指我们对于nginx配置里面的动态请求和静态文件都做了一定的分离。例如以下的配置信息： 1234567891011121314server &#123; listen 80; server_name www.idea.com *.idea.com idea.*; root /usr/local/www; #这里面添加映射static的记录 location /static &#123; root /usr/local/static/; &#125; location / &#123; root /usr/local/www/; index idea.html; &#125; &#125; ps:这里面的配置内容，我引入了host文件的修改 123192.168.43.235 www.idea.com192.168.43.235 test.idea.com192.168.43.235 idea.test 图片的实际存储位置是： 1/usr/local/static/img/logo.jpg 按照上述的配置来讲，访问的方式是： 1http://www.idea.com/static/img/logo.jpg 但是这种情况下，我们通过nginx来访问图片的方式是不会成功的，原因是这个地址会被nginx处理成为： 前往root地址+static的最终地址进行查询： 1/usr/local/static/static 为了避免这种情况，通常会用别名alias来进行匹配，具体配置如下： 12345678910111213server &#123; listen 80; server_name www.idea.com *.idea.com idea.*; root /usr/local/www; location /static &#123; alias /usr/local/static/; &#125; location / &#123; root /usr/local/www/; index idea.html; &#125; &#125; 这个时候，我们再去访问 http://www.idea.com/static/img/logo.jpg 就会访问成功了。 同样对于别名我们可以进行更加复杂一些的逻辑操作案例： 假设我们需要访问static底下的文件内容，这个时候不需要携带相应的名称，直接通过访问http://www.idea.com/static/css/test.css 就可以访问到 /usr/local/www/static/css/ 底下的内容了 那么，假如说有很多种类型的静态文件需要进行映射该如何配置呢？这个时候可以引入正则表达式的配置：1234567891011121314151617server &#123; listen 80; server_name www.idea.com *.idea.com idea.*; root /usr/local/www; location /static &#123; alias /usr/local/static/; &#125; #这里的~是指忽略大小写 location ~* \.(gif|png|jpg|css|js) &#123; root /usr/local/static/; &#125; location / &#123; root /usr/local/www/; index idea.html; &#125;&#125; 加入了这段正则表达式的配置之后（~* .(gif|png|css|js)$） 通过访问该路径：http://www.idea.com/css/test.css 我们就可以访问到/usr/local/static/css/底下的文件内容了。 代理访问机制nginx还提供了一个非常灵活的代理访问机制，供我们通过代理的方式来进行访问location 通过nginx配置完全匹配代理进行页面跳转： 123456789101112131415161718192021server &#123; listen 80; server_name www.idea.com *.idea.com idea.*; root /usr/local/www; location /static &#123; alias /usr/local/static/; &#125; location ~* \.(gif|png|jpg|css|js) &#123; alias /usr/local/static/; &#125; location =/idea-serach &#123; proxy_pass http://www.baidu.com &#125; location / &#123; root /usr/local/www/; index idea.html; &#125;&#125; 通过proxy_pass配置来提供代理的请求转发，当我们访问http://www.idea.com/idea-serach的时候，就会匹配到访问百度的页面了。 小结 nginx对于访问的常见配置支持以下几种： 通过全路径访问location 通过关键字static匹配来访问location 通过正则表达式来访问location 通过反向代理进行访问location 防盗链匹配设置某些链接只允许在固定的网站站点进行访问，防止某些特殊的域名进行ip访问之后盗取本网站的资源文件，因此可以设置这个防盗链的功能。具体配置如下： 12345678910111213141516171819202122232425server &#123; listen 80; server_name www.idea.com *.idea.com idea.*; root /usr/local/www/; location /static &#123; alias /usr/local/static/; &#125; # 防盗链配置 location ~* \.(gif|png|jpg|css|js) &#123; valid_referers none blocked *.idea.com; if ($invalid_referer) &#123; return 403; &#125; root /usr/local/static/; &#125; location =/idea-serach &#123; proxy_pass http://www.baidu.com; &#125; location / &#123; root /usr/local/www/; &#125;&#125; 网站黑名单黑名单的配置比较简单，只需要先创建好黑名单文件，然后在http块里面引入就好了 1234# 创建黑名单文件echo 'deny 192.168.0.132;' &gt;&gt; balck.ip#http 配置块中引入 黑名单文件include black.ip; 记得配置成功之后要让nginx reload一下,同时请求的时候查看是否是203，如果是的话说明是缓存请求。 Nginx的日志配置当我们需要对于客户端发送的数据进行一些详细信息的查看时候，需要对nginx进行日志的记录，相应的可选参数有以下几点： 1234567891011log_format格式变量： $remote_addr #记录访问网站的客户端地址 $remote_user #远程客户端用户名 $time_local #记录访问时间与时区 $request #用户的http请求起始行信息 $status #http状态码，记录请求返回的状态码，例如：200、301、404等 $body_bytes_sent #服务器发送给客户端的响应body字节数 $http_referer #记录此次请求是从哪个连接访问过来的，可以根据该参数进行防盗链设置。 $http_user_agent #记录客户端访问信息，例如：浏览器、手机客户端等 $http_x_forwarded_for #当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器也要进行相关的x_forwarded_for设置将日志配置放在http块里面即可。 通常我们会将日志的配置放置于http模块当中，例如下边的这组案例： 12345678910111213141516171819202122232425262728293031323334events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log logs/access.log main; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; Nginx 负载均衡nginx里面有个叫做upstream的模块，专门用于配置负载均衡的内容，upstream里面提供有以下的相关参数： service 反向服务地址 加端口weight 权重max_fails 失败多少次 认为主机已挂掉则，踢出fail_timeout 移除server之后重新请求的时间 当服务挂了之后，这段时间内重新连接backup 备用服务 （当服务全部都挂了，那么就会请求这里的服务）max_conns 允许最大连接数slow_start 当节点恢复，不立即加入,而是等待 slow_start 后加入服务对列。 相应参数的具体配置如下： 1234upstream backend &#123; server 192.168.43.191:8080 weight=5 fail_timeout=10s; server 192.168.43.191:8089 weight=5 fail_timeout=10s; &#125; ngixn里面默认支持的负载均衡策略是轮询加权重的方式，除此之外，nginx自身还支持额外的多种负载均衡策略： ll+weight：轮询加权重 (默认)容易出现失重的情况，例如说某一台机器的访问过慢，容易导致请求堆积。 ip_hash : 基于Hash 计算 ,常用于保持session 一致性基于hash计算的时候，可以根据ip进行hash计算请求到指定的服务器。（通常session一致性在分布式中最好的处理手段是将session存储在第三方的存储中心） 首先对ip进行hash计算之后，将该值和服务器个数进行取模运算。 url_hash: 静态资源缓存,节约存储，加快速度可以根据图片的url请求到指定的服务器，比较好理解。 least_conn : 最小连接数每次请求都只会请求到最少客户端连接数的那台服务器去。 least_time ：最小的响应时间计算节点平均响应时间，然后取响应最快的那个，分配权重更高 通过使用ip进行哈希计算的方式来请求后端服务器 1234567891011upstream backend &#123; ip_hash; server 192.168.43.191:8080 weight=1; server 192.168.43.191:8089 weight=8 fail_timeout=10s;&#125;location / &#123; #root html; index index.html index.htm; proxy_pass http://backend; &#125; 当然Nginx除了这些常用功能以外，还提供有非常丰富的其他功能配置，具体配置可以参考nginx的官方文档配置信息http://nginx.org/en/docs/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合]]></title>
    <url>%2F2018%2F11%2F22%2F2018-11-22-java-collection%2F</url>
    <content type="text"><![CDATA[接口继承关系和实现集合类存放于Java.util 包中，主要有3 种：set(集）、list(列表包含Queue）和map(映射)。 Collection：Collection 是集合List、Set、Queue 的最基本的接口。 Iterator：迭代器，可以通过迭代器遍历集合中的数据 Map：是映射表的基础接口 ListJava 的List 是非常常用的数据类型。List 是有序的Collection。Java List 一共三个实现类：分别是ArrayList、Vector 和LinkedList。 ArrayList(数组)ArrayList 是最常用的List 实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要将已经有数组的数据复制到新的存储空间中。当从ArrayList 的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。 Vector(数组、synchronized实现线程安全)Vector 与ArrayList 一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一个线程能够写Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问它比访问ArrayList 慢。 LinkedList(链表)LinkedList 是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，他还提供了List 接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。 SetSet 注重独一无二的性质,该体系集合用于存储无序(存入和取出的顺序不一定相同)元素，值不能重复。对象的相等性本质是对象hashCode 值（java 是依据对象的内存地址计算出的此序号）判断的，如果想要让两个不同的对象视为相等的，就必须覆盖Object 的hashCode 方法和equals 方法。 HashSet(Hash表)哈希表边存放的是哈希值。HashSet 存储元素的顺序并不是按照存入时的顺序（和List 显然不同） 而是按照哈希值来存的所以取数据也是按照哈希值取得。元素的哈希值是通过元素的hashcode 方法来获取的, HashSet 首先判断两个元素的哈希值，如果哈希值一样，接着会比较equals 方法 如果 equls 结果为true ，HashSet 就视为同一个元素。如果equals 为false 就不是同一个元素。哈希值相同equals 为false 的元素是怎么存储呢,就是在同样的哈希值下顺延（可以认为哈希值相同的元素放在一个哈希桶中）。也就是哈希一样的存一列。如图1 表示hashCode 值不相同的情况；图2 表示hashCode 值相同，但equals 不相同的情况。 HashSet 通过hashCode 值来确定元素在内存中的位置。一个hashCode 位置上可以存放多个元素。 TreeSet(二叉树) TreeSet()是使用二叉树的原理对新add()的对象按照指定的顺序排序（升序、降序），每增加一个对象都会进行排序，将对象插入的二叉树指定的位置。 Integer 和String 对象都可以进行默认的TreeSet 排序，而自定义类的对象是不可以的，自己定义的类必须实现Comparable 接口，并且覆写相应的compareTo()函数，才可以正常使用。 在覆写compare()函数时，要返回相应的值才能使TreeSet 按照一定的规则来排序 比较此对象与指定对象的顺序。如果该对象小于、等于或大于指定对象，则分别返回负整数、零或正整数。 LinkHashSet(HashSet+LinkedHashMap）对于LinkedHashSet 而言， 它继承与HashSet 、又基于LinkedHashMap 来实现的。LinkedHashSet 底层使用LinkedHashMap 来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet 相同，因此LinkedHashSet 的实现上非常简单，只提供了四个构造方法，并通过传递一个标识参数，调用父类的构造器，底层构造一个LinkedHashMap 来实现，在相关操作上与父类HashSet 的操作相同，直接调用父类HashSet 的方法即可。 Map HashMap(数组+链表+红黑树)HashMap 根据键的hashCode 值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap 最多只允许一条记录的键为null，允许多条记录的值为null。HashMap 非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections 的synchronizedMap 方法使HashMap 具有线程安全的能力，或者使用ConcurrentHashMap。我们用下面这张图来介绍HashMap 的结构。 Java7实现大方向上，HashMap 里面是一个数组，然后数组中每个元素是一个单向链表。上图中，每个绿色的实体是嵌套类 Entry 的实例，Entry 包含四个属性：key, value, hash 值和用于单向链表的 next。 capacity：当前数组容量，始终保持 2^n，可以扩容，扩容后数组大小为当前的 2 倍。 loadFactor：负载因子，默认为 0.75。 threshold：扩容的阈值，等于 capacity * loadFactor Java8实现Java8 对 HashMap 进行了一些修改，最大的不同就是利用了红黑树，所以其由 数组+链表+红黑树 组成。根据 Java7 HashMap 的介绍，我们知道，查找的时候，根据 hash 值我们能够快速定位到数组的具体下标，但是之后的话，需要顺着链表一个个比较下去才能找到我们需要的，时间复杂度取决于链表的长度，为 O(n)。为了降低这部分的开销，在 Java8 中，当链表中的元素超过了 8 个以后，会将链表转换为红黑树，在这些位置进行查找的时候可以降低时间复杂度为 O(logN)。 ConcurrentHashMapSegment段ConcurrentHashMap 和 HashMap 思路是差不多的，但是因为它支持并发操作，所以要复杂一些。整个 ConcurrentHashMap 由一个个 Segment 组成，Segment 代表”部分“或”一段“的意思，所以很多地方都会将其描述为分段锁。注意，行文中，我很多地方用了“槽”来代表一个segment。 线程安全(Segment 继承 ReentrantLock 加锁)简单理解就是，ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 并行度(默认16)concurrencyLevel：并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是 16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的。再具体到每个 Segment 内部，其实每个 Segment 很像之前介绍的 HashMap，不过它要保证线程安全，所以处理起来要麻烦些。 Java8 实现(引入了红黑树)Java8 对 ConcurrentHashMap 进行了比较大的改动,Java8 也引入了红黑树。 HashTable(线程安全)HashTable 是遗留类，很多映射的常用功能与HashMap 类似，不同的是它承自Dictionary 类，并且是线程安全的，任一时间只有一个线程能写HashTable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap 引入了分段锁。HashTable 不建议在新代码中使用，不需要线程安全的场合可以用HashMap 替换，需要线程安全的场合可以用ConcurrentHashMap 替换。 TreeMap(可排序)TreeMap 实现SortedMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap 时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap 时，key 必须实现Comparable 接口或者在构造TreeMap 传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException 类型的异常。 LinkHashMap(记录插入顺序)LinkedHashMap 是HashMap 的一个子类，保存了记录的插入顺序，在用Iterator 遍历LinkedHashMap 时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。]]></content>
      <categories>
        <category>集合</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ]]></title>
    <url>%2F2018%2F10%2F22%2F2018-10-22-rabbitmq%2F</url>
    <content type="text"><![CDATA[概念RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括： 可靠性（Reliability）：RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。 灵活的路由（Flexible Routing）：在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。 消息集群（Clustering）：多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。 高可用（Highly Available Queues）：队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。 多种协议（Multi-protocol）：RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT等等。 多语言客户端（Many Clients）：RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。 管理界面（Management UI）:RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。 跟踪机制（Tracing）:如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。 插件机制（Plugin System）:RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。 RabbitMQ架构 Message消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange（将消息路由给队列 ） 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Binding（消息队列和交换器之间的关联） 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection 网络连接，比如一个TCP 连接。 Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP 连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。 Broker 表示消息队列服务器实体。 Exchange(将消息路由给队列)Exchange 分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： Direct 键（routing key）分布： Direct：消息中的路由键（routing key）如果和 Binding 中的 binding key 一致，交换器就将消息发到对应的队列中。它是完全匹配、单播的模式。 Fanout（广播分发） Fanout：每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 topic 交换器（模式匹配） topic 交换器：topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“”。#匹配0 个或多个单词，匹配不多不少一个单词。]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 常用类库]]></title>
    <url>%2F2018%2F08%2F11%2F2018-08-11-java-common-library%2F</url>
    <content type="text"><![CDATA[前言记录开发中经常用到的类库 日志相关类库日志库是很常见的，因为你在每一个项目中都需要他们。打印日志是服务器端应用中最重要的事情，因为日志是你了解你的程序发生了什么的唯一途径。尽管JDK附带自己的日志库，但是还是有很多更好的选择可用，例如 Log4j 、 SLF4j 和 LogBack。 Java开发人员应该熟悉日志记录的利弊， 并且了解为什么SLF4J要比Log4J要好。 SLF4J JSON解析库在当今世界的web服务和物联网中(IoT)，JSON已经取代了XML，成为从客户端到服务器传送信息的首选协议。有一个好消息和一个坏消息。坏消息 是JDK没有提供JSON库。好消息是有许多优秀的第三方库可以用来解析和创建JSON消息，如 Jackson 和 Gson。 一个Java web开发人员应该熟悉Jackson 和 Gson这两种中的至少一种库。 Jackson 单元测试库单元测试技术的使用，是区分一个一般的开发者和好的开发者的重要指标。程序员经常有各种借口不写单元测试，但最常见的借口就是缺乏经验和知识。常见的单测框架有 JUnit , Mockito 和PowerMock 。 JUnit 通用类库有几个很好的第三方通用库可供Java开发人员使用，例如 Apache Commons 和 Google Guava 。我会经常在我的代码中使用这些通用类库，因为这些类库都是经过无数开发者实践过的，无论是实用性还是在性能等方面都是最佳的。 Apache Commons 和 Google Guava Http 库我不是很喜欢JDK的一个重要原因就包括他们缺乏对HTTP的支持。虽然可以使用java.net包类，但是这和直接使用像 Apache HttpClient 和 HttpCore 等开源类库比起来麻烦太多了。 尽管JDK 9将开始HTTP 2.0，也对HTTP的支持做了优化，但是我还是强烈建议所有的Java开发人员熟悉流行的HTTP处理类库，例如HttpClient和HttpCore HTTP等库。 XML解析库市面上有很多XML解析的类库，如 Xerces , JAXB , JAXP , Dom4j , Xstream 等。 Xerces2是下一代高性能，完全兼容的XML解析工具。Xerces2定义了 Xerces Native Interface (XNI)规范，并提供了一个完整、兼容标准的 XNI 规范实现。该解析器是完全重新设计和实现的，更简单以及模块化。 Excel读写库许多应用程序需要提供把数据导出到Excel的功能，如果你要做相同的Java应用程序,那么你需要 Apache POI API 。这是一个非常丰富的类库，你可以从Java程序读写XLS文件。 字节码库如果你正在编写一个框架或者类库。有一些受欢迎的字节码库如 javassist 和 Cglib Nodep 可以供你选择，他们可以让你阅读和修改应用程序生成的字节码。 Javassist使得JAVA字节码操作非常简单。它是一个为编辑Java字节码而生的类库。 ASM 是另一个有用的字节码编辑库。 数据库连接池库如果你的Java应用程序与数据库交互不是使用数据库连接池库的话，那么你就大错特错了。因为在运行时创建数据库连接非常耗时并且会拖慢你的程序。所以墙裂建议使用，有些好用的连接池可供选择，如 Commons Pool 和 DBCP 和 Alibaba druid。 在web应用程序中，web服务器通常提供了这些功能。但是在java项目中需要把数据库连接池的类库导入到应用中。 消息传递库像日志和数据库连接池一样，消息传递也是很多实际的Java项目中必备的。Java提供了JMS Java消息服务，但这不是JDK的一部分,你需要单独的引入jms.jar。类似地，如果您准备使用第三方消息传递协议， Tibco RV 是个不错的选择。 PDF处理库除了Excel和Word，PDF也是一种常用的文件格式。如果你的应用程序要支持PDF格式的文件处理，你可以使用 iText 和 Apache FOP 类库。两者都提供了非常有用的PDF处理功能。 日期和时间库在Java之前，JDK的日期和时间库一直被人们所诟病，比如其非线程安全的、不可变的、容易出错等。很多开发人员会选择更好用的 JodaTime 类库。 但是在Java8推出之后，我们就可以彻底放弃JodaTime了，因为Java 8提供了其所有功能。但是，如果你的代码运行在一个低版本的JDK中，那么JodaTime还是值得使用的。 集合类库虽然JDK有丰富的集合类，但还是有很多第三方类库可以提供更多更好的功能。如 Apache Commons Collections 、 Goldman Sachs collections 、 Google Collections 和 Trove 。Trove尤其有用，因为它提供所有标准Collections 类的更快的版本以及能够直接在原语（primitive）（例如包含int 键或值的Map 等）上操作的Collections 类的功能。 FastUtil是另一个类似的API，它继承了Java Collection Framework，提供了数种特定类型的容器，包括映射map、集合set、列表list、优先级队列（prority queue），实现了java.util包的标准接口（还提供了标准类所没有的双向迭代器），还提供了很大的（64位）的array、set、list，以及快速、实用的二进制或文本文件的I/O操作类。 邮件APIjavax.mail 和 Apache Commons Email 提供了发送邮件的api。它们建立在JavaMail API的基础上，提供简化的用法。 HTML解析库和XML与JSON类似，HTML是另外一种我们可能要打交道的传输格式。值得庆幸的是，我们有jsoup可以大大简化Java应用程序使用HTML。你不仅可以使用 JSoup 解析HTML还可以创建HTML文档。 加密库Apache Commons家族中的 Commons Codec 就提供了一些公共的编解码实现，比如Base64, Hex, MD5,Phonetic and URLs等等。 嵌入式SQL数据库我真的是非常喜欢像 H2 这种内存数据库，他可以嵌入到你的Java应用中。在你跑单测的时候如果你需要一个数据库，用来验证你的SQL的话，他是个很好的选择。顺便说一句,H2不是唯一嵌入式DB，你还有 Apache Derby 和 HSQL 可供选择。 JDBC故障诊断库有不错的JDBC扩展库的存在使得调试变得很容易，例如P6spy，这是一个针对数据库访问操作的动态监测框架，它使得数据库数据可无缝截取和操纵，而不必对现有应用程序的代码作任何修改。 P6Spy 分发包包括P6Log，它是一个可记录任何 Java 应用程序的所有JDBC事务的应用程序。其配置完成使用时，可以进行数据访问性能的监测。 序列化库Google Protocol Buffer是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。 网络库一些有用的网络库主要有 Netty 的和 Apache MINA 。如果您正在编写一个应用程序，你需要做的底层网络任务，可以考虑使用这些库。 这都是每位Java开发人员应该熟悉的，并且十分有用的库。Java生态系统非常庞大的，你会发现有很多不同的类库可以做不同的事情。每个你想到的东西，都可能有一个库可以做到。 参考来源公众号:Hollis]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring原理]]></title>
    <url>%2F2018%2F07%2F09%2F2018-07-09-spring-principle%2F</url>
    <content type="text"><![CDATA[前言Spring是一个全面的、企业应用开发一站式的解决方案，贯穿表现层、业务层、持久层。但是Spring仍然可以和其他的框架无缝整合。 Spring特点轻量级控制反转面向切面容器框架集合 Spring核心组件 Spring常用模块 Spring主要包 常用注解bean 注入与装配的的方式有很多种，可以通过xml，get set 方式，构造函数或者注解等。简单易用的方式就是使用Spring 的注解了，Spring 提供了大量的注解方式。 Spring第三方结合 Spring IOC原理概念Spring 通过一个配置文件描述 Bean 及 Bean 之间的依赖关系，利用 Java 语言的反射功能实例化Bean 并建立 Bean 之间的依赖关系。 Spring 的 IoC 容器在完成这些底层工作的基础上，还提供了 Bean 实例缓存、生命周期管理、 Bean 实例代理、事件发布、资源装载等高级服务。 Spring 容器高层视图Spring 启动时读取应用程序提供的Bean 配置信息，并在Spring 容器中生成一份相应的Bean 配置注册表，然后根据这张注册表实例化Bean，装配好Bean 之间的依赖关系，为上层应用提供准备就绪的运行环境。其中Bean 缓存池为HashMap 实现。 IOC容器实现BeanFactory-框架基础设施BeanFactory 是 Spring 框架的基础设施，面向 Spring 本身；ApplicationContext 面向使用Spring 框架的开发者，几乎所有的应用场合我们都直接使用 ApplicationContext 而非底层的 BeanFactory。 BeanDefinitionRegistry 注册表 Spring 配置文件中每一个节点元素在 Spring 容器里都通过一个 BeanDefinition 对象表示，它描述了 Bean 的配置信息。而 BeanDefinitionRegistry 接口提供了向容器手工注册BeanDefinition 对象的方法。 BeanFactory 顶层接口 位于类结构树的顶端 ，它最主要的方法就是 getBean(String beanName)，该方法从容器中返回特定名称的 Bean，BeanFactory 的功能通过其他的接口得到不断扩展： ListableBeanFactory 该接口定义了访问容器中 Bean 基本信息的若干方法，如查看Bean 的个数、获取某一类型Bean 的配置名、查看容器中是否包括某一 Bean 等方法； HierarchicalBeanFactory父子级联 父子级联 IoC 容器的接口，子容器可以通过接口方法访问父容器； 通过HierarchicalBeanFactory 接口， Spring 的 IoC 容器可以建立父子层级关联的容器体系，子容器可以访问父容器中的 Bean，但父容器不能访问子容器的 Bean。Spring 使用父子容器实现了很多功能，比如在 Spring MVC 中，展现层 Bean 位于一个子容器中，而业务层和持久层的 Bean 位于父容器中。这样，展现层 Bean 就可以引用业务层和持久层的 Bean，而业务层和持久层的 Bean 则看不到展现层的 Bean。 ConfigurableBeanFactory 是一个重要的接口，增强了 IoC 容器的可定制性，它定义了设置类装载器、属性编辑器、容器初始化后置处理器等方法； AutowireCapableBeanFactory 自动装配 定义了将容器中的 Bean 按某种规则（如按名字匹配、按类型匹配等）进行自动装配的方法； SingletonBeanRegistry 运行期间注册单例 Bean 定义了允许在运行期间向容器注册单实例 Bean 的方法；对于单实例（ singleton）的 Bean来说，BeanFactory 会缓存 Bean 实例，所以第二次使用 getBean() 获取 Bean 时将直接从IoC 容器的缓存中获取 Bean 实例。Spring 在 DefaultSingletonBeanRegistry 类中提供了一个用于缓存单实例 Bean 的缓存器，它是一个用HashMap 实现的缓存器，单实例的 Bean 以beanName 为键保存在这个HashMap 中。 依赖日志框架 在初始化 BeanFactory 时，必须为其提供一种日志框架，比如使用Log4J， 即在类路径下提供 Log4J 配置文件，这样启动 Spring 容器才不会报错。 ApplicationContextApplicationContext 由 BeanFactory 派生而来， 提供了更多面向实际应用的功能。ApplicationContext 继承了 HierarchicalBeanFactory 和 ListableBeanFactory 接口，在此基础上，还通过多个其他的接口扩展了 BeanFactory 的功能： ClassPathXmlApplicationContext：默认从类路径加载配置文件 FileSystemXmlApplicationContext：默认从文件系统中装载配置文件 ApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。 MessageSource：为应用提供 i18n 国际化消息访问的功能； ResourcePatternResolver ： 所 有 ApplicationContext 实现类都实现了类似于PathMatchingResourcePatternResolver 的功能，可以通过带前缀的 Ant 风格的资源文件路径装载 Spring 的配置文件。 LifeCycle：该接口是 Spring 2.0 加入的，该接口提供了 start()和 stop()两个方法，主要用于控制异步处理过程。在具体使用时，该接口同时被 ApplicationContext 实现及具体Bean 实现， ApplicationContext 会将 start/stop 的信息传递给容器中所有实现了该接口的 Bean，以达到管理和控制 JMX、任务调度等目的。 ConfigurableApplicationContext 扩展于 ApplicationContext，它新增加了两个主要的方法： refresh()和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力。在应用上下文关闭的情况下调用 refresh()即可启动应用上下文，在已经启动的状态下，调用 refresh()则清除缓存并重新装载配置信息，而调用close()则可关闭应用上下文。 WebApplication 体系架构WebApplicationContext 是专门为 Web 应用准备的，它允许从相对于 Web 根目录的路径中装载配置文件完成初始化工作。从WebApplicationContext 中可以获得ServletContext 的引用，整个 Web 应用上下文对象将作为属性放置到 ServletContext中，以便 Web 应用环境可以访问 Spring 应用上下文。 Spring Bean 作用域Spring 3 中为Bean 定义了5 中作用域，分别为singleton（单例）、prototype（原型）、request、session 和global session，5 种作用域说明如下： singleton:单例模式(多线程下不安全) singleton：单例模式，Spring IoC 容器中只会存在一个共享的Bean 实例，无论有多少个Bean 引用它，始终指向同一对象。该模式在多线程下是不安全的。Singleton 作用域是Spring 中的缺省作用域，也可以显示的将Bean 定义为singleton 模式，配置为： 1&lt;bean id="userDao" class="com.ioc.UserDaoImpl" scope="singleton"/&gt; prototype:原型模式每次使用时创建 prototype:原型模式，每次通过Spring 容器获取prototype 定义的bean 时，容器都将创建一个新的Bean 实例，每个Bean 实例都有自己的属性和状态，而singleton 全局只有一个对象。根据经验，对有状态的bean使用prototype作用域，而对无状态的bean使用singleton作用域。 Request: 一次 request 一个实例 request：在一次Http 请求中，容器会返回该Bean 的同一实例。而对不同的Http 请求则会产生新的Bean，而且该bean 仅在当前Http Request 内有效,当前Http 请求结束，该bean实例也将会被销毁。 1&lt;bean id="loginAction" class="com.cnblogs.Login" scope="request"/&gt; session session：在一次Http Session 中，容器会返回该Bean 的同一实例。而对不同的Session 请求则会创建新的实例，该bean 实例仅在当前Session 内有效。同Http 请求相同，每一次session 请求创建新的实例，而不同的实例之间不共享属性，且实例仅在自己的session 请求内有效，请求结束，则实例将被销毁。 1&lt;bean id="userPreference" class="com.ioc.UserPreference" scope="session"/&gt; global Session global Session：在一个全局的Http Session 中，容器会返回该Bean 的同一个实例，仅在使用portlet context 时有效。 Spring Bean 生命周期实例化 实例化一个Bean，也就是我们常说的new。 IOC 依赖注入 按照Spring 上下文对实例化的Bean 进行配置，也就是IOC 注入。 setBeanName 实现 如果这个Bean 已经实现了BeanNameAware 接口，会调用它实现的setBeanName(String)方法，此处传递的就是Spring 配置文件中Bean 的id 值 BeanFactoryAware 实现 如果这个Bean 已经实现了BeanFactoryAware 接口，会调用它实现的setBeanFactory，setBeanFactory(BeanFactory)传递的是Spring 工厂自身（可以用这个方式来获取其它Bean，只需在Spring 配置文件中配置一个普通的Bean 就可以）。 ApplicationContextAware 实现 如果这个Bean已经实现了ApplicationContextAware 接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring 上下文（同样这个方式也可以实现步骤4 的内容，但比4 更好，因为ApplicationContext 是BeanFactory 的子接口，有更多的实现方法） postProcessBeforeInitialization 接口实现-初始化预处理 如果这个Bean 关联了BeanPostProcessor 接口，将会调用postProcessBeforeInitialization(Object obj, String s)方法，BeanPostProcessor 经常被用作是Bean 内容的更改，并且由于这个是在Bean 初始化结束时调用那个的方法，也可以被应用于内存或缓存技术。 init-method 如果Bean 在Spring 配置文件中配置了init-method 属性会自动调用其配置的初始化方法。postProcessAfterInitialization 如果这个Bean 关联了BeanPostProcessor 接口，将会调用postProcessAfterInitialization(Object obj, String s)方法。注：以上工作完成以后就可以应用这个Bean 了，那这个Bean 是一个Singleton 的，所以一般情况下我们调用同一个id 的Bean 会是在内容地址相同的实例，当然在Spring 配置文件中也可以配置非Singleton。 Destroy 过期自动清理阶段 当Bean 不再需要时，会经过清理阶段，如果Bean 实现了DisposableBean 这个接口，会调用那个其实现的destroy()方法； destroy-method 自配置清理 最后，如果这个Bean 的Spring 配置中配置了destroy-method 属性，会自动调用其配置的销毁方法。 bean 标签有两个重要的属性（init-method 和destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct 和@PreDestroy）。1&lt;bean id="" class="" init-method="初始化方法" destroy-method="销毁方法"&gt; Spring依赖注入的四种方式构造器注入1234/*带参数，方便利用构造器进行注入*/public CatDaoImpl(String message)&#123; this. message = message;&#125; 123&lt;bean id="CatDaoImpl" class="com.CatDaoImpl"&gt; &lt;constructor-arg value=" message "&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; setter方法注入12345public class Id &#123; private int id; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125;&#125; 12&lt;bean id="id" class="com.id "&gt; &lt;property name="id" value="123"&gt;&lt;/property&gt;&lt;/bean&gt; 静态工厂注入静态工厂顾名思义，就是通过调用静态工厂的方法来获取自己需要的对象，为了让spring 管理所有对象，我们不能直接通过”工程类.静态方法()”来获取对象，而是依然通过spring 注入的形式获取：123456// 静态工厂public class DaoFactory &#123; public static final FactoryDao getStaticFactoryDaoImpl()&#123; return new StaticFacotryDaoImpl(); &#125;&#125; 12345678public class SpringAction &#123; // 注入对象 private FactoryDao staticFactoryDao; // 注入对象的 set 方法 public void setStaticFactoryDao(FactoryDao staticFactoryDao) &#123; this.staticFactoryDao = staticFactoryDao; &#125;&#125; 12345678&lt;!--factory-method="getStaticFactoryDaoImpl" 指定调用哪个工厂方法 --&gt;&lt;bean name="springAction" class=" SpringAction" &gt;&lt;!-- 使用静态工厂的方法注入对象，对应下面的配置文件 --&gt;&lt;property name="staticFactoryDao" ref="staticFactoryDao"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 此处获取对象的方式是从工厂类中获取静态方法 --&gt;&lt;bean name="staticFactoryDao" class="DaoFactory" factory-method="getStaticFactoryDaoImpl"&gt;&lt;/bean&gt; 实例工厂实例工厂的意思是获取对象实例的方法不是静态的，所以你需要首先new 工厂类，再调用普通的实例方法：123456// 实例工厂public class DaoFactory &#123; public FactoryDao getFactoryDaoImpl()&#123; return new FactoryDaoImpl(); &#125;&#125; 123456public class SpringAction &#123; private FactoryDao factoryDao; //注入对象 public void setFactoryDao(FactoryDao factoryDao) &#123; this.factoryDao = factoryDao; &#125;&#125; 12345678&lt;bean name="springAction" class="SpringAction"&gt;&lt;!--使用实例工厂的方法注入对象,对应下面的配置文件--&gt;&lt;property name="factoryDao" ref="factoryDao"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--此处获取对象的方式是从工厂类中获取实例方法--&gt;&lt;bean name="daoFactory" class="com.DaoFactory"&gt;&lt;/bean&gt;&lt;bean name="factoryDao" factory-bean="daoFactory" factory-method="getFactoryDaoImpl"&gt;&lt;/bean&gt; 5种自动装配方式Spring 装配包括手动装配和自动装配，手动装配是有基于xml 装配、构造方法、setter 方法等自动装配有五种自动装配的方式，可以用来指导Spring 容器用自动装配方式来进行依赖注入。 no：默认的方式是不进行自动装配，通过显式设置ref 属性来进行装配。 byName：通过参数名 自动装配，Spring 容器在配置文件中发现bean 的autowire 属性被设置成byname，之后容器试图匹配、装配和该bean 的属性具有相同名字的bean。 byType：通过参数类型自动装配，Spring 容器在配置文件中发现bean 的autowire 属性被设置成byType，之后容器试图匹配、装配和该bean 的属性具有相同类型的bean。如果有多个bean 符合条件，则抛出错误。 constructor：这个方式类似于byType， 但是要提供给构造器参数，如果没有确定的带参数的构造器参数类型，将会抛出异常。 autodetect：首先尝试使用constructor 来自动装配，如果无法工作，则使用byType 方式。 Spring AOP原理概念“横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其命名为”Aspect”，即切面。所谓”切面”，简单说就是那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块之间的耦合度，并有利于未来的可操作性和可维护性。使用”横切”技术，AOP 把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处基本相似，比如权限认证、日志、事物。AOP 的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。AOP 主要应用场景有： Authentication 权限 Caching 缓存 Context passing 内容传递 Error handling 错误处理 Lazy loading 懒加载 Debugging 调试 logging, tracing, profiling and monitoring 记录跟踪 优化 校准 Performance optimization 性能优化 Persistence 持久化 Resource pooling 资源池 Synchronization 同步 Transactions 事务 AOP核心概念 切面（aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象 横切关注点：对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点。 连接点（joinpoint）：被拦截到的点，因为Spring 只支持方法类型的连接点，所以在Spring 中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器。 切入点（pointcut）：对连接点进行拦截的定义 通知（advice）：所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类。 目标对象：代理的目标对象 织入（weave）：将切面应用到目标对象并导致代理对象创建的过程 引入（introduction）：在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段。 AOP两种代理方式Spring 提供了两种方式来生成代理对象: JDKProxy 和Cglib，具体使用哪种方式生成由AopProxyFactory 根据AdvisedSupport 对象的配置来决定。默认的策略是如果目标类是接口，则使用JDK 动态代理技术，否则使用Cglib 来生成代理。 JDK动态接口代理 JDK 动态代理主要涉及到java.lang.reflect 包中的两个类：Proxy 和InvocationHandler。InvocationHandler 是一个接口，通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编制在一起。Proxy 利用InvocationHandler 动态创建一个符合某一接口的实例，生成目标类的代理对象。 CGLib动态代理 ：CGLib 全称为Code Generation Library，是一个强大的高性能，高质量的代码生成类库，可以在运行期扩展Java 类与实现Java 接口，CGLib 封装了asm，可以再运行期动态生成新的class。和JDK 动态代理相比较：JDK 创建代理有一个限制，就是只能为接口创建代理实例，而对于没有通过接口定义业务方法的类，则可以通过CGLib 创建动态代理。 AOP实现原理12345678910111213141516171819202122232425@Aspectpublic class TransactionDemo &#123; @Pointcut(value="execution(* com.yangxin.core.service.*.*.*(..))") public void point()&#123; &#125; @Before(value="point()") public void before()&#123; System.out.println("transaction begin"); &#125; @AfterReturning(value = "point()") public void after()&#123; System.out.println("transaction commit"); &#125; @Around("point()") public void around(ProceedingJoinPoint joinPoint) throws Throwable&#123; System.out.println("transaction begin"); joinPoint.proceed(); System.out.println("transaction commit"); &#125;&#125; Spring MVC原理Spring 的模型-视图-控制器（MVC）框架是围绕一个DispatcherServlet 来设计的，这个Servlet会把请求分发给各个处理器，并支持可配置的处理器映射、视图渲染、本地化、时区与主题渲染等，甚至还能支持文件上传。 MVC流程 Http请求到DispatcherServlet(1) 客户端请求提交到DispatcherServlet。 HandlerMapping寻找处理器(2) 由DispatcherServlet 控制器查询一个或多个HandlerMapping，找到处理请求的Controller。 调用处理器Controller(3) DispatcherServlet 将请求提交到Controller。 Controller调用业务逻辑处理后，返回ModelAndView(4)(5)调用业务处理和返回结果：Controller 调用业务逻辑处理后，返回ModelAndView。 DispatcherServlet查询ModelAndView(6)(7)处理视图映射并返回模型： DispatcherServlet 查询一个或多个ViewResoler 视图解析器，找到ModelAndView 指定的视图。 ModelAndView反馈浏览器HTTP(8) Http 响应：视图负责将结果显示到客户端。 MVC常用注解 Spring Boot原理Spring Boot 是由Pivotal 团队提供的全新框架，其设计目的是用来简化新Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot 致力于在蓬勃发展的快速应用开发领域(rapid applicationdevelopment)成为领导者。其特点如下： 创建独立的Spring应用程序 嵌入的Tomcat，无需部署WAR文件 简化Maven配置 自动配置Spring 提供生产就绪型功能，如指标，健康检查和外部配置 绝对没有代码生成和对XML没有要求配置 JPA原理事务事务是计算机应用中不可或缺的组件模型，它保证了用户操作的原子性 ( Atomicity )、一致性( Consistency )、隔离性 ( Isolation ) 和持久性 ( Durabilily )。 本地事务紧密依赖于底层资源管理器（例如数据库连接 )，事务处理局限在当前事务资源内。此种事务处理方式不存在对应用服务器的依赖，因而部署灵活却无法支持多数据源的分布式事务。在数据库连接中使用本地事务示例如下：1234567891011121314151617181920212223public void transferAccount() &#123; Connection conn = null; Statement stmt = null; try&#123; conn = getDataSource().getConnection(); // 将自动提交，true则数据库将会把每一次数据更新认定为一个事务并自动提交 conn.setAutoCommit(false); stmt = conn.createStatement(); // A账号金额减少500 stmt.execute("update t_account set amount = amount - 500 where account_id = 'A'"); // B账户金额增加500 stmt.execute("update t_account set amount = amount + 500 where account_id = 'B'"); // 提交事务 conn.commit(); // 事务提交：转账的两步操作同时成功 &#125; catch(SQLException sqle)&#123; // 发生异常，回滚操作 conn.rollback(); // 事务回滚：转账两步操作撤销 stmt.close(); conn.close(); &#125;&#125; 分布式事务Java 事务编程接口（JTA：Java Transaction API）和 Java 事务服务 (JTS；Java TransactionService) 为 J2EE 平台提供了分布式事务服务。分布式事务（Distributed Transaction）包括事务管理器（ Transaction Manager ）和一个或多个支持 XA 协议的资源管理器 ( ResourceManager )。我们可以将资源管理器看做任意类型的持久化数据存储；事务管理器承担着所有事务参与单元的协调与控制。 12345678910111213141516171819202122public void transferAccount() &#123; UserTransaction userTx = null; Connection connA = null; Statement stmtA = null; Connection connB = null; Statement stmtB = null; try&#123; // 获得 Transaction 管理对象 userTx = (UserTransaction)getContext().lookup("java:comp/UserTransaction"); connA = getDataSourceA().getConnection();// 从数据库 A 中取得数据库连接 connB = getDataSourceB().getConnection();// 从数据库 B 中取得数据库连接 userTx.begin(); // 启动事务 stmtA = connA.createStatement();// 将 A 账户中的金额减少 500 stmtA.execute("update t_account set amount = amount - 500 where account_id = 'A'"); // 将 B 账户中的金额增加 500 stmtB = connB.createStatement(); stmtB.execute("update t_account set amount = amount + 500 where account_id = 'B'"); userTx.commit();// 提交事务 // 事务提交：转账的两步操作同时成功（数据库 A 和数据库 B 中的数据被同时更新） &#125; catch(SQLException exception)&#123; // 发生异常，回滚在本事务中的操纵 userTx.rollback();// 事务回滚：数据库 A 和数据库 B 中的数据更新被同时撤销 &#125; catch(Exception e)&#123; &#125;&#125; 两阶段提交两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做，所谓的两个阶段是指：第一阶段：准备阶段；第二阶段：提交阶段。 1 准备阶段事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare 消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo 和undo 日志，但不提交，到达一种“万事俱备，只欠东风”的状态。 2 提交阶段如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)将提交分成两阶段进行的目的很明确，就是尽可能晚地提交事务，让事务在提交前尽可能地完成所有能完成的工作。 TCC方案TCC的全程是：Try、Confirm、Cancel。 这个其实是用到了补偿的概念，分为了三个阶段： 1）Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留2）Confirm阶段：这个阶段说的是在各个服务中执行实际的操作3）Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作 给大家举个例子吧，比如说跨银行转账的时候，要涉及到两个银行的分布式事务，如果用TCC方案来实现，思路是这样的： 1）Try阶段：先把两个银行账户中的资金给它冻结住就不让操作了2）Confirm阶段：执行实际的转账操作，A银行账户的资金扣减，B银行账户的资金增加3）Cancel阶段：如果任何一个银行的操作执行失败，那么就需要回滚进行补偿，就是比如A银行账户如果已经扣减了，但是B银行账户资金增加失败了，那么就得把A银行账户资金给加回去 这种方案说实话几乎很少用人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。 比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，在资金上出现问题 比较适合的场景：这个就是除非你是真的一致性要求太高，是你系统中核心之核心的场景，比如常见的就是资金类的场景，那你可以用TCC方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否ok，不ok就执行补偿/回滚代码。 而且最好是你的各个业务执行的时间都比较短。 但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码很难维护。 本地消息表国外的ebay搞出来的这么一套思想 这个大概意思是这样的 1）A系统在自己本地一个事务里操作同时，插入一条数据到消息表2）接着A系统将这个消息发送到MQ中去3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止 这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的？？？这个会导致如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用 可靠消息最终一致性方案这个的意思，就是干脆不要用本地的消息表了，直接基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务。 大概的意思就是：1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了2）如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用RocketMQ支持的，要不你就自己基于类似ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的 Mybatis 缓存Mybatis 中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession 级别的缓存，当在同一个SqlSession 中进行相同的SQL 语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024 条SQL。二级缓存是指可以跨SqlSession 的缓存。是mapper 级别的缓存，对于mapper 级别的缓存不同的sqlsession 是可以共享的。 Mybatis的一级缓存原理(sqlsession 级别)第一次发出一个查询sql，sql 查询结果写入sqlsession 的一级缓存中，缓存使用的数据结构是一个map。key：MapperID+offset+limit+Sql+所有的入参value：用户信息同一个sqlsession 再次发出相同的sql，就从缓存中取出数据。如果两次中间出现commit 操作（修改、添加、删除），本sqlsession 中的一级缓存区域全部清空，下次再去缓存中查询不到所以要从数据库查询，从数据库查询到再写入缓存。 二级缓存原理(mapper 基本)二级缓存的范围是mapper 级别（mapper 同一个命名空间），mapper 以命名空间为单位创建缓存数据结构，结构是map。mybatis 的二级缓存是通过CacheExecutor 实现的。CacheExecutor其实是Executor 的代理对象。所有的查询操作，在CacheExecutor 中都会先匹配缓存中是否存在，不存在则查询数据库。key：MapperID+offset+limit+Sql+所有的入参具体使用需要配置： Mybatis 全局配置中启用二级缓存配置 在对应的Mapper.xml 中配置cache 节点 在对应的select 查询节点中添加useCache=true]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java CPU 100%排查问题与解决]]></title>
    <url>%2F2018%2F06%2F07%2F2018-06-07-java-cpu-100-resolve%2F</url>
    <content type="text"><![CDATA[前言记一次线上服务器Java程序CPU 100%排查问题与解决 top命令首先用top命令查看当前占用CPU高的进程(PID)1[mingdong@VM_45_31_centos V2]$ top 小写键盘的情况下，Shift+P键按照CPU占用由高到低排序打开大写键盘的情况下，直接按P键 top命令 可以看到占用CPU最高的进程PID是30672然后通过top -Hp 30672查看每个线程占用的CPU情况1[mingdong@VM_45_31_centos V2]$ top -Hp 30672 然后可以看到占用CPU最高的线程tid是30679得到tid之后再将tid转化为16进制30679转化为16进制就是0x77d7用jstack PID查看java虚拟机的堆栈情况1[mingdong@VM_45_31_centos V2]$ jstack 30672 1234567891011121314151617181920212223"Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007fe9d8114000 nid=0x77d8 in Object.wait() [0x00007fe9df6a1000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at java.lang.ref.Reference.tryHandlePending(Reference.java:191) - locked &lt;0x00000000fc0067d0&gt; (a java.lang.ref.Reference$Lock) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)"VM Thread" os_prio=0 tid=0x00007fe9d810a000 nid=0x77d7 runnable"Gang worker#0 (Parallel GC Threads)" os_prio=0 tid=0x00007fe9d801a800 nid=0x77d2 runnable"Gang worker#1 (Parallel GC Threads)" os_prio=0 tid=0x00007fe9d801c800 nid=0x77d3 runnable"Gang worker#2 (Parallel GC Threads)" os_prio=0 tid=0x00007fe9d801e000 nid=0x77d4 runnable"Gang worker#3 (Parallel GC Threads)" os_prio=0 tid=0x00007fe9d8020000 nid=0x77d5 runnable"Concurrent Mark-Sweep GC Thread" os_prio=0 tid=0x00007fe9d8063000 nid=0x77d6 runnable"VM Periodic Task Thread" os_prio=0 tid=0x00007fe9d8162000 nid=0x77e0 waiting on conditionJNI global references: 1281 可以看到nid=0x77d7是GC所在的线程, 原因是Parallel GC线程频繁Full GC导致CPU跑到100%。 再review代码，发现是有一段代码中的WebSocket发生异常时未关闭，导致内存泄露，频繁Full GC，最终CPU跑得太高。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2018%2F03%2F17%2F2018-03-17-java-virtual-machine%2F</url>
    <content type="text"><![CDATA[基本概念JVM是可运行Java 代码的假想计算机 ，包括一套字节码指令集、一组寄存器、一个栈、一个垃圾回收，堆和一个存储方法域。JVM 是运行在操作系统之上的，它与硬件没有直接的交互。 运行过程我们都知道Java 源文件，通过编译器，能够生产相应的.Class 文件，也就是字节码文件，而字节码文件又通过Java 虚拟机中的解释器，编译成特定机器上的机器码 。也就是如下：① Java 源文件—-&gt;编译器—-&gt;字节码文件② 字节码文件—-&gt;JVM—-&gt;机器码每一种平台的解释器是不同的，但是实现的虚拟机是相同的，这也就是Java 为什么能够跨平台的原因了 ，当一个程序从开始运行，这时虚拟机就开始实例化了，多个程序启动就会存在多个虚拟机实例。程序退出或者关闭，则虚拟机实例消亡，多个虚拟机实例之间数据不能共享。 线程JVM类加载机制加载(Loading)加载是类加载过程中的一个阶段，这个阶段会在内存中生成一个代表这个类的java.lang.Class 对象，作为方法区这个类的各种数据的入口。注意这里不一定非得要从一个Class 文件获取，这里既可以从ZIP 包中读取（比如从jar 包和war 包中读取），也可以在运行时计算生成（动态代理），也可以由其它文件生成（比如将JSP 文件转换成对应的Class 类）。 验证(Verification)这一阶段的主要目的是为了确保Class 文件的字节流中包含的信息是否符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 准备(Preparation)准备阶段是正式为类变量分配内存并设置类变量的初始值阶段，即在方法区中分配这些变量所使用的内存空间。注意这里所说的初始值概念，比如一个类变量定义为： 1public static int v = 8080; 实际上变量v 在准备阶段过后的初始值为0 而不是8080，将v 赋值为8080 的put static 指令是程序被编译后，存放于类构造器方法之中。 但是注意如果声明为：1public static final int v = 8080; 在编译阶段会为v 生成ConstantValue 属性，在准备阶段虚拟机会根据ConstantValue 属性将v赋值为8080。 解析(Resolution)解析阶段是指虚拟机将常量池中的符号引用替换为直接引用的过程。符号引用就是class 文件中的： CONSTANT_Class_info CONSTANT_Field_info CONSTANT_Method_info等类型的常量。 符号引用符号引用与虚拟机实现的布局无关，引用的目标并不一定要已经加载到内存中。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须是一致的，因为符号引用的字面量形式明确定义在Java 虚拟机规范的Class 文件格式中。 直接引用直接引用可以是指向目标的指针，相对偏移量或是一个能间接定位到目标的句柄。如果有了直接引用，那引用的目标必定已经在内存中存在。 初始化(Initialization)类构造器初始化阶段是执行类构造器方法的过程。方法是由编译器自动收集类中的类变量的赋值操作和静态语句块中的语句合并而成的。虚拟机会保证子方法执行之前，父类的方法已经执行完毕，如果一个类中没有对静态变量赋值也没有静态语句块，那么编译器可以不为这个类生成()方法。 注意以下几种情况不会执行类初始化： 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。 定义对象数组，不会触发该类的初始化。 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发定义常量所在的类。 通过类名获取Class 对象，不会触发类的初始化。 通过Class.forName 加载指定类时，如果指定参数initialize 为false 时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否要对类进行初始化。 通过ClassLoader 默认的loadClass 方法，也不会触发初始化动作。 使用(Using)卸载(Unloading)类加载器虚拟机设计团队把加载动作放到JVM外部实现，以便让应用程序决定如何获取所需的类，JVM 提供了3 种类加载器： 启动类加载器(Bootstrap ClassLoader)负责加载 JAVA_HOME\lib 目录中的，或通过-Xbootclasspath 参数指定路径中的，且被虚拟机认可（按文件名识别，如rt.jar）的类。 扩展类加载器(Extension ClassLoader)负责加载 JAVA_HOME\lib\ext 目录中的，或通过java.ext.dirs 系统变量指定路径中的类库。 应用程序类加载器(Application ClassLoader)负责加载用户路径（classpath）上的类库。JVM 通过双亲委派模型进行类的加载，当然我们也可以通过继承java.lang.ClassLoader实现自定义的类加载器。 双亲委派机制当一个类收到了类加载请求，他首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，每一个层次类加载器都是如此，因此所有的加载请求都应该传送到启动类加载其中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的Class），子类加载器才会尝试自己去加载。 采用双亲委派的一个好处是比如加载位于rt.jar 包中的类java.lang.Object，不管是哪个加载器加载这个类，最终都是委托给顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个Object 对象。 OSGI(动态模型系统)OSGi(Open Service Gateway Initiative)，是面向Java 的动态模型系统，是Java 动态化模块化系统的一系列规范。 动态改变构造OSGi 服务平台提供在多种网络设备上无需重启的动态改变构造的功能。为了最小化耦合度和促使这些耦合度可管理，OSGi 技术提供一种面向服务的架构，它能使这些组件动态地发现对方。 模块化编程与热插拔OSGi 旨在为实现Java 程序的模块化编程提供基础条件，基于OSGi 的程序很可能可以实现模块级的热插拔功能，当程序升级更新时，可以只停用、重新安装然后启动程序的其中一部分，这对企业级程序开发来说是非常具有诱惑力的特性。 OSGi 描绘了一个很美好的模块化开发目标，而且定义了实现这个目标的所需要服务与架构，同时也有成熟的框架进行实现支持。但并非所有的应用都适合采用OSGi 作为基础架构，它在提供强大功能同时，也引入了额外的复杂度，因为它不遵守了类加载的双亲委托模型。 Java内存模型JVM 内存区域主要分为线程私有区域【程序计数器、虚拟机栈、本地方法区】、线程共享区域【JAVA 堆、方法区】、直接内存。 线程私有数据区域生命周期与线程相同, 依赖用户线程的启动/结束 而 创建/销毁(在HotspotVM 内, 每个线程都与操作系统的本地线程直接映射, 因此这部分内存区域的存/否跟随本地线程的生/死对应)。 线程共享区域随虚拟机的启动/关闭而创建/销毁。 直接内存并不是JVM运行时数据区的一部分, 但也会被频繁的使用: 在JDK 1.4 引入的NIO 提供了基于Channel 与Buffer 的IO 方式, 它可以使用Native 函数库直接分配堆外内存, 然后使用DirectByteBuffer 对象作为这块内存的引用进行操作(详见: Java I/O 扩展), 这样就避免了在Java堆和Native 堆中来回复制数据, 因此在一些场景中可以显著提高性能。 程序计数器(线程私有)一块较小的内存空间, 是当前线程所执行的字节码的行号指示器，每条线程都要有一个独立的程序计数器，这类内存也称为“线程私有”的内存。 正在执行java 方法的话，计数器记录的是虚拟机字节码指令的地址（当前指令的地址）。如果还是Native 方法，则为空。 这个内存区域是唯一一个在虚拟机中没有规定任何OutOfMemoryError 情况的区域。 虚拟机栈(线程私有)是描述java 方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 栈帧(Frame）是用来存储数据和部分过程结果的数据结构，同时也被用来处理动态链接 (Dynamic Linking)、 方法返回值和异常分派（ Dispatch Exception）。栈帧随着方法调用而创建，随着方法结束而销毁——无论方法是正常完成还是异常完成（抛出了在方法内未被捕获的异常）都算作方法结束。 本地方法区(线程私有)本地方法区和Java Stack 作用类似, 区别是虚拟机栈为执行Java 方法服务, 而本地方法栈则为Native 方法服务, 如果一个VM 实现使用C-linkage 模型来支持Native 调用, 那么该栈将会是一个C 栈，但HotSpot VM直接就把本地方法栈和虚拟机栈合二为一。 堆(Heap-线程共享)-运行时数据区是被线程共享的一块内存区域，创建的对象和数组都保存在Java 堆内存中，也是垃圾收集器进行垃圾收集的最重要的内存区域。由于现代VM 采用分代收集算法, 因此Java 堆从GC 的角度还可以细分为: 新生代(Eden 区、From Survivor 区和To Survivor 区)和老年代。 方法区/永久代(线程共享)即我们常说的永久代(Permanent Generation), 用于存储被JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码等数据. HotSpot VM把GC分代收集扩展至方法区, 即使用Java堆的永久代来实现方法区, 这样HotSpot 的垃圾收集器就可以像管理Java 堆一样管理这部分内存,而不必为方法区开发专门的内存管理器(永久带的内存回收的主要目标是针对常量池的回收和类型的卸载, 因此收益一般很小)。 运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 Java 虚拟机对Class 文件的每一部分（自然也包括常量池）的格式都有严格的规定，每一个字节用于存储哪种数据都必须符合规范上的要求，这样才会被虚拟机认可、装载和执行。 JVM运行时内存Java 堆从GC 的角度还可以细分为: 新生代(Eden 区、From Survivor 区和To Survivor 区)和老年代。 新生代是用来存放新生的对象。一般占据堆的1/3 空间。由于频繁创建对象，所以新生代会频繁触发MinorGC 进行垃圾回收。新生代又分为 Eden 区、ServivorFrom、ServivorTo 三个区。 Eden区Java 新对象的出生地（如果新创建的对象占用内存很大，则直接分配到老年代）。当Eden 区内存不够的时候就会触发MinorGC，对新生代区进行一次垃圾回收。 SurvivorFrom上一次GC 的幸存者，作为这一次GC 的被扫描者 SurvivorTo保留了一次MinorGC 过程中的幸存者。 MinorGC的过程(复制-&gt;清空-&gt;互换)MinorGC 采用复制算法。 因为新生代存活率低，适合复制算法 (1)eden、survivorFrom复制到SurvivorTo，年龄+1首先，把Eden 和SurvivorFrom区域中存活的对象复制到SurvivorTo 区域（如果有对象的年龄以及达到了老年的标准，则赋值到老年代区），同时把这些对象的年龄+1（如果SurvivorTo 不够位置了就放到老年区) (2)清空eden、SurvivorFrom然后，清空Eden 和SurvivorFrom 中的对象； (3)SurvivorTo 􀪼 SurvivorFrom互换最后，SurvivorTo 和SurvivorFrom 互换，原SurvivorTo 成为下一次GC 时的SurvivorFrom。 老年代主要存放应用程序中生命周期长的内存对象。 老年代的对象比较稳定，所以MajorGC 不会频繁执行。在进行MajorGC 前一般都先进行了一次MinorGC，使得有新生代的对象晋身入老年代，导致空间不够用时才触发。当无法找到足够大的连续空间分配给新创建的较大对象时也会提前触发一次MajorGC 进行垃圾回收腾出空间。 MajorGC 采用标记清除算法：首先扫描一次所有老年代，标记出存活的对象，然后回收没有标记的对象。MajorGC 的耗时比较长，因为要扫描再回收。MajorGC 会产生内存碎片，为了减少内存损耗，我们一般需要进行合并或者标记出来方便下次直接分配。当老年代也满了装不下的时候，就会抛出OOM（Out of Memory）异常。 永久代指内存的永久保存区域，主要存放Class 和Meta（元数据）的信息,Class 在被加载的时候被放入永久区域，它和和存放实例的区域不同,GC 不会在主程序运行期对永久区域进行清理。所以这也导致了永久代的区域会随着加载的Class 的增多而胀满，最终抛出OOM异常。 Java8与元数据在Java8 中，永久代已经被移除，被一个称为“元数据区”（元空间）的区域所取代。元空间的本质和永久代类似，元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。类的元数据放入 nativememory, 字符串池和类的静态变量放入java 堆中，这样可以加载多少类的元数据就不再由MaxPermSize 控制, 而由系统的实际可用空间来控制。 垃圾回收与算法 如何确定垃圾引用计数法在Java 中，引用和对象是有关联的。如果要操作对象则必须用引用进行。因此，很显然一个简单的办法是通过引用计数来判断一个对象是否可以被回收。每个对象实例都有一个引用计数器，被引用则+1，完成引用则-1。任何引用计数为0的对象实例可以被当作垃圾收集。优点：执行效率高，程序执行受影响较小 缺点：无法检测出循环引用，导致内存泄露 可达性分析为了解决引用计数法的循环引用问题，Java 使用了可达性分析的方法。通过一系列的“GC roots”对象作为起点搜索。如果在“GC roots”和一个对象之间没有可达路径，则称该对象是不可达的。要注意的是，不可达对象不等价于可回收对象，不可达对象变为可回收对象至少要经过两次标记过程。两次标记后仍然是可回收对象，则将面临回收。可以作为GC Root的对象 虚拟机栈中引用的对象（栈帧中的本地变量表) 方法区中的常量引用的对象 方法区中的类静态属性引用的对象 本地方法栈中JNI(Native方法)的引用对象 活跃线程的引用对象 垃圾回收算法标记清除算法(Mark-Sweep)最基础的垃圾回收算法，分为两个阶段，标注和清除。标记阶段标记出所有需要回收的对象，清除阶段回收被标记的对象所占用的空间。如图从图中我们就可以发现，该算法 最大的问题 是内存碎片化严重，后续可能发生大对象不能找到可利用空间的问题。 复制算法为了解决Mark-Sweep 算法内存碎片化的缺陷而被提出的算法。按内存容量将内存划分为等大小的两块。每次只使用其中一块，当这一块内存满后将尚存活的对象复制到另一块上去，把已使用的内存清掉，如图：优点： 解决碎片化问题 顺序分配内存，简单高效 使用于对象存活率低的场景 缺点： 可用内存被压缩为原来的一般 存活对象多的情况下效率低 这种算法虽然实现简单，内存效率高，不易产生碎片，但是最大的问题是可用内存被压缩到了原本的一半。且存活对象增多的话，Copying 算法的效率会大大降低。 标记整理算法(Mark-Compact)结合了以上两个算法，为了避免缺陷而提出。标记阶段和Mark-Sweep 算法相同，标记后不是清理对象，而是将存活对象移向内存的一端。然后清除端边界外的对象。如图： 分代收集算法分代收集法是目前大部分JVM 所采用的方法，其核心思想是根据对象存活的不同生命周期将内存划分为不同的域，一般情况下将GC 堆划分为老生代(Tenured/Old Generation)和新生代(YoungGeneration)。老生代的特点是每次垃圾回收时只有少量对象需要被回收，新生代的特点是每次垃圾回收时都有大量垃圾需要被回收，因此可以根据不同区域选择不同的算法。 新生代与复制算法Minor GC目前大部分JVM的GC 对于新生代都采取Copying 算法，因为新生代中每次垃圾回收都要回收大部分对象，即要复制的操作比较少，但通常并不是按照1：1 来划分新生代。一般将新生代划分为一块较大的Eden 空间和两个较小的Survivor 空间(From Space, To Space)，每次使用Eden 空间和其中的一块Survivor 空间，当进行回收时，将该两块空间中还存活的对象复制到另一块Survivor 空间中。 老年代与标记整理算法MajorGC/Full GC而老年代因为每次只回收少量对象，因而采用Mark-Compact 算法。 JAVA 虚拟机提到过的处于方法区的永生代(Permanet Generation)，它用来存储class 类，常量，方法描述等。对永生代的回收主要包括废弃常量和无用的类。 对象的内存分配主要在新生代的Eden Space 和Survivor Space 的From Space(Survivor 目前存放对象的那一块)，少数情况会直接分配到老生代。 当新生代的Eden Space 和From Space 空间不足时就会发生一次GC，进行GC 后，EdenSpace 和From Space 区的存活对象会被挪到To Space，然后将Eden Space 和FromSpace 进行清理。 如果To Space 无法足够存储某个对象，则将这个对象存储到老生代。 在进行GC 后，使用的便是Eden Space 和To Space 了，如此反复循环。 当对象在Survivor 区躲过一次GC 后，其年龄就会+1。默认情况下年龄到达15 的对象会被移到老生代中。 触发Full GC的条件 老年代空间不足 永久代空间不足(JDK1.8前,1.8后永久代换成了元空间MetaSpace) 调用System.gc() CMS GC时出现promotion failed, concurrent mode failure Minor GC晋升到老年代的平均大小大于老年代的剩余空间 使用RMI来进行RPC或管理的JDK应用，每小时执行1次Full GC Java四种引用类型强引用(Strong Reference)在Java 中最常见的就是强引用，把一个对象赋给一个引用变量，这个引用变量就是一个强引用。当一个对象被强引用变量引用时，它处于可达状态，它是不可能被垃圾回收机制回收的，即使该对象以后永远都不会被用到JVM也不会回收。因此强引用是造成Java 内存泄漏的主要原因之一。通过将对象设置为null来弱化引用，使其可被回收。12Object obj = new Object(); // Strong Referenceobj = null; // help gc 软引用(Soft Reference)软引用需要用SoftReference 类来实现，对于只有软引用的对象来说，当系统内存足够时它不会被回收，当系统内存空间不足时它会被回收。软引用通常用在对内存敏感的程序中。可以用来实现高速缓存。12String str = new String("abc"); // Strong ReferenceSoftReference&lt;String&gt; softRef = new SoftReference&lt;&gt;(str); // Soft Reference 弱引用(Weak Reference)弱引用需要用WeakReference 类来实现，它比软引用的生存期更短，对于只有弱引用的对象来说，只要垃圾回收机制扫描到弱引用对象，不管JVM 的内存空间是否足够，总会回收该对象占用的内存。适用于偶尔被使用且不影响垃圾收集的对象。12String str = new String("abc"); // Strong ReferenceWeakReference&lt;String&gt; weakRef = new WeakReference&lt;&gt;(str); // Weak Reference 虚引用(Phantom Reference)虚引用需要PhantomReference 类来实现，任何适合都可能被垃圾收集器回收，它不能单独使用，必须和引用队列(ReferenceQueue)联合使用。虚引用的主要作用是跟踪对象被垃圾回收的状态，起哨兵作用。123String str = new String("abc"); // Strong ReferenceReferenceQueue queue = new ReferenceQueue();PhantomReference&lt;String&gt; ref = new PhantomReference&lt;&gt;(str,queue); // Phantom Reference 引用类型 被垃圾回收时间 用途 生存时间 强引用 从来不会 对象的一般状态 JVM停止运行时终止 软引用 内存不足时 对象缓存 内存不足时终止 弱引用 垃圾回收时 对象缓存 gc运行后终止 虚引用 Unknown 标记、哨兵 Unknown GC分代收集算法VS分区收集算法分代收集算法当前主流VM 垃圾收集都采用”分代收集”(Generational Collection)算法, 这种算法会根据对象存活周期的不同将内存划分为几块, 如JVM中的 新生代、老年代、永久代，这样就可以根据各年代特点分别采用最适当的GC 算法 在新生代-复制算法每次垃圾收集都能发现大批对象已死, 只有少量存活. 因此选用复制算法, 只需要付出少量存活对象的复制成本就可以完成收集。 在老年代-标记整理算法因为对象存活率高、没有额外空间对它进行分配担保, 就必须采用“标记—清理”或“标记—整理”算法来进行回收, 不必进行内存复制, 且直接腾出空闲内存。 分区收集算法分区算法则将整个堆空间划分为连续的不同小区间, 每个小区间独立使用, 独立回收. 这样做的好处是可以控制一次回收多少个小区间 , 根据目标停顿时间, 每次合理地回收若干个小区间(而不是整个堆), 从而减少一次GC 所产生的停顿。 GC垃圾收集器Java 堆内存被划分为新生代和年老代两部分，新生代主要使用复制和标记-清除垃圾回收算法；年老代主要使用标记-整理垃圾回收算法，因此java 虚拟中针对新生代和年老代分别提供了多种不同的垃圾收集器，JDK1.6 中Sun HotSpot 虚拟机的垃圾收集器如下： Serial收集器(单线程、复制算法 -XX:+UseSerialGC)Serial（英文连续）是最基本垃圾收集器，使用复制算法，曾经是JDK1.3.1 之前新生代唯一的垃圾收集器。Serial 是一个单线程的收集器，它不但只会使用一个CPU 或一条线程去完成垃圾收集工作，并且在进行垃圾收集的同时，必须暂停其他所有的工作线程，直到垃圾收集结束。Serial 垃圾收集器虽然在收集垃圾过程中需要暂停所有其他的工作线程，但是它简单高效，收集时间一般在几十毫秒左右，对于限定单个CPU 环境来说，没有线程交互的开销，可以获得最高的单线程垃圾收集效率，因此Serial垃圾收集器依然是java 虚拟机运行在Client 模式下默认的新生代垃圾收集器。 ParNew收集器(Serial+多线程 -XX:+UseParNewGC)ParNew 垃圾收集器其实是Serial 收集器的多线程版本，也使用复制算法，除了使用多线程进行垃圾收集之外，其余的行为和Serial 收集器完全一样，ParNew 垃圾收集器在垃圾收集过程中同样也要暂停所有其他的工作线程。ParNew 收集器默认开启和CPU 数目相同的线程数，可以通过-XX:ParallelGCThreads 参数来限制垃圾收集器的线程数。【Parallel：平行的】ParNew虽然是除了多线程外和Serial 收集器几乎完全一样，但是ParNew垃圾收集器是很多java虚拟机运行在Server 模式下新生代的默认垃圾收集器。 Parallel Scavenge收集器(多线程复制算法、高效 -XX:+UseParallelGC)Parallel Scavenge 收集器也是一个新生代垃圾收集器，同样使用复制算法，也是一个多线程的垃圾收集器，它重点关注的是程序达到一个可控制的吞吐量（Thoughput，CPU 用于运行用户代码的时间/CPU 总消耗时间，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)），高吞吐量可以最高效率地利用CPU 时间，尽快地完成程序的运算任务，主要适用于在后台运算而不需要太多交互的任务。自适应调节策略(-XX:+UseAdaptiveSizePolicy)也是ParallelScavenge 收集器与ParNew 收集器的一个重要区别。Parallel Scavenge是java虚拟机运行在Server 模式下年轻代的默认垃圾收集器。 Serial Old收集器(单线程标记整理算法 -XX:+UseSerialOldGC)Serial Old 是Serial 垃圾收集器年老代版本，它同样是个单线程的收集器，使用标记-整理算法，这个收集器也主要是运行在Client 默认的java 虚拟机默认的年老代垃圾收集器。 Parallel Old收集器(多线程标记整理算法 -XX:+UseParallelOldGC)Parallel Old 收集器是Parallel Scavenge 的年老代版本，使用多线程的标记-整理算法，在JDK1.6才开始提供。在JDK1.6 之前，新生代使用ParallelScavenge 收集器只能搭配年老代的Serial Old 收集器，只能保证新生代的吞吐量优先，无法保证整体的吞吐量，Parallel Old 正是为了在年老代同样提供吞吐量优先的垃圾收集器，如果系统对吞吐量要求比较高，可以优先考虑新生代Parallel Scavenge和年老代Parallel Old 收集器的搭配策略。 CMS收集器(多线程标记清除算法)Concurrent mark sweep(CMS)收集器是一种年老代垃圾收集器，其最主要目标是获取最短垃圾回收停顿时间，和其他年老代使用标记-整理算法不同，它使用多线程的标记-清除算法。最短的垃圾收集停顿时间可以为交互比较高的程序提高用户体验。CMS 工作机制相比其他的垃圾收集器来说更复杂，整个过程分为以下阶段： 初始标记（CMS initial mark)(Stop The World)暂停所有工作线程 并发标记（CMS concurrent mark）并发追溯标记，程序不会停顿 并发预清理（CMS-concurrent-preclean)查找执行并发标记阶段从年轻代晋升到老年代的对象 重新标记（CMS remark)(Stop The World)暂停虚拟机，扫描CMS堆中的剩余对象 并发清除（CMS concurrent sweep)清理垃圾对象，和用户线程一起工作，程序不会停顿 并发重置（CMS-concurrent-reset)重置CMS收集器的数据结构 G1收集器Garbage First 垃圾收集器是目前垃圾收集器理论发展的最前沿成果，可用于年轻代、老年代，G1 收集器优点是： 并行和并发 分代收集 基于标记-整理算法，不产生内存碎片。 可以非常精确控制停顿时间，在不牺牲吞吐量前提下，实现低停顿垃圾回收。 G1 收集器避免全区域垃圾收集，它把堆内存划分为大小固定的几个独立区域，并且跟踪这些区域的垃圾收集进度，同时在后台维护一个优先级列表，每次根据所允许的收集时间，优先回收垃圾最多的区域。区域划分和优先级区域回收机制，确保G1 收集器可以在有限时间获得最高的垃圾收集效率。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next主题配置]]></title>
    <url>%2F2018%2F02%2F22%2F2018-02-22-hexo-next-setting%2F</url>
    <content type="text"><![CDATA[1. 搜索功能1.1. 在Hexo根目录下执行 npm install hexo-generator-searchdb –save1.2. blog下的_config.yml文件编辑12345search: path: search.xml field: post format: html limit: 10000 1.3. /blog/themes/next下的_config.yml文件编辑12local_search: enable: true 2. RSS订阅2.1. 在Hexo根目录下执行 npm install hexo-generator-feed –save2.2. blog下的_config.yml文件编辑123456789# RSS订阅plugin:- hexo-generator-feed# RSSfeed: type: atom path: atom.xml limit: 20 2.3. /blog/themes/next下的_config.yml文件编辑1rss: /atom.xml 发布到Github在Hexo根目录下执行 npm install –save hexo-deployer-git自动关联域名 创建../Hexo根目录/source/CNAME 内容为域名，如：www.mingdong.onlinehexo d文章字数统计和阅读时间统计https://github.com/theme-next/hexo-symbols-count-time在Hexo根目录下执行 npm install hexo-symbols-count-time –saveblog下的_config.yml文件编辑12345symbols_count_time: symbols: true time: true total_symbols: true total_time: true 博客中插入图片在Hexo根目录下执行 npm install hexo-asset-image –saveblog下的_config.yml文件编辑1post_asset_folder: true live2dhttps://github.com/EYHN/hexo-helper-live2d/blob/master/README.md安装live2d 在Hexo根目录下执行 npm install –save hexo-helper-live2d安装模块包 在Hexo根目录下执行 npm install {packagename} 如:npm install live2d-widget-model-wankoblog下的_config.yml文件编辑12345678910111213141516171819# Live2D## https://github.com/EYHN/hexo-helper-live2dlive2d: enable: true # enable: false scriptFrom: local # 默认 pluginRootPath: live2dw/ # 插件在站点上的根目录(相对路径) pluginJsPath: lib/ # 脚本文件相对与插件根目录路径 pluginModelPath: assets/ # 模型文件相对与插件根目录路径 # scriptFrom: jsdelivr # jsdelivr CDN # scriptFrom: unpkg # unpkg CDN # scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 你的自定义 url tagMode: false # 标签模式, 是否仅替换 live2d tag标签而非插入到所有页面中 debug: false # 调试, 是否在控制台输出日志 model: use: live2d-widget-model-wanko # npm-module package name # use: wanko # 博客根目录/live2d_models/ 下的目录名 # use: ./wives/wanko # 相对于博客根目录的路径 # use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 你的自定义 url aplayerhttps://github.com/MoePlayer/hexo-tag-aplayer在Hexo根目录下执行 npm install –save hexo-tag-aplayer使用 console.error("Error: [hexo-tag-aplayer] Specified asset file not found ([picture_url,)"); title : 曲目标题 author: 曲目作者 url: 音乐文件 URL 地址 picture_url: (可选) 音乐对应的图片地址 narrow: （可选）播放器袖珍风格 autoplay: (可选) 自动播放，移动端浏览器暂时不支持此功能 width:xxx: (可选) 播放器宽度 (默认: 100%) lrc:xxx: （可选）歌词文件 URL 地址当开启 Hexo 的 文章资源文件夹post_asset_folder: true 功能时，可以将图片、音乐文件、歌词文件放入与文章对应的资源文件夹中，然后直接引用： console.error("Error: [hexo-tag-aplayer] Specified asset file not found (picture.jpg)");]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL加强]]></title>
    <url>%2F2017%2F10%2F22%2F2017-10-22-mysql-plus%2F</url>
    <content type="text"><![CDATA[索引为什么使用索引海量数据全表扫描慢，快速查询数据 什么信息能成为索引主键、唯一键 索引的数据结构二叉查找树B-TreeB+-Tree(MySQL) 有m个子树的节点包含有m个元素（B-Tree中是m-1） 所有叶子节点之间都有一个链指针指向下一个节点。 根节点和分支节点中不保存数据，只用于索引，所有数据都保存在叶子节点中。 所有分支节点和根节点都同时存在于子节点中，在子节点元素中是最大或者最小的元素。 优点： 磁盘读写代价更低 查询效率更加稳定 更有利于对数据库的扫描Hash效率高 缺点 Hash索引比较的是Hash算法后的Hash值 只能做等值确定，仅仅能满足”=”，”IN”，不能使用范围查询 无法用于数据排序操作 不能利用部分索引键查询 不能避免表扫描 大量Hash值相等的情况性能可能更低 BitMapOracle数据库使用 索引模块密集索引密集索引文件中的每一个索引码值都对应一个索引值，决定表的排列顺序 稀疏索引稀疏索引文件只为索引码的某些值建立索引项，叶子结点仅键位信息及其主键，定位到叶子结点还要进一步才能定位到信息 MyISAM稀疏索引 索引和数据分开存储索引: .MYI数据: .MYD InnoDB密集索引 若一个主键被定义，该主键则作为密集索引若没有主键被定义，该表的第一个唯一非空索引则作为密集索引若不满足以上条件，InnoDB内存会生成一个隐藏主键（密集索引）非主键索引存储相关键位和其对应的主键值，包含两次查找 索引和数据共同存储: *.ibd 索引的问题如何定位并优化慢查询Sql根据慢日志定位慢查询sqlshow variable like ‘%quer%’; // 查看慢查询日志是否开启show status like ‘%slow_queries%’; // 查询慢sql语句次数set global slow_query_log = on; // 打开慢查询日志set global long_query_time = 1; // 设置列入慢查询的超时时间，1秒 使用explain等工具分析Sqlexplain + select … type index&gt;all 表示全表扫描，需要优化sql extra Using filesort:外部文件索引排序，不是通过构建的索引，远慢于索引, Using temporary，对查询结果排序时使用临时表，常见于order by，group by，这两个需要优化修改sql或者让sql走索引1.添加PRIMARY KEY（主键索引）mysql&gt;ALTER TABLE table_name ADD PRIMARY KEY ( column )2.添加UNIQUE(唯一索引)mysql&gt;ALTER TABLE table_name ADD UNIQUE (column)3.添加INDEX(普通索引)mysql&gt;ALTER TABLE table_name ADD INDEX index_name ( column )4.添加FULLTEXT(全文索引)mysql&gt;ALTER TABLE table_name ADD FULLTEXT ( column)5.添加多列索引mysql&gt;ALTER TABLE table_name ADD INDEX index_name ( column1, column2, column3 ) 联合索引的最左匹配原则的成因 最左前缀匹配原则，mysql会一直向右匹配直到遇到范围查询（&gt;、&lt;、between、like)就停止匹配，比如a=3 and b=4 c&gt;5 and d=6 如果简历(a,b,c,d)顺序的索引,d是用不到索引的，如果简历(a,b,d,c)索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a=1 and b=2 and c=3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 成因结合MySQL的B+Tree结构，最左的索引在根节点，之后的索引都是非根节点，直接查找非根节点不通过根节点无法直接到达叶子结点，只能全表扫描。通过B+Tree的结构索引查找，只能从根节点往子节点最后查找到叶子结点。 索引越多越好吗 数据量小的表不需要建立索引，建立会增加额外的索引开销 数据变更需要维护索引，更多的索引需要更多维护成本 更多的索引需要更多的空间 索引失效的情况 条件中用or，即使其中有条件带索引，也不会使用索引查询（这就是查询尽量不要用or的原因，用in吧）注意：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 对于多列索引，不是使用的第一部分，则不会使用索引 like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 MyISAM与InnoDBMyISAM与InnoDB关于锁方面的区别锁的分类 按锁的粒度划分，可分为表级锁、行级锁、页级锁 按锁级别划分，可分为共享锁、排他锁 按加锁方式划分，可分为自动锁、显示锁 按操作划分，可分为DML锁（增删改）、DDL锁（表结构改变，ALTER） 按使用方式划分，可分为乐观锁、悲观锁MyISAM默认用的是表级锁，不支持行级锁锁住整张表 读锁：共享锁lock tables table_name read; 写锁：排他锁 InnoDB默认用的是行级锁，也支持表级锁默认支持事务，二段锁，加锁解锁，事务自动提交锁 不走索引的查询会锁住整张表，也就是表级锁没有用到索引是表级锁，用到索引是行级锁 MyISAM适合的场景频繁执行全表count语句，保存了一个表的行数对数据进行增删改的频率不高，查询非常频繁没有事务 InnoDB适合的场景数据增删改查都相当频繁可靠性要求比较高，要求支持事务 数据库事务的四大特性⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 MySQL事务的隔离级别事务并发访问引起的问题更新丢失–mysql所有事务可以避免 脏读 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。 当一个事务正在多次修改某个数据，而在这个事务中这多次的修改都还未提交，这时一个并发的事务来访问该数据，就会造成两个事务得到的数据不一致。例如：用户A向用户B转账100元，对应SQL命令如下123update account set money=money+100 where name=’B’; (此时A通知B)update account set money=money - 100 where name=’A’; 当只执行第一条SQL时，A通知B查看账户，B发现确实钱已到账（此时即发生了脏读），而之后无论第二条SQL是否执行，只要该事务不提交，则所有操作都将回滚，那么当B以后再次查看账户时就会发现钱其实并没有转。 不可重复读 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 在某些情况下，不可重复读并不是问题，比如我们多次查询某个数据当然以最后查询得到的结果为主。但在另一些情况下就有可能发生问题，例如对于同一个数据A和B依次查询就可能不同，A和B就可能打起来了…… 幻读 幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 MySQL的四种隔离级别Read Uncommitted (读未提交)Read Committed (读已提交)Repeatable Read (可重复读)Serializable (串行化) ① Read Uncommitted (读未提交)：最低级别，任何情况都无法保证。 ② Read Committed (读已提交)：可避免脏读的发生。 ③ Repeatable Read (可重复读)：可避免脏读、不可重复读的发生。 ④ Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 在MySQL数据库中，支持上面四种隔离级别，默认的为Repeatable read (可重复读)；而在Oracle数据库中，只支持Serializable (串行化)级别和Read committed (读已提交)这两种级别，其中默认的为Read committed级别。 在MySQL数据库中查看当前事务的隔离级别：1select @@tx_isolation; 在MySQL数据库中设置事务的隔离 级别：123456 set [glogal | session] transaction isolation level 隔离级别名称; set tx_isolation=’隔离级别名称;’select @@tx_isolation; // 查看mysql事务隔离级别set session transaction isolation level read uncommited; // 设置mysql事务隔离级别 关键语法GROUP BYHAVINGCOUTSUMMAXMINAVG]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存]]></title>
    <url>%2F2017%2F08%2F06%2F2017-08-06-cache-redis%2F</url>
    <content type="text"><![CDATA[为啥在项目里要用缓存呢？用缓存，主要是俩用途，高性能和高并发 高性能假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作mysql，半天查出来一个结果，耗时600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？ 缓存啊，折腾600ms查出来的结果，扔缓存里，一个key对应一个value，下次再有人查，别走mysql折腾600ms了。直接从缓存里，通过一个key查出来一个value，2ms搞定。性能提升300倍。 这就是所谓的高性能。 就是把你一些复杂操作耗时查出来的结果，如果确定后面不咋变了，然后但是马上还有很多读请求，那么直接结果放缓存，后面直接读缓存就好了。 高并发mysql这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql单机支撑到2000qps也开始容易报警了。 所以要是你有个系统，高峰期一秒钟过来的请求有1万，那一个mysql单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放mysql。缓存功能简单，说白了就是key-value式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发so easy。单机承载并发量是mysql单机的几十倍。 用了缓存之后会有啥不良的后果1）缓存与数据库双写不一致2）缓存雪崩3）缓存穿透4）缓存并发竞争 redis和memcached有啥区别Redis支持服务器端的数据操作：Redis相比Memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，通常在Memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。在Redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果需要缓存能够支持更复杂的结构和操作，那么Redis会是不错的选择。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是redis目前是原生支持cluster模式的，redis官方就是支持redis cluster集群模式的，比memcached来说要更好 redis单线程为什么会是高性能的redis基于reactor模式开发了网络事件处理器，这个处理器叫做文件事件处理器，file event handler。这个文件事件处理器是单线程的，redis才叫做单线程的模型，采用IO多路复用机制同时监听多个socket，根据socket上的事件来选择对应的事件处理器来处理这个事件。 1）纯内存操作2）核心是基于非阻塞的IO多路复用机制3）单线程反而避免了多线程的频繁上下文切换问题上下文切换会导致CPU在寄存器和运行队列之间来回奔波，CPU寄存器需要保存和加载, 系统调度器的代码需要执行。 redis的数据类型StringHashListSetSorted setpub/subTransactions redis的过期策略设置过期时间的删除方式定期删除所谓定期删除，指的是redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。假设redis里放了10万个key，都设置了过期时间，你每隔几百毫秒，就检查10万个key，那redis基本上就死了，cpu负载会很高的，消耗在你的检查过期key上了。注意，这里可不是每隔100ms就遍历所有的设置过期时间的key，那样就是一场性能上的灾难。实际上redis是每隔100ms随机抽取一些key来检查和删除的。 惰性删除惰性删除了。这就是说，在你获取某个key的时候，redis会检查一下 ，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 内存淘汰机制如果定期删除漏掉了很多过期key，然后也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽。内存淘汰机制可以防止上述情况的发生。 1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧 2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） 3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的key给干掉啊 4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key（这个一般不太合适） 5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key 6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除 LRU算法利用已有的jdk数据结构实现一个java版的LRU123456789101112public class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private final int CACHE_SIZE; // 这里就是传递进来最多能缓存多少数据 public LRUCache(int cacheSize) &#123; super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); // 这块就是设置一个hashmap的初始大小，同时最后一个true指的是让linkedhashmap按照访问顺序来进行排序，最近访问的放在头，最老访问的就在尾 CACHE_SIZE = cacheSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; CACHE_SIZE; // 这个意思就是说当map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据 &#125;&#125; redis高并发主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万QPS，多从用来查询数据，多个从实例可以提供每秒10万的QPS。并且支持水平扩展，QPS高了只要加机器就可以 redis高并发的同时，还需要容纳大量的数据：一主多从，每个实例都容纳了完整的数据，比如redis主就10G的内存量，其实你就最对只能容纳10g的数据量。如果你的缓存要容纳的数据量很大，达到了几十g，甚至几百g，或者是几t，那你就需要redis集群，而且用redis集群之后，可以提供可能每秒几十万的读写并发。 redis高可用如果你做主从架构部署，其实就是加上哨兵就可以了，就可以实现，任何一个实例宕机，自动会进行主备切换。 redis缓存一致性redis持久化redis clusterredis 哨兵模式(Sentinel)解决主从同步Master宕机后的主从切换问题：监控：检查主从服务器是否运行正常提醒：通过api向管理员或者其他应用程序发送故障通知自动故障迁移：主从切换 缓存雪崩事前：redis高可用，主从+哨兵，redis cluster，避免全盘崩溃事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL被打死事后：redis持久化，快速恢复缓存数据 缓存穿透黑客发送缓存和数据库里都不存在的数据，然后每次都会到数据库中查询，性能消耗很大。比如数据的id都是从1开始的，黑客的恶意请求id都是负数，请求到缓存中查不到，接着到数据库中查询，还是查不到，下次进行相同的请求也是会到数据库中查询。 解决方案： 对id先进行校验，不符合规则的id直接返回 将黑客的查询id也放入缓存中，key为id,value为空。 缓存与数据的双写一致性不管是先写库，再删除缓存；还是先删缓存，再写库，都有可能出现数据不一致的情况 因为写和读是并发的，没法保证顺序，如果删了缓存，还没有来得及写库，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。如果先写了库，再删除缓存前，写库的线程宕机了，没有删除掉缓存，则也会出现数据不一致情况。 如果是redis集群，或者主从模式，写主读从，由于redis复制存在一定的时间延迟，也有可能导致数据不一致。 优化思路双删加超时 在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。这样最差的情况是在超时时间内存在不一致，当然这种情况极其少见，可能的原因就是服务宕机。此种情况可以满足绝大多数需求。 当然这种策略要考虑redis和数据库主从同步的耗时，所以在第二次删除前最好休眠一定时间，比如500毫秒，这样毫无疑问又增加了写请求的耗时 redis并发竞争问题线上非常常见的一个问题，就是多客户端同时并发写一个key，可能本来应该先到的数据后到了，导致数据版本错了。或者是多客户端同时获取一个key，修改值之后再写回去，只要顺序错了，数据就错了。 setnx 而且redis自己就有天然解决这个问题的CAS类的乐观锁方案 分布式锁]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hashCode and equals]]></title>
    <url>%2F2017%2F05%2F06%2F2017-05-06-hashCode-and-equals%2F</url>
    <content type="text"><![CDATA[hashCode()方法和equals()方法的区别和联系很多人觉得很简单，但是要回答的时候却答得差强人意。正确的回答应该是先介绍hashCode()和equals()方法的作用是什么，然后才说他的区别，说了区别之后再说使用的时候需要注意到的地方。如果你在了解一些其他人不知道的那就更好了！下边我们就开始介绍 hashCode()和equals()是什么hashCode()方法和equals()方法的作用其实一样，在Java里都是用来对比两个对象是否相等一致。 hashCode()和equals()的区别hashCodehashCode()效率高，但并不是完全可靠的，有时候不同的对象他们生成的hashcode也会一样（生成hash值得公式可能存在的问题），所以hashCode()只能说是大部分时候可靠，并不是绝对可靠。 equalsequals()是完全可靠的，但是重写的equals()里一般比较的比较全面比较复杂，这样效率就比较低，而利用hashCode()进行对比，则只要生成一个hash值进行比较就可以了，效率很高。 总结 equals()相等的两个对象他们的hashCode()肯定相等，也就是用equals()对比是绝对可靠的，equals()性能较差。 hashCode()相等的两个对象他们的equals()不一定相等，也就是hashCode()不是绝对可靠的，hashCode()性能较好。 hashCode()和equals()使用的注意事项 对于需要大量并且快速的对比的话如果都用equals()去做显然效率太低，所以解决方式是，每当需要对比的时候，首先用hashCode()去对比，如果hashCode()不一样，则表示这两个对象肯定不相等（也就是不必再用equals()去再对比了）,如果hashCode()相同，此时再对比他们的equals()，如果equals()也相同，则表示这两个对象是真的相同了，这样既能大大提高了效率也保证了对比的绝对正确性！ 这种大量的并且快速的对象对比一般使用的hash容器中，比如HashSet,HashMap,HashTable等等，比如HashSet里要求对象不能重复，则他内部必然要对添加进去的每个对象进行对比，而他的对比规则就是像上面说的那样，先hashCode()，如果hashCode()相同，再用equals()验证，如果hashCode()都不同，则肯定不同，这样对比的效率就很高了。 然而hashCode()和equals()一样都是基本类Object里的方法，而和equals()一样，Object里hashCode()里面只是返回当前对象的地址，如果是这样的话，那么我们相同的一个类，new两个对象，由于他们在内存里的地址不同，则他们的hashCode（）不同，所以这显然不是我们想要的，所以我们必须重写我们类的hashCode()方法，即一个类，在hashCode()里面返回唯一的一个hash值，比如下面： 123456789public class Person &#123; private int num; private String name; @Override public int hashCode() &#123; return num * name.hashCode(); &#125;&#125; 由于标识这个类的是他的内部的变量num和name,所以我们就根据他们返回一个hash值，作为这个类的唯一hash值。 所以如果我们的对象要想放进hashSet，并且发挥hashSet的特性（即不包含一样的对象），则我们就要重写我们类的hashCode()和equals()方法了。像String,Integer等这种类内部都已经重写了这两个方法。 当然如果我们只是平时想对比两个对象是否一致，则只重写一个equals()，然后利用equals()去对比也行的。 使用规则阿里巴巴规定【强制】关于hashCode和equals的处理，遵循如下规则： 1） 只要重写equals，就必须重写hashCode。2） 因为Set存储的是不重复的对象，依据hashCode和equals进行判断，所以Set存储的对象必须重写这两个方法。3） 如果自定义对象作为Map的键，那么必须重写hashCode和equals。 说明： String重写了hashCode和equals方法，所以我们可以非常愉快地使用String对象作为key来使用。 什么时候需要重写一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会重载hashCode。 为什么要重载hashCode如果你重写了equals，比如说是基于对象的内容实现的，而保留hashCode的实现不变，那么很可能某两个对象明明是“相等”，而hashCode却不一样。 这样，当你用其中的一个作为键保存到hashMap、hasoTable或hashSet中，再以“相等的”找另一个作为键值去查找他们的时候，则根本找不到。 为什么equals()相等，hashCode就一定要相等，而hashCode相等，却不要求equals相等 因为是按照hashCode来访问小内存块，所以hashCode必须相等。 HashMap获取一个对象是比较key的hashCode相等和equals为true。 之所以hashCode相等，却可以equal不等，就比如ObjectA和ObjectB他们都有属性name，那么hashCode都以name计算，所以hashCode一样，但是两个对象属于不同类型，所以equals为false。 为什么需要hashCode 通过hashCode可以很快的查到小内存块。 通过hashCode比较比equals方法快，当get时先比较hashCode，如果hashCode不同，直接返回false。]]></content>
      <categories>
        <category>源码</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记]]></title>
    <url>%2F2017%2F04%2F25%2F2017-04-25-mysql%2F</url>
    <content type="text"><![CDATA[MySQL学习笔记Windows启动MySQL服务1234-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格) 连接与断开服务器1234mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息 数据库操作123456789101112131415161718-- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE 'PATTERN']-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576-- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT 'string']-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = '目录' -- 索引文件目录 INDEX DIRECTORY = '目录' -- 表注释 COMMENT = 'string' -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE 'pattern'] SHOW TABLES FROM 表名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE 'PATTERN'] SHOW TABLE STATUS [FROM db_name] [LIKE 'pattern']-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作12345678910111213141516-- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码1234567891011121314151617-- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE 'character_set_%' -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE 'pattern']/SHOW CHARSET [LIKE 'pattern'] 查看所有字符集 SHOW COLLATION [LIKE 'pattern'] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型（列类型）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数'123'，补填后为'00123' - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set('男', '女', '无') ); insert into tab values ('男, 女'); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 选择类型12345678910111213141516-- PHP角度1. 功能满足2. 存储空间尽量小，处理效率更高3. 考虑兼容问题-- IP存储 ----------1. 只需存储，可用字符串2. 如果需计算，查找等，可存储为4个字节的无符号int，即unsigned 1) PHP函数转换 ip2long可转换为整型，但会出现携带符号问题。需格式化为无符号的整型。 利用sprintf函数格式化字符串 sprintf("%u", ip2long('192.168.3.134')); 然后用long2ip将整型转回IP字符串 2) MySQL函数转换(无符号整型，UNSIGNED) INET_ATON('127.0.0.1') 将IP转为整型 INET_NTOA(2130706433) 将整型转为IP 列属性（列约束）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647481. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, 'val'); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, 'val'); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment '注释内容';7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范12345678910111213-- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表-- 1NF, 第一范式 字段不能再分，就满足第一范式。-- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。-- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION1234567将多个select查询的结果组合成一个结果集合。SELECT ... UNION [ALL|DISTINCT] SELECT ...默认 DISTINCT 方式，即所有返回的行都是唯一的建议，对每个SELECT查询加上小括号包裹。ORDER BY 排序时，需加上 LIMIT 进行结合。需要各select查询的字段数量一样。每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询12345678910111213141516171819202122232425262728 - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join)123456789101112131415161718192021222324 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; 导出12345678910111213141516171819202122select * into outfile 文件地址 [控制格式] from 表名; -- 导出表数据load data [local] infile 文件地址 [replace|ignore] into table 表名 [控制格式]; -- 导入数据 生成的数据默认的分隔符是制表符 local未指定，则数据文件必须在服务器上 replace 和 ignore 关键词控制对现有的唯一键记录的重复的处理-- 控制格式fields 控制字段格式默认：fields terminated by ' ' enclosed by '' escaped by '\' terminated by 'string' -- 终止 enclosed by 'char' -- 包裹 escaped by 'char' -- 转义 -- 示例： SELECT a,b,a+b INTO OUTFILE '/tmp/result.text' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"' LINES TERMINATED BY '' FROM test_table;lines 控制行格式默认：lines terminated by '' terminated by 'string' -- 终止 INSERT1234567891011121314select语句获得的数据可以用insert插入。可以省略对列的指定，要求 values () 括号内，提供给了按照列顺序出现的所有字段的值。 或者使用set语法。 INSERT INTO tbl_name SET field=value,...；可以一次性使用多个值，采用(), (), ();的形式。 INSERT INTO tbl_name VALUES (), (), ();可以在列值指定时，使用表达式。 INSERT INTO tbl_name VALUES (field_value, 10+10, now());可以使用一个特殊值 DEFAULT，表示该列使用默认值。 INSERT INTO tbl_name VALUES (field_value, DEFAULT);可以通过一个查询的结果，作为需要插入的值。 INSERT INTO tbl_name SELECT ...;可以指定在插入的值出现主键（或唯一索引）冲突时，更新其他非主键列的信息。 INSERT INTO tbl_name VALUES/SET/SELECT ON DUPLICATE KEY UPDATE 字段=值, …; DELETE123456DELETE FROM tbl_name [WHERE where_definition] [ORDER BY ...] [LIMIT row_count]按照条件删除。where指定删除的最多记录数。limit可以通过排序条件删除。order by + limit支持多表删除，使用类似连接语法。delete from 需要删除数据多表1，表2 using 表连接操作 条件。 TRUNCATE12345678TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区 备份与还原1234567891011121314151617181920备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件 视图1234567891011121314151617181920212223242526272829什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)123456789101112131415161718192021222324252627282930313233343536373839404142434445事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表123456表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES 触发器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157--// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date('yyyy-mm-dd hh:ii:ss'); -- 获取日期部分time('yyyy-mm-dd hh:ii:ss'); -- 获取时间部分date_format('yyyy-mm-dd hh:ii:ss', '%d %y %a %d %m %b %j'); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由"参数名"和"参数类型"组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE 'partten' SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 用户和权限管理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576-- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD("密码") WHERE `user` = "root";5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 'user_name'@'192.168.1.1' - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD('密码') -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD('密码') -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] 'password'] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO 'pms'@'%' IDENTIFIED BY 'pms0817';-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 表维护1234567-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项123456789101112131. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 '5. CMD命令行内的语句结束符可以为 ";", "G", "g"，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：c]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java工程师之路]]></title>
    <url>%2F2017%2F04%2F20%2F2017-04-20-to-be-java-coder%2F</url>
    <content type="text"><![CDATA[Java工程师之路 一、基础篇面向对象什么是面向对象 面向对象、面向过程 面向对象的三大基本特征和五大基本原则 平台无关性 Java如何实现的平台无关 JVM还支持哪些语言（Kotlin、Groovy、JRuby、Jython、Scala） 值传递 值传递、引用传递 为什么说Java中只有值传递 封装、继承、多态 什么是多态、方法重写与重载 Java的继承与实现 构造函数与默认构造函数 类变量、成员变量和局部变量 成员变量和方法作用域 Java基础知识基本数据类型 7种基本数据类型：整型、浮点型、布尔型、字符型 整型中byte、short、int、long的取值范围 什么是浮点型？什么是单精度和双精度？为什么不能用浮点型表示金额？ 自动拆装箱 什么是包装类型、什么是基本类型、什么是自动拆装箱 Integer的缓存机制 String 字符串的不可变性 JDK 6和JDK 7中substring的原理及区别、 replaceFirst、replaceAll、replace区别、 String对“+”的重载、字符串拼接的几种方式和区别 String.valueOf和Integer.toString的区别、 switch对String的支持 字符串池、常量池（运行时常量池、Class常量池）、intern 熟悉Java中各种关键字 transient、instanceof、volatile、synchronized、final、static、const 原理及用法。 集合类 常用集合类的使用、ArrayList和LinkedList和Vector的区别 、SynchronizedList和Vector的区别、HashMap、HashTable、ConcurrentHashMap区别、 Set和List区别？Set如何保证元素不重复？ Java 8中stream相关用法、apache集合处理工具类的使用、不同版本的JDK中HashMap的实现的区别以及原因 Collection和Collections区别 Arrays.asList获得的List使用时需要注意什么 Enumeration和Iterator区别 fail-fast 和 fail-safe CopyOnWriteArrayList、ConcurrentSkipListMap 枚举 枚举的用法、枚举的实现、枚举与单例、Enum类 Java枚举如何比较 switch对枚举的支持 枚举的序列化如何实现 枚举的线程安全性问题 IO 字符流、字节流、输入流、输出流、 同步、异步、阻塞、非阻塞、Linux 5种IO模型 BIO、NIO和AIO的区别、三种IO的用法与原理、netty Java反射与javassist 反射与工厂模式、 反射有什么作用 Class类 java.lang.reflect.* 动态代理 静态代理、动态代理 动态代理和反射的关系 动态代理的几种实现方式 AOP 序列化 什么是序列化与反序列化、为什么序列化、序列化底层原理、序列化与单例模式、protobuf、为什么说序列化并不安全 注解 元注解、自定义注解、Java中常用注解使用、注解与反射的结合 Spring常用注解 JMS 什么是Java消息服务、JMS消息传送模型 JMX java.lang.management.*、 javax.management.* 泛型 泛型与继承、类型擦除、泛型中K T V E ？ object等的含义、泛型各种用法 限定通配符和非限定通配符、上下界限定符extends 和 super List和原始类型List之间的区别? List&lt;?&gt;和List之间的区别是什么? 单元测试 junit、mock、mockito、内存数据库（h2） 正则表达式 java.lang.util.regex.* 常用的Java工具库 commons.lang, commons.*... guava-libraries netty API&amp;SPI API、API和SPI的关系和区别 如何定义SPI、SPI的实现原理 异常 异常类型、正确处理异常、自定义异常 Error和Exception 异常链、try-with-resources finally和return的执行顺序 时间处理 时区、冬令时和夏令时、时间戳、Java中时间API 格林威治时间、CET,UTC,GMT,CST几种常见时间的含义和关系 SimpleDateFormat的线程安全性问题 Java 8中的时间处理 如何在东八区的计算机上获取美国时间 编码方式 Unicode、有了Unicode为啥还需要UTF-8 GBK、GB2312、GB18030之间的区别 UTF8、UTF16、UTF32区别 URL编解码、Big Endian和Little Endian 如何解决乱码问题 语法糖 Java中语法糖原理、解语法糖 语法糖：switch 支持 String 与枚举、泛型、自动装箱与拆箱、方法变长参数、枚举、内部类、条件编译、 断言、数值字面量、for-each、try-with-resource、Lambda表达式、 阅读源代码 String、Integer、Long、Enum、BigDecimal、ThreadLocal、ClassLoader &amp; URLClassLoader、ArrayList &amp; LinkedList、 HashMap &amp; LinkedHashMap &amp; TreeMap &amp; CouncurrentHashMap、HashSet &amp; LinkedHashSet &amp; TreeSet Java并发编程并发与并行 什么是并发 什么是并行 并发与并行的区别 线程 线程的实现、线程的状态、优先级、线程调度、创建线程的多种方式、守护线程 线程与进程的区别 线程池 自己设计线程池、submit() 和 execute()、线程池原理 为什么不允许使用Executors创建线程池 线程安全 死锁、死锁如何排查、线程安全和内存模型的关系 锁 CAS、乐观锁与悲观锁、数据库相关锁机制、分布式锁、偏向锁、轻量级锁、重量级锁、monitor、 锁优化、锁消除、锁粗化、自旋锁、可重入锁、阻塞锁、死锁 死锁 死锁的原因 死锁的解决办法 synchronized synchronized是如何实现的？ synchronized和lock之间关系、不使用synchronized如何实现一个线程安全的单例 synchronized和原子性、可见性和有序性之间的关系 volatile happens-before、内存屏障、编译器指令重排和CPU指令重排 volatile的实现原理 volatile和原子性、可见性和有序性之间的关系 有了symchronized为什么还需要volatile sleep 和 waitwait 和 notifynotify 和 notifyAllThreadLocal写一个死锁的程序写代码来解决生产者消费者问题并发包阅读源代码，并学会使用 Thread、Runnable、Callable、ReentrantLock、ReentrantReadWriteLock、Atomic*、Semaphore、CountDownLatch、、ConcurrentHashMap、Executors 二、底层篇JVMJVM内存结构 class文件格式、运行时数据区：堆、栈、方法区、直接内存、运行时常量池、 堆和栈区别 Java中的对象一定在堆上分配吗？ Java内存模型 计算机内存模型、缓存一致性、MESI协议 可见性、原子性、顺序性、happens-before、 内存屏障、synchronized、volatile、final、锁 垃圾回收 GC算法：标记清除、引用计数、复制、标记压缩、分代回收、增量式回收 GC参数、对象存活的判定、垃圾收集器（CMS、G1、ZGC、Epsilon） JVM参数及调优 -Xmx、-Xmn、-Xms、Xss、-XX:SurvivorRatio、 -XX:PermSize、-XX:MaxPermSize、-XX:MaxTenuringThreshold Java对象模型 oop-klass、对象头 HotSpot 即时编译器、编译优化 虚拟机性能监控与故障处理工具 jps, jstack, jmap、jstat, jconsole, jinfo, jhat, javap, btrace、TProfiler Arthas 类加载机制 classLoader、类加载过程、双亲委派（破坏双亲委派）、模块化（jboss modules、osgi、jigsaw） 编译与反编译 什么是编译（前端编译、后端编译）、什么是反编译 JIT、JIT优化（逃逸分析、栈上分配、标量替换、锁优化） 编译工具：javac 反编译工具：javap 、jad 、CRF 三、 进阶篇Java底层知识字节码、class文件格式CPU缓存，L1，L2，L3和伪共享尾递归位运算 用位运算实现加、减、乘、除、取余 设计模式 设计模式的六大原则： 开闭原则（Open Close Principle）、里氏代换原则（Liskov Substitution Principle）、依赖倒转原则（Dependence Inversion Principle） 接口隔离原则（Interface Segregation Principle）、迪米特法则（最少知道原则）（Demeter Principle）、合成复用原则（Composite Reuse Principle） 了解23种设计模式 创建型模式：单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。 结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。 行为型模式：模版方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式（Interpreter模式）、状态模式、策略模式、职责链模式(责任链模式)、访问者模式。 会使用常用设计模式 单例的七种写法：懒汉——线程不安全、懒汉——线程安全、饿汉、饿汉——变种、静态内部类、枚举、双重校验锁 工厂模式、适配器模式、策略模式、模板方法模式、观察者模式、外观模式、代理模式等必会 不用synchronized和lock，实现线程安全的单例模式实现AOP实现IOCnio和reactor设计模式网络编程知识tcp、udp、http、https等常用协议 三次握手与四次关闭、流量控制和拥塞控制、OSI七层模型、tcp粘包与拆包 http/1.0 http/1.1 http/2之间的区别 http中 get和post区别 常见的web请求返回的状态码 404、302、301、500分别代表什么 http/3Java RMI，Socket，HttpClientcookie 与 session cookie被禁用，如何实现session 用Java写一个简单的静态文件的HTTP服务器了解nginx和apache服务器的特性并搭建一个对应的服务器用Java实现FTP、SMTP协议进程间通讯的方式什么是CDN？如果实现？DNS？ 什么是DNS 、记录类型:A记录、CNAME记录、AAAA记录等 域名解析、根域名服务器 DNS污染、DNS劫持、公共DNS：114 DNS、Google DNS、OpenDNS 反向代理 正向代理、反向代理 反向代理服务器 框架知识Servlet 生命周期 线程安全问题 filter和listener web.xml中常用配置及作用 Hibernate 什么是OR Mapping Hibernate的缓存机制 Hibernate的懒加载 Hibernate/Ibatis/MyBatis之间的区别 Spring Bean的初始化 AOP原理 实现Spring的IOC spring四种依赖注入方式 Spring MVC 什么是MVC Spring mvc与Struts mvc的区别 Spring Boot Spring Boot 2.0、起步依赖、自动配置、 Spring Boot的starter原理，自己实现一个starter Spring SecuritySpring Cloud 服务发现与注册：Eureka、Zookeeper、Consul 负载均衡：Feign、Spring Cloud Loadbalance 服务配置：Spring Cloud Config 服务限流与熔断：Hystrix 服务链路追踪：Dapper 服务网关、安全、消息 应用服务器知识JBosstomcatjettyWeblogic工具git &amp; svnmaven &amp; gradleIntellij IDEA 常用插件：Maven Helper 、FindBugs-IDEA、阿里巴巴代码规约检测、GsonFormat、aceJump Lombok plugin、.ignore、Mybatis plugin 四、 高级篇新技术Java 8 lambda表达式、Stream API、时间API Java 9 Jigsaw、Jshell、Reactive Streams Java 10 局部变量类型推断、G1的并行Full GC、ThreadLocal握手机制 Java 11 ZGC、Epsilon、增强var、 Spring 5 响应式编程 Spring Boot 2.0http/2http/3性能优化 使用单例、使用Future模式、使用线程池、选择就绪、减少上下文切换、减少锁粒度、数据压缩、结果缓存 线上问题分析dump获取 线程Dump、内存Dump、gc情况 dump分析 分析死锁、分析内存泄露 dump分析及获取工具 jstack、jstat、jmap、jhat、Arthas 自己编写各种outofmemory，stackoverflow程序 HeapOutOfMemory、 Young OutOfMemory、MethodArea OutOfMemory、ConstantPool OutOfMemory、DirectMemory OutOfMemory、Stack OutOfMemory Stack OverFlow Arthas jvm相关、class/classloader相关、monitor/watch/trace相关、 options、管道、后台异步任务 文档：https://alibaba.github.io/arthas/advanced-use.html 常见问题解决思路 内存溢出、线程死锁、类加载冲突 使用工具尝试解决以下问题，并写下总结 当一个Java程序响应很慢时如何查找问题、 当一个Java程序频繁FullGC时如何解决问题、 如何查看垃圾回收日志、 当一个Java应用发生OutOfMemory时该如何解决、 如何判断是否出现死锁、 如何判断是否存在内存泄露 使用Arthas快速排查Spring Boot应用404/401问题 使用Arthas排查线上应用日志打满问题 利用Arthas排查Spring Boot应用NoSuchMethodError 编译原理知识编译与反编译Java代码的编译与反编译Java的反编译工具 javap 、jad 、CRF 即时编译器词法分析，语法分析（LL算法，递归下降算法，LR算法），语义分析，运行时环境，中间代码，代码生成，代码优化操作系统知识Linux的常用命令进程间通信进程同步 生产者消费者问题、哲学家就餐问题、读者写者问题 缓冲区溢出分段和分页虚拟内存与主存虚拟内存管理换页算法数据库知识MySql 执行引擎MySQL 执行计划 如何查看执行计划，如何根据执行计划进行SQL优化 索引 Hash索引、B树索引（B+树、和B树、R树） 普通索引、唯一索引 覆盖索引、最左前缀原则、索引下推 SQL优化数据库事务和隔离级别 事务的隔离级别、事务能不能实现锁的功能 数据库锁 行锁、表锁、使用数据库锁实现乐观锁、 连接 内连接，左连接，右连接 数据库主备搭建binlogredolog内存数据库 h2 分库分表读写分离常用的nosql数据库 redis、memcached 分别使用数据库锁、NoSql实现分布式锁性能调优数据库连接池数据结构与算法知识简单的数据结构 栈、队列、链表、数组、哈希表、 栈和队列的相同和不同之处 栈通常采用的两种存储结构 树 二叉树、字典树、平衡树、排序树、B树、B+树、R树、多路树、红黑树 堆 大根堆、小根堆 图 有向图、无向图、拓扑 排序算法 稳定的排序：冒泡排序、插入排序、鸡尾酒排序、桶排序、计数排序、归并排序、原地归并排序、二叉排序树排序、鸽巢排序、基数排序、侏儒排序、图书馆排序、块排序 不稳定的排序：选择排序、希尔排序、Clover排序算法、梳排序、堆排序、平滑排序、快速排序、内省排序、耐心排序 各种排序算法和时间复杂度 深度优先和广度优先搜索全排列、贪心算法、KMP算法、hash算法海量数据处理 分治，hash映射，堆排序，双层桶划分，Bloom Filter，bitmap，数据库索引，mapreduce等。 两个栈实现队列，和两个队列实现栈大数据知识Zookeeper 基本概念、常见用法 Solr，Lucene，ElasticSearch 在linux上部署solr，solrcloud，，新增、删除、查询索引 Storm，流式计算，了解Spark，S4 在linux上部署storm，用zookeeper做协调，运行storm hello world，local和remote模式运行调试storm topology。 Hadoop，离线计算 HDFS、MapReduce 分布式日志收集flume，kafka，logstash数据挖掘，mahout网络安全知识XSS XSS的防御 CSRF注入攻击 SQL注入、XML注入、CRLF注入 文件上传漏洞加密与解密 对称加密、非对称加密、哈希算法、加盐哈希算法 MD5，SHA1、DES、AES、RSA、DSA 彩虹表 DDOS攻击 DOS攻击、DDOS攻击 memcached为什么可以导致DDos攻击、什么是反射型DDoS 如何通过Hash碰撞进行DOS攻击 SSL、TLS，HTTPS用openssl签一个证书部署到apache或nginx五、架构篇分布式 数据一致性、服务治理、服务降级 分布式事务 2PC、3PC、CAP、BASE、 可靠消息最终一致性、最大努力通知、TCC Dubbo 服务注册、服务发现，服务治理 http://dubbo.apache.org/zh-cn/ 分布式数据库 怎样打造一个分布式数据库、什么时候需要分布式数据库、mycat、otter、HBase 分布式文件系统 mfs、fastdfs 分布式缓存 缓存一致性、缓存命中率、缓存冗余 限流降级 Hystrix、Sentinal 算法 共识算法、Raft协议、Paxos 算法与 Raft 算法、拜占庭问题与算法 2PC、3PC 微服务 SOA、康威定律 ServiceMesh sidecar Docker &amp; KubernetsSpring BootSpring Cloud高并发分库分表CDN技术消息队列 ActiveMQ 监控监控什么 CPU、内存、磁盘I/O、网络I/O等 监控手段 进程监控、语义监控、机器资源监控、数据波动 监控数据采集 日志、埋点 Dapper负载均衡 tomcat负载均衡、Nginx负载均衡 四层负载均衡、七层负载均衡 DNS DNS原理、DNS的设计 CDN 数据一致性 六、 扩展篇云计算 IaaS、SaaS、PaaS、虚拟化技术、openstack、Serverlsess 搜索引擎 Solr、Lucene、Nutch、Elasticsearch 权限管理 Shiro 区块链 哈希算法、Merkle树、公钥密码算法、共识算法、Raft协议、Paxos 算法与 Raft 算法、拜占庭问题与算法、消息认证码与数字签名 比特币 挖矿、共识机制、闪电网络、侧链、热点问题、分叉 以太坊超级账本人工智能 数学基础、机器学习、人工神经网络、深度学习、应用场景。 常用框架 TensorFlow、DeepLearning4J IoT量子计算AR &amp; VR其他语言 Groovy、Python、Go、NodeJs、Swift、Rust 六、 推荐书籍 《深入理解Java虚拟机》 《Effective Java》 《深入分析Java Web技术内幕》 《大型网站技术架构》 《代码整洁之道》 《架构整洁之道》 《Head First设计模式》 《maven实战》 《区块链原理、设计与应用》 《Java并发编程实战》 《鸟哥的Linux私房菜》 《从Paxos到Zookeeper》 《架构即未来》]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring]]></title>
    <url>%2F2017%2F04%2F15%2F2017-04-15-spring-introduction%2F</url>
    <content type="text"><![CDATA[Why为什么要使用Spring？ Spring主要两个有功能为我们的业务对象管理提供了非常便捷的方法： DI（Dependency Injection，依赖注入） AOP（Aspect Oriented Programming，面向切面编程） Java Bean每一个类实现了Bean的规范才可以由Spring来接管，那么Bean的规范是什么呢？ 必须是个公有(public)类 有无参构造函数 用公共方法暴露内部成员属性(getter,setter) 实现这样规范的类，被称为Java Bean。即是一种可重用的组件。 DI-依赖注入简单来说，一个系统中可能会有成千上万个对象。如果要手工维护它们之间的关系，这是不可想象的。我们可以在Spring的XML文件描述它们之间的关系，由Spring自动来注入它们——比如A类的实例需要B类的实例作为参数set进去。 AOP-面向切面编程就以日志系统为例。在执行某个操作前后都需要输出日志，如果手工加代码，那简直太可怕了。而且等代码庞大起来，也是非常难维护的一种情况。这里就需要面向切面来编程 How关于BeanBean的生命周期如你所见，在bean准备就绪之前，bean工厂执行了若干启动步骤。我们对图进行详细描述： Spring对bean进行实例化； Spring将值和bean的引用注入到bean对应的属性中； 如果bean实现了BeanNameAware接口，Spring将bean的ID传递给setBean-Name()方法； 如果bean实现了BeanFactoryAware接口，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入； 如果bean实现了ApplicationContextAware接口，Spring将调用setApplicationContext()方法，将bean所在的应用上下文的引用传入进来； 如果bean实现了BeanPostProcessor接口，Spring将调用它们的post-ProcessBeforeInitialization()方法； 如果bean实现了InitializingBean接口，Spring将调用它们的after-PropertiesSet()方法。类似地，如果bean使用init-method声明了初始化方法，该方法也会被调用； 如果bean实现了BeanPostProcessor接口，Spring将调用它们的post-ProcessAfterInitialization()方法； 此时，bean已经准备就绪，可以被应用程序使用了，它们将一直驻留在应用上下文中，直到该应用上下文被销毁； 如果bean实现了DisposableBean接口，Spring将调用它的destroy()接口方法。同样，如果bean使用destroy-method声明了销毁方法，该方法也会被调用。 Bean的作用域Spring定义了多种Bean作用域，可以基于这些作用域创建bean，包括： 单例（Singleton）：在整个应用中，只创建bean的一个实例。 原型（Prototype）：每次注入或者通过Spring应用上下文获取的时候，都会创建一个新的bean实例。 会话（Session）：在Web应用中，为每个会话创建一个bean实例。 请求（Rquest）：在Web应用中，为每个请求创建一个bean实例。 在代码里看起来是这样的：12@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)public class MyIsBean&#123;...&#125; XML版本：1234&lt;bean id="BEANID" class = "net.itxm.beans" scope="prototype"&gt; 在默认情况下，Spring应用上下文中所有bean都是作为以单例（singleton）的形式创建的。也就是说，不管给定的一个bean被注入到其他bean多少次，每次所注入的都是同一个实例。 在大多数情况下，单例bean是很理想的方案。初始化和垃圾回收对象实例所带来的成本只留给一些小规模任务，在这些任务中，让对象保持无状态并且在应用中反复重用这些对象可能并不合理。 有时候，可能会发现，你所使用的类是易变的（mutable），它们会保持一些状态，因此重用是不安全的。在这种情况下，将class声明为单例的bean就不是什么好主意了，因为对象会被污染，稍后重用的时候会出现意想不到的问题。 声明Bean以下是声明Bean的注解： @Component 组件，没有明确的角色 @Service 在业务逻辑层使用 @Repository 在数据访问层使用 @Controller 在展现层使用(MVC -&gt; Spring MVC)使用 在这里，可以指定bean的id名：Component(“yourBeanName”) 同时，Spring支持将@Named作为@Component注解的替代方案。两者之间有一些细微的差异，但是在大多数场景中，它们是可以互相替换的。 关于依赖注入注入Bean的注解@Autowired Spring提供的注解 不仅仅是对象，还有在构造器上，还能用在属性的Setter方法上。 不管是构造器、Setter方法还是其他的方法，Spring都会尝试满足方法参数上所声明的依赖。假如有且只有一个bean匹配依赖需求的话，那么这个bean将会被装配进来。 如果没有匹配的bean，那么在应用上下文创建的时候，Spring会抛出一个异常。为了避免异常的出现，你可以将@Autowired的required属性设置为false。 将required属性设置为false时，Spring会尝试执行自动装配，但是如果没有匹配的bean的话，Spring将会让这个bean处于未装配的状态。但是，把required属性设置为false时，你需要谨慎对待。如果在你的代码中没有进行null检查的话，这个处于未装配状态的属性有可能会出现NullPointerException。 @Inject注解来源于Java依赖注入规范，该规范同时还为我们定义了@Named注解。在自动装配中，Spring同时支持@Inject和@Autowired。尽管@Inject和@Autowired之间有着一些细微的差别，但是在大多数场景下，它们都是可以互相替换的。 @Autowired 是最常见的注解之一，但在老项目中，你可能会看到这些注解，它们的作用和@Autowired 相近： @Inject 是JSR-330提供的注解@Resource 是JSR-250提供的注解 条件化的Bean假设你希望一个或多个bean只有在应用的类路径下包含特定的库时才创建。或者我们希望某个bean只有当另外某个特定的bean也声明了之后才会创建。我们还可能要求只有某个特定的环境变量设置之后，才会创建某个bean。 在Spring 4之前，很难实现这种级别的条件化配置，但是Spring 4引入了一个新的@Conditional注解，它可以用到带有@Bean注解的方法上。如果给定的条件计算结果为true，就会创建这个bean，否则的话，这个bean会被忽略。 通过ConditionContext，我们可以做到如下几点： 借助getRegistry()返回的BeanDefinitionRegistry检查bean定义； 借助getBeanFactory()返回的ConfigurableListableBeanFactory检查bean是否存在，甚至探查bean的属性； 借助getEnvironment()返回的Environment检查环境变量是否存在以及它的值是什么； 读取并探查getResourceLoader()返回的ResourceLoader所加载的资源； 借助getClassLoader()返回的ClassLoader加载并检查类是否存在。 处理自动装配的歧义性标示首选的bean 在声明bean的时候，通过将其中一个可选的bean设置为首选（primary）bean能够避免自动装配时的歧义性。当遇到歧义性的时候，Spring将会使用首选的bean，而不是其他可选的bean。实际上，你所声明就是“最喜欢”的bean。 限定自动装配的bean 设置首选bean的局限性在于@Primary无法将可选方案的范围限定到唯一一个无歧义性的选项中。它只能标示一个优先的可选方案。当首选bean的数量超过一个时，我们并没有其他的方法进一步缩小可选范围。 与之相反，Spring的限定符能够在所有可选的bean上进行缩小范围的操作，最终能够达到只有一个bean满足所规定的限制条件。如果将所有的限定符都用上后依然存在歧义性，那么你可以继续使用更多的限定符来缩小选择范围。 @Qualifier注解是使用限定符的主要方式。它可以与@Autowired和@Inject协同使用，在注入的时候指定想要注入进去的是哪个bean。例如，我们想要确保要将IceCream注入到setDessert()之中： 12345@Autowired@Qualifier("iceCream")public void setDessert(Dessert dessert)&#123; this.dessert = dessert;&#125; 这是使用限定符的最简单的例子。为@Qualifier注解所设置的参数就是想要注入的bean的ID。所有使用@Component注解声明的类都会创建为bean，并且bean的ID为首字母变为小写的类名。因此，@Qualifier(“iceCream”)指向的是组件扫描时所创建的bean，并且这个bean是IceCream类的实例。 实际上，还有一点需要补充一下。更准确地讲，@Qualifier(“iceCream”)所引用的bean要具有String类型的“iceCream”作为限定符。如果没有指定其他的限定符的话，所有的bean都会给定一个默认的限定符，这个限定符与bean的ID相同。因此，框架会将具有“iceCream”限定符的bean注入到setDessert()方法中。这恰巧就是ID为iceCream的bean，它是IceCream类在组件扫描的时候创建的。 基于默认的bean ID作为限定符是非常简单的，但这有可能会引入一些问题。如果你重构了IceCream类，将其重命名为Gelato的话，那此时会发生什么情况呢？如果这样的话，bean的ID和默认的限定符会变为gelato，这就无法匹配setDessert()方法中的限定符。自动装配会失败。 这里的问题在于setDessert()方法上所指定的限定符与要注入的bean的名称是紧耦合的。对类名称的任意改动都会导致限定符失效。 SpringEL Value实现资源的注入 Bean的初始化和销毁 Java配置方式：initMethod和destoryMethod 注解：@PostConstruct和@PreDestory Profile提供在不同的环境下使用不同的配置 激活Profile Spring在确定哪个profile处于激活状态时，需要依赖两个独立的属性：spring.profiles.active和spring.profiles.default。如果设置了spring.profiles.active属性的话，那么它的值就会用来确定哪个profile是激活的。但如果没有设置spring.profiles.active属性的话，那Spring将会查找spring.profiles.default的值。如果spring.profiles.active和spring.profiles.default均没有设置的话，那就没有激活的profile，因此只会创建那些没有定义在profile中的bean。 使用profile进行测试 当运行集成测试时，通常会希望采用与生产环境（或者是生产环境的部分子集）相同的配置进行测试。但是，如果配置中的bean定义在了profile中，那么在运行测试时，我们就需要有一种方式来启用合适的profile。 Spring提供了@ActiveProfiles注解，我们可以使用它来指定运行测试时要激活哪个profile。在集成测试时，通常想要激活的是开发环境的profile。 比如Profile(“dev”) Application Event使用Application Event可以做到Bean与Bean之间的通信 Spring的事件需要遵循如下流程： 自定义事件，集成ApplicationEvent 定义事件监听器，实现ApplicationListener 使用容器发布事件 关于AOP名词介绍通知（Advice） 通知定义了切面是什么以及何时使用。除了描述切面要完成的工作，通知还解决了何时执行这个工作的问题。它应该应用在某个方法被调用之前？之后？之前和之后都调用？还是只在方法抛出异常时调用？ Spring切面可以应用5种类型的通知： 前置通知（Before）：在目标方法被调用之前调用通知功能； 后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么； 返回通知（After-returning）：在目标方法成功执行之后调用通知； 异常通知（After-throwing）：在目标方法抛出异常后调用通知； 环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。 对应注解： 注解 通知 @After 通知方法会在目标方法返回或抛出异常后调用 —- @AfterReturning 通知方法会在目标方法返回后调用 —- @AfterThrowing 通知方法会在目标方法抛出异常后调用 —- @Around 通知方法会将目标方法封装起来 —- @Before 通知方法会在目标方法调用之前执行 —- 连接点（Join point） 连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。 切点（Pointcut） 如果说通知定义了切面的“什么”和“何时”的话，那么切点就定义了“何处” 。切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或是利用正则表达式定义所匹配的类和方法名称来指定这些切点。有些AOP框架允许我们创建动态的切点，可以根据运行时的决策（比如方法的参数值）来决定是否应用通知。 切面（Aspect） 通知+切点=切面 引入（Introduction） 引入允许我们向现有的类添加新方法或属性 织入（Weaving） 织入是把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。在目标对象的生命周期里有多个点可以进行织入： 编译期：切面在目标类编译时被织入。这种方式需要特殊的编译器。AspectJ的织入编译器就是以这种方式织入切面的。 类加载期：切面在目标类加载到JVM时被织入。这种方式需要特殊的类加载器（ClassLoader），它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ 5的加载时织入（load-time weaving，LTW）就支持以这种方式织入切面。 运行期：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。Spring AOP就是以这种方式织入切面的。 Spring对AOP的支持： 基于代理的经典Spring AOP； 纯POJO切面（4.x版本需要XML配置）； @AspectJ注解驱动的切面； 注入式AspectJ切面（适用于Spring各版本）。 例子 123public interface Performance()&#123; public void perform();&#125; 现在来写一个切点表达式，这个表达式能够设置当perform()方法执行时触发通知的调用。 123456execution(* concert.Performance.perform(..))//execution：在方法执行时触发//*：返回任意类型//concert.Performance：方法所属类//perform：方法名//(..)：使用任意参数 不仅如此，还可以写的更复杂一点 12execution(* concert.Performance.perform(..)&amp;&amp;within(concert.*))//增加了一个与操作，当concert包下的任意类方法被调用时也会触发 在切点中选择bean 12execution(*concert.Performance.perform()) and bean('woodstock')//限定bean id为woodstock 来个完整的切面 12345678910111213141516171819@Aspectpublic class Audience&#123; @Before("execution(**concert.Performance.perform(..))") public void silenceCellPhones()&#123; System.out.println("Silencing cell phones"); &#125; @Before("execution&#123;** concert.Performance.perform&#123;..&#125;&#125;") public void taskSeats()&#123; System.out.println("Talking seats"); &#125; @AfterReturning("execution&#123;** concert.Performance.perform&#123;..&#125;&#125;") public void applause()&#123; System.out.println("CLAP CLAP CLAP!!!"); &#125; @AfterThrowing("execution&#123;** concert.Performance.perform&#123;..&#125;&#125;") public void demanRefund()&#123; System.out.println("Demanding a refund"); &#125;&#125; 可以简化一下 1234567891011121314151617181920212223@Aspectpublic class Audience&#123; //避免频繁使用切点表达式 @Pointcut("execution(** concert.Performance.perform(..))") public void performance()&#123;&#125; @Before("performance()") public void silenceCellPhones()&#123; System.out.println("Silencing cell phones"); &#125; @Before("performance()") public void taskSeats()&#123; System.out.println("Talking seats"); &#125; @AfterReturning("performance()") public void applause()&#123; System.out.println("CLAP CLAP CLAP!!!"); &#125; @AfterThrowing("performance()") public void demanRefund()&#123; System.out.println("Demanding a refund"); &#125;&#125; XML中声明切面 AOP配置元素 用途 &lt;aop:advisor&gt; 定义AOP通知器 —- &lt;aop:after&gt; 定义AOP后置通知（不管被通知的方法是否执行成功） —- &lt;aop:after-returning&gt; 定义AOP返回通知 —- &lt;aop:after-throwing&gt; 定义AOP异常通知 —- &lt;aop:around&gt; 定义AOP环绕通知 —- &lt;aop:aspect&gt; 定义一个切面 —- &lt;aop:aspectj-autoproxy&gt; 启用@AspectJ注解驱动的切面 —- &lt;aop:before&gt; 定义一个AOP前置通知 —- &lt;aop:config&gt; 顶层的AOP配置元素。大多数的aop:*元素必须包含在aop:config元素内 —- &lt;aop:declare-parents&gt; 以透明的方式为被通知的对象引入额外的接口 —- &lt;aop:pointcut&gt; 定义一个切点 —- 来个栗子 1234567891011121314public class Audience&#123; public void silenceCellPhones()&#123; System.out.println("Silencing cell phones"); &#125; public void taskSeats()&#123; System.out.println("Talking seats"); &#125; public void applause()&#123; System.out.println("CLAP CLAP CLAP!!!"); &#125; public void demandRefund()&#123; System.out.println("Demanding a refund"); &#125;&#125; 通过XML将无注解的Audience声明为切面 12345678910111213141516&lt;aop:config&gt; &lt;aop:aspect ref="audience"&gt; &lt;aop:before pointcut ="execution(** concert.Performance.perform(..))" method="sillenceCellPhones"/&gt; &lt;aop:before pointcut ="execution(** concert.Performance.perform(..))" method="taskSeats"/&gt; &lt;aop:after-returning pointcut ="execution(** concert.Performance.perform(..))" method="applause"/&gt; &lt;aop:After-throwing pointcut ="execution(** concert.Performance.perform(..))" method="demanRefund"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; AspectJ关于Spring AOP的AspectJ切点，最重要的一点就是Spring仅支持AspectJ切点指示器（pointcut designator）的一个子集。让我们回顾下，Spring是基于代理的，而某些切点表达式是与基于代理的AOP无关的。下表列出了Spring AOP所支持的AspectJ切点指示器。 Spring借助AspectJ的切点表达式语言来定义Spring切面 AspectJ指示器 描述 arg() 限制连接点匹配参数为指定类型的执行方法 @args() 限制连接点匹配参数由指定注解标注的执行方法 execution() 用于匹配是连接点的执行方法 this() 限制连接点匹配AOP代理的bean引用为指定类型的类 target 限制连接点匹配目标对象为指定类型的类 @target() 限制连接点匹配特定的执行对象，这些对象对应的类要具有指定类型的注解 within() 限制连接点匹配指定的类型 @within() 限制连接点匹配指定注解所标注的类型（当使用Spring AOP时，方法定义在由指定的注解所标注的类里） @annotation 限定匹配带有指定注解的连接点 Spring高级特性由于Spring特殊的依赖注入技巧，导致Bean之间没有耦合度。 但是Bean有时需要使用spring容器本身的资源，这时你的Bean必须意识到Spring容器的存在。所以得使用Spring Aware，下面来看看Spring Aware提供的接口 BeanNameAware 获得到容器中Bean的名称 BeanFactory 获得当前的bean factory，这样可以调用容器的服务 ApplicationContextAware* 当前application context，这样可以调用容器的服务 MessageSourceAware 获得Message source ApplicationEventPublisherAware 应用时间发布器，可以发布时间， ResourceLoaderAware 获得资源加载器，可以获得外部资源文件 @TaskExecutor这样可以实现多线程和并发编程。通过@EnableAsync开启对异步任务的支持，并通过实际执行的Bean的方法始中使用@Async注解来声明其是一个异步任务 @Scheduled 计划任务首先通过在配置类注解@EnableScheduling来开启对计划任务的支持，然后在要执行计划任务的方法上注解@Scheduled，声明这是一个计划任务 @Conditional根据满足某一个特定条件创建一个特定的Bean。 组合注解与元注解元注解就是可以注解到别的注解上的注解，被注解的注解称之为组合注解，组合注解具备注解其上的元注解的功能。 @Enable*注解的工作原理 通过观察这些@Enable*注解的源码，我们发现所有的注解都有一个@Import注解，@Import是用来导入配置类的，这也就意外着这些自动开启的实现其实是导入了一些自动配置的Bean。这些导入配置的方式主要范围以下三种类型： 第一类：直接导入配置类 第二类：依据条件选择配置类 第三类：动态注册Bean What简单的分析一下Spring。 Spring 框架中的核心组件只有三个：Core、Context 和 Bean。它们构建起了整个 Spring 的骨骼架构。没有它们就不可能有 AOP、Web 等上层的特性功能。下面也将主要从这三个组件入手分析 Spring。 Spring的设计理念用过Spring的同学都知道Bean在Spring的作用是非常重要的。通过一系列简单的配置来满足类与类之间的依赖关系——这叫做依赖注入。而依赖注入的关系是在一个叫IOC的容器中进行管理。 核心组件我们说到Spring 框架中的核心组件只有三个：Core、Context 和 Bean。那么Core和Context是如何协作的呢？ 我们知道 Bean 包装的是 Object，而 Object 必然有数据，如何给这些数据提供生存环境就是 Context 要解决的问题，对 Context 来说他就是要发现每个 Bean 之间的关系，为它们建立这种关系并且要维护好这种关系。所以 Context 就是一个 Bean 关系的集合，这个关系集合又叫 Ioc 容器 ，一旦建立起这个 Ioc 容器后 Spring 就可以为你工作了。那 Core 组件又有什么用武之地呢？其实 Core 就是发现、建立和维护每个 Bean 之间的关系所需要的一些列的工具。 Bean前面已经说明了 Bean 组件对 Spring 的重要性，下面看看 Bean 这个组件式怎么设计的。Bean 组件在 Spring 的 org.springframework.beans 包下。这个包下的所有类主要解决了三件事：Bean 的定义、Bean 的创建以及对 Bean 的解析。对 Spring 的使用者来说唯一需要关心的就是 Bean 的创建，其他两个由 Spring 在内部帮你完成了，对你来说是透明的。 ContextApplicationContext 是 Context 的顶级父类，他除了能标识一个应用环境的基本信息外，他还继承了五个接口，这五个接口主要是扩展了 Context 的功能。 ApplicationContext 的子类主要包含两个方面： ConfigurableApplicationContext 表示该 Context 是可修改的，也就是在构建 Context 中用户可以动态添加或修改已有的配置信息，它下面又有多个子类，其中最经常使用的是可更新的 Context，即 AbstractRefreshableApplicationContext类。 WebApplicationContext 顾名思义，就是为 web 准备的 Context 他可以直接访问到 ServletContext，通常情况下，这个接口使用的少。 再往下分就是按照构建 Context 的文件类型，接着就是访问 Context 的方式。这样一级一级构成了完整的 Context 等级层次。 总体来说 ApplicationContext 必须要完成以下几件事： 标识一个应用环境 利用 BeanFactory 创建 Bean 对象 保存对象关系表 能够捕获各种事件 Context 作为 Spring 的 IOC 容器，基本上整合了 Spring 的大部分功能，或者说是大部分功能的基础。 CoreCore 组件作为 Spring 的核心组件，他其中包含了很多的关键类，其中一个重要组成部分就是定义了资源的访问方式。这种把所有资源都抽象成一个接口的方式很值得在以后的设计中拿来学习。]]></content>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis]]></title>
    <url>%2F2017%2F04%2F08%2F2017-04-08-mybatis-introduction%2F</url>
    <content type="text"><![CDATA[MyBatis使用JDBC编程问题总结1.JDBC编程步骤 加载数据库驱动 创建并获取数据库链接 创建jdbc statement对象 设置sql语句 设置sql语句中的参数(使用preparedStatement) 通过statement执行sql并获取结果 对sql执行结果进行解析处理 释放资源(resultSet、preparedstatement、connection) 2.JDBC问题总结 数据库连接创建、释放频繁造成系统资源浪费，从而影响系统性能。如果使用数据库连接池可解决此问题。 Sql语句在代码中硬编码，造成代码不易维护，实际应用中sql变化的可能较大，sql变动需要改变java代码。 使用preparedStatement向占有位符号传参数存在硬编码，因为sql语句的where条件不一定，可能多也可能少，修改sql还要修改代码，系统不易维护。 对结果集解析存在硬编码（查询列名），sql变化导致解析代码变化，系统不易维护，如果能将数据库记录封装成pojo对象解析比较方便。 1. MyBatis介绍MyBatis 本是apache的一个开源项目iBatis, 2010年这个项目由apache software foundation 迁移到了google code，并且改名为MyBatis 。2013年11月迁移到Github。MyBatis是一个优秀的持久层框架，它对jdbc的操作数据库的过程进行封装，使开发者只需要关注 SQL 本身，而不需要花费精力去处理例如注册驱动、创建connection、创建statement、手动设置参数、结果集检索等jdbc繁杂的过程代码。Mybatis通过xml或注解的方式将要执行的各种statement（statement、preparedStatemnt、CallableStatement）配置起来，并通过java对象和statement中的sql进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射成java对象并返回。MyBatis架构 2. MyBatis配置 mybatis配置SqlMapConfig.xml，此文件作为mybatis的全局配置文件，配置了mybatis的运行环境等信息。mapper.xml文件即sql映射文件，文件中配置了操作数据库的sql语句。此文件需要在SqlMapConfig.xml中加载。 通过mybatis环境等配置信息构造SqlSessionFactory即会话工厂 由会话工厂创建sqlSession即会话，操作数据库需要通过sqlSession进行。 mybatis底层自定义了Executor执行器接口操作数据库，Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。 Mapped Statement也是mybatis一个底层封装对象，它包装了mybatis配置信息及sql映射信息等。mapper.xml文件中一个sql对应一个Mapped Statement对象，sql的id即是Mapped statement的id。 Mapped Statement对sql执行输入参数进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql前将输入的java对象映射至sql中，输入参数映射就是jdbc编程中对preparedStatement设置参数。 Mapped Statement对sql执行输出结果进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql后将输出结果映射至java对象中，输出结果映射过程相当于jdbc编程中对结果的解析处理过程。 从 XML 中构建 SqlSessionFactory每个基于 MyBatis 的应用都是以一个 SqlSessionFactory 的实例为中心的。SqlSessionFactory 的实例可以通过 SqlSessionFactoryBuilder 获得。而 SqlSessionFactoryBuilder 则可以从 XML 配置文件或一个预先定制的 Configuration 的实例构建出 SqlSessionFactory 的实例。 从 XML 文件中构建 SqlSessionFactory 的实例非常简单，建议使用类路径下的资源文件进行配置。但是也可以使用任意的输入流(InputStream)实例，包括字符串形式的文件路径或者 file:// 的 URL 形式的文件路径来配置。MyBatis 包含一个名叫 Resources 的工具类，它包含一些实用方法，可使从 classpath 或其他位置加载资源文件更加容易。123String resource = "org/mybatis/example/mybatis-config.xml";InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); XML 配置文件（configuration XML）中包含了对 MyBatis 系统的核心设置，包含获取数据库连接实例的数据源（DataSource）和决定事务作用域和控制方式的事务管理器（TransactionManager）。这里先给出一个简单的示例：1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;driver&#125;"/&gt; &lt;property name="url" value="$&#123;url&#125;"/&gt; &lt;property name="username" value="$&#123;username&#125;"/&gt; &lt;property name="password" value="$&#123;password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource="org/mybatis/example/BlogMapper.xml"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 当然，还有很多可以在XML 文件中进行配置，上面的示例指出的则是最关键的部分。要注意 XML 头部的声明，用来验证 XML 文档正确性。environment 元素体中包含了事务管理和连接池的配置。mappers 元素则是包含一组 mapper 映射器（这些 mapper 的 XML 文件包含了 SQL 代码和映射定义信息）。 3. Mapper XML 文件MyBatis 的真正强大在于它的映射语句，也是它的魔力所在。由于它的异常强大，映射器的 XML 文件就显得相对简单。如果拿它跟具有相同功能的 JDBC 代码进行对比，你会立即发现省掉了将近 95% 的代码。MyBatis 就是针对 SQL 构建的，并且比普通的方法做的更好。 SQL 映射文件有很少的几个顶级元素（按照它们应该被定义的顺序）： cache – 给定命名空间的缓存配置。 cache-ref – 其他命名空间缓存配置的引用。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 parameterMap – 已废弃！老式风格的参数映射。内联参数是首选,这个元素可能在将来被移除，这里不会记录。 sql – 可被其他语句引用的可重用语句块。 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 select – 映射查询语句 select查询语句是 MyBatis 中最常用的元素之一，光能把数据存到数据库中价值并不大，如果还能重新取出来才有用，多数应用也都是查询比修改要频繁。对每个插入、更新或删除操作，通常对应多个查询操作。这是 MyBatis 的基本原则之一，也是将焦点和努力放到查询和结果映射的原因。简单查询的 select 元素是非常简单的。比如： &lt;select id=&quot;selectPerson&quot; parameterType=&quot;int&quot; resultType=&quot;hashmap&quot;&gt; SELECT * FROM PERSON WHERE ID = #{id} &lt;/select&gt; 这个语句被称作 selectPerson，接受一个 int（或 Integer）类型的参数，并返回一个 HashMap 类型的对象，其中的键是列名，值便是结果行中的对应值。 注意参数符号： #{id}这就告诉 MyBatis 创建一个预处理语句参数，通过 JDBC，这样的一个参数在 SQL 中会由一个“?”来标识，并被传递到一个新的预处理语句中，就像这样：1234// Similar JDBC code, NOT MyBatis…String selectPerson = "SELECT * FROM PERSON WHERE ID=?";PreparedStatement ps = conn.prepareStatement(selectPerson);ps.setInt(1,id); 下面就是 insert，update 和 delete 语句的示例： &lt;insert id=&quot;insertAuthor&quot;&gt; insert into Author (id,username,password,email,bio) values (#{id},#{username},#{password},#{email},#{bio}) &lt;/insert&gt; &lt;update id=&quot;updateAuthor&quot;&gt; update Author set username = #{username}, password = #{password}, email = #{email}, bio = #{bio} where id = #{id} &lt;/update&gt; &lt;delete id=&quot;deleteAuthor&quot;&gt; delete from Author where id = #{id} &lt;/delete&gt; 如前所述，插入语句的配置规则更加丰富，在插入语句里面有一些额外的属性和子元素用来处理主键的生成，而且有多种生成方式。 首先，如果你的数据库支持自动生成主键的字段（比如 MySQL 和 SQL Server），那么你可以设置 useGeneratedKeys=”true”，然后再把 keyProperty 设置到目标属性上就OK了。例如，如果上面的 Author 表已经对 id 使用了自动生成的列类型，那么语句可以修改为: &lt;insert id=&quot;insertAuthor&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt; insert into Author (username,password,email,bio) values (#{username},#{password},#{email},#{bio}) &lt;/insert&gt; 如果你的数据库还支持多行插入, 你也可以传入一个Authors数组或集合，并返回自动生成的主键。 &lt;insert id=&quot;insertAuthor&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt; insert into Author (username, password, email, bio) values &lt;foreach item=&quot;item&quot; collection=&quot;list&quot; separator=&quot;,&quot;&gt; (#{item.username}, #{item.password}, #{item.email}, #{item.bio}) &lt;/foreach&gt; &lt;/insert&gt; 动态 SQLMyBatis 的强大特性之一便是它的动态 SQL。如果你有使用 JDBC 或其它类似框架的经验，你就能体会到根据不同条件拼接 SQL 语句的痛苦。例如拼接时要确保不能忘记添加必要的空格，还要注意去掉列表最后一个列名的逗号。利用动态 SQL 这一特性可以彻底摆脱这种痛苦。 虽然在以前使用动态 SQL 并非一件易事，但正是 MyBatis 提供了可以被用在任意 SQL 映射语句中的强大的动态 SQL 语言得以改进这种情形。 动态 SQL 元素和 JSTL 或基于类似 XML 的文本处理器相似。在 MyBatis 之前的版本中，有很多元素需要花时间了解。MyBatis 3 大大精简了元素种类，现在只需学习原来一半的元素便可。MyBatis 采用功能强大的基于 OGNL 的表达式来淘汰其它大部分元素。 if choose (when, otherwise) trim (where, set) foreachif 动态 SQL 通常要做的事情是根据条件包含 where 子句的一部分。比如： &lt;select id=&quot;findActiveBlogWithTitleLike&quot; resultType=&quot;Blog&quot;&gt; SELECT * FROM BLOG WHERE state = ‘ACTIVE’ &lt;if test=&quot;title != null&quot;&gt; AND title like #{title} &lt;/if&gt; &lt;/select&gt; 这条语句提供了一种可选的查找文本功能。如果没有传入“title”，那么所有处于“ACTIVE”状态的BLOG都会返回；反之若传入了“title”，那么就会对“title”一列进行模糊查找并返回 BLOG 结果（细心的读者可能会发现，“title”参数值是可以包含一些掩码或通配符的）。 如果希望通过“title”和“author”两个参数进行可选搜索该怎么办呢？首先，改变语句的名称让它更具实际意义；然后只要加入另一个条件即可。 &lt;select id=&quot;findActiveBlogLike&quot; resultType=&quot;Blog&quot;&gt; SELECT * FROM BLOG WHERE state = ‘ACTIVE’ &lt;if test=&quot;title != null&quot;&gt; AND title like #{title} &lt;/if&gt; &lt;if test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #{author.name} &lt;/if&gt; &lt;/select&gt; choose, when, otherwise 有时我们不想应用到所有的条件语句，而只想从中择其一项。针对这种情况，MyBatis 提供了 choose 元素，它有点像 Java 中的 switch 语句。 还是上面的例子，但是这次变为提供了“title”就按“title”查找，提供了“author”就按“author”查找的情形，若两者都没有提供，就返回所有符合条件的 BLOG（实际情况可能是由管理员按一定策略选出 BLOG 列表，而不是返回大量无意义的随机结果）。 &lt;select id=&quot;findActiveBlogLike&quot; resultType=&quot;Blog&quot;&gt; SELECT * FROM BLOG WHERE state = ‘ACTIVE’ &lt;choose&gt; &lt;when test=&quot;title != null&quot;&gt; AND title like #{title} &lt;/when&gt; &lt;when test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #{author.name} &lt;/when&gt; &lt;otherwise&gt; AND featured = 1 &lt;/otherwise&gt; &lt;/choose&gt; &lt;/select&gt; trim, where, set 前面几个例子已经合宜地解决了一个臭名昭著的动态 SQL 问题。现在回到“if”示例，这次我们将“ACTIVE = 1”也设置成动态的条件，看看会发生什么。 &lt;select id=&quot;findActiveBlogLike&quot; resultType=&quot;Blog&quot;&gt; SELECT * FROM BLOG WHERE &lt;if test=&quot;state != null&quot;&gt; state = #{state} &lt;/if&gt; &lt;if test=&quot;title != null&quot;&gt; AND title like #{title} &lt;/if&gt; &lt;if test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #{author.name} &lt;/if&gt; &lt;/select&gt; 如果这些条件没有一个能匹配上会发生什么？最终这条 SQL 会变成这样： SELECT * FROM BLOG WHERE 这会导致查询失败。如果仅仅第二个条件匹配又会怎样？这条 SQL 最终会是这样: SELECT * FROM BLOG WHERE AND title like ‘someTitle’ 这个查询也会失败。这个问题不能简单地用条件句式来解决，如果你也曾经被迫这样写过，那么你很可能从此以后都不会再写出这种语句了。 MyBatis 有一个简单的处理，这在 90% 的情况下都会有用。而在不能使用的地方，你可以自定义处理方式来令其正常工作。一处简单的修改就能达到目的： &lt;select id=&quot;findActiveBlogLike&quot; resultType=&quot;Blog&quot;&gt; SELECT * FROM BLOG &lt;where&gt; &lt;if test=&quot;state != null&quot;&gt; state = #{state} &lt;/if&gt; &lt;if test=&quot;title != null&quot;&gt; AND title like #{title} &lt;/if&gt; &lt;if test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #{author.name} &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; where 元素只会在至少有一个子元素的条件返回 SQL 子句的情况下才去插入“WHERE”子句。而且，若语句的开头为“AND”或“OR”，where 元素也会将它们去除。 如果 where 元素没有按正常套路出牌，我们可以通过自定义 trim 元素来定制 where 元素的功能。比如，和 where 元素等价的自定义 trim 元素为： &lt;trim prefix=&quot;WHERE&quot; prefixOverrides=&quot;AND |OR &quot;&gt; ... &lt;/trim&gt; prefixOverrides 属性会忽略通过管道分隔的文本序列（注意此例中的空格也是必要的）。它的作用是移除所有指定在 prefixOverrides 属性中的内容，并且插入 prefix 属性中指定的内容。 类似的用于动态更新语句的解决方案叫做 set。set 元素可以用于动态包含需要更新的列，而舍去其它的。比如： &lt;update id=&quot;updateAuthorIfNecessary&quot;&gt; update Author &lt;set&gt; &lt;if test=&quot;username != null&quot;&gt;username=#{username},&lt;/if&gt; &lt;if test=&quot;password != null&quot;&gt;password=#{password},&lt;/if&gt; &lt;if test=&quot;email != null&quot;&gt;email=#{email},&lt;/if&gt; &lt;if test=&quot;bio != null&quot;&gt;bio=#{bio}&lt;/if&gt; &lt;/set&gt; where id=#{id} &lt;/update&gt; 这里，set 元素会动态前置 SET 关键字，同时也会删掉无关的逗号，因为用了条件语句之后很可能就会在生成的 SQL 语句的后面留下这些逗号。（译者注：因为用的是“if”元素，若最后一个“if”没有匹配上而前面的匹配上，SQL 语句的最后就会有一个逗号遗留） 若你对 set 元素等价的自定义 trim 元素的代码感兴趣，那这就是它的真面目： &lt;trim prefix=&quot;SET&quot; suffixOverrides=&quot;,&quot;&gt; ... &lt;/trim&gt; 注意这里我们删去的是后缀值，同时添加了前缀值。 foreach 动态 SQL 的另外一个常用的操作需求是对一个集合进行遍历，通常是在构建 IN 条件语句的时候。比如： &lt;select id=&quot;selectPostIn&quot; resultType=&quot;domain.blog.Post&quot;&gt; SELECT * FROM POST P WHERE ID in &lt;foreach item=&quot;item&quot; index=&quot;index&quot; collection=&quot;list&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #{item} &lt;/foreach&gt; &lt;/select&gt; foreach 元素的功能非常强大，它允许你指定一个集合，声明可以在元素体内使用的集合项（item）和索引（index）变量。它也允许你指定开头与结尾的字符串以及在迭代结果之间放置分隔符。这个元素是很智能的，因此它不会偶然地附加多余的分隔符。 注意 你可以将任何可迭代对象（如 List、Set 等）、Map 对象或者数组对象传递给 foreach 作为集合参数。当使用可迭代对象或者数组时，index 是当前迭代的次数，item 的值是本次迭代获取的元素。当使用 Map 对象（或者 Map.Entry 对象的集合）时，index 是键，item 是值。 到此我们已经完成了涉及 XML 配置文件和 XML 映射文件的讨论。 bind bind 元素可以从 OGNL 表达式中创建一个变量并将其绑定到上下文。比如： &lt;select id=&quot;selectBlogsLike&quot; resultType=&quot;Blog&quot;&gt; &lt;bind name=&quot;pattern&quot; value=&quot;&apos;%&apos; + _parameter.getTitle() + &apos;%&apos;&quot; /&gt; SELECT * FROM BLOG WHERE title LIKE #{pattern} &lt;/select&gt; 4. 小结(1) #{}和${}#{}表示一个占位符号，通过#{}可以实现preparedStatement向占位符中设置值，自动进行java类型和jdbc类型转换。#{}可以有效防止sql注入。 #{}可以接收简单类型值或pojo属性值。 如果parameterType传输单个简单类型值，#{}括号中可以是value或其它名称。 ${}表示拼接sql串，通过${}可以将parameterType 传入的内容拼接在sql中且不进行jdbc类型转换， ${}可以接收简单类型值或pojo属性值，如果parameterType传输单个简单类型值，${}括号中只能是value。 (2) parameterType和resultTypeparameterType：指定输入参数类型，mybatis通过ognl从输入对象中获取参数值拼接在sql中。 resultType：指定输出结果类型，mybatis将sql查询结果的一行记录数据映射为resultType指定类型的对象。如果有多条数据，则分别进行映射，并把对象放到容器List中 (3) selectOne和selectListselectOne查询一条记录，如果使用selectOne查询多条记录则抛出异常。 selectList可以查询一条或多条记录。]]></content>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EL表达式]]></title>
    <url>%2F2017%2F04%2F05%2F2017-04-05-el%2F</url>
    <content type="text"><![CDATA[EL表达式1．EL 表达式概述EL（Express Lanuage）表达式可以嵌入在jsp页面内部，减少jsp脚本的编写，EL出现的目的是要替代jsp页面中脚本的编写。 2．EL从域中取出数据EL最主要的作用是获得四大域中的数据，格式${EL表达式}EL获得pageContext域中的值：$(pageContextScope.key);EL获得request域中的值：$(request.key);EL获得session域中的值：$(session.key);EL获得application域中的值：$(application.key);EL从四个域中获得某个值$(key);—同样是依次从pageContext域，request域，session域，application域中 获取属性，在某个域中获取后将不在向后寻找 3．EL的内置对象pageScoperequestScopesessionScopeapplicationScope 获取JSP中域中的数据 param,paramValues 接收参数 —- header,headerValues 获取请求头信息 —- initParam 获取全局初始化参数 —- cookie WEB开发中cookie —- pageContext WEB开发中的pageContext —- $(pageContext.request.contextPath)相当于&lt;%=pageContext.getRequest().getContextPath%&gt;]]></content>
      <tags>
        <tag>java</tag>
        <tag>el</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP]]></title>
    <url>%2F2017%2F04%2F01%2F2017-04-01-jsp%2F</url>
    <content type="text"><![CDATA[JSPJSP语法1．jsp脚本和注释jsp脚本：(1)&lt;%java代码%&gt; —– 内部的java代码翻译到service方法的内部(2)&lt;%=java变量或表达式&gt; —– 会被翻译成service方法内部out.print()(3)&lt;%!java代码%&gt; —- 会被翻译成servlet的成员的内容jsp注释： 不同的注释可见范围是不同(1)Html注释： —可见范围 jsp源码、翻译后的servlet、页面显示html源码(2)java注释：//单行注释 /多行注释/ –可见范围 jsp源码 翻译后的servlet(3)jsp注释：&lt;%–注释内容–%&gt; —– 可见范围 jsp源码可见 2．jsp运行原理—–jsp本质就是servletjsp在第一次被访问时会被Web容器翻译成servlet，在执行过程：PS：被翻译后的servlet在Tomcat的work目录中可以找到 3．jsp指令jsp的指令是指导jsp翻译和运行的命令，jsp包括三大指令：(1)page指令 — 属性最多的指令（实际开发中page指令默认）属性最多的一个指令，根据不同的属性，指导整个页面特性格式：&lt;%@ page 属性名1= “属性值1” 属性名2= “属性值2” …%&gt;常用属性如下：language：jsp脚本中可以嵌入的语言种类pageEncoding：当前jsp文件的本身编码—内部可以包含contentTypecontentType：response.setContentType(text/html;charset=UTF-8)session：是否jsp在翻译时自动创建sessionimport：导入java的包errorPage：当当前页面出错后跳转到哪个页面isErrorPage：当前页面是一个处理错误的页面 (2)include指令页面包含（静态包含）指令，可以将一个jsp页面包含到另一个jsp页面中格式：&lt;%@ include file=”被包含的文件地址”%&gt; (3)taglib指令在jsp页面中引入标签库（jstl标签库、struts2标签库）格式：&lt;%@ taglib uri=”标签库地址” prefix=”前缀”%&gt; 4．jsp内置对象（9个）jsp被翻译成servlet之后，service方法中有9个对象定义并初始化完毕，我们在jsp 脚本中可以直接使用这9个对象 名称 类型 描述 out javax.servlet.jsp.JspWriter 用于页面输出 —- request javax.servlet.http.HttpServletRequest 得到用户请求信息 —- response javax.servlet.http.HttpServletResponse 服务器向客户端的回应信息 —- config javax.servlet.ServletConfig 服务器配置，可以取得初始化参数 —- session javax.servlet.http.HttpSession 用来保存用户的信息 —- application javax.servlet.ServletContext 所有用户的共享信息 —- page java.lang.Object 指当前页面转换后的Servlet类的实例 —- pageContext javax.servlet.jsp.PageContext JSP的页面容器 —- exception java.lang.Throwable 表示JSP页面所发生的异常，在错误页中才起作用 —- (1)out对象out的类型：JspWriterout作用就是想客户端输出内容—-out.write()out缓冲区默认8kb 可以设置成0 代表关闭out缓冲区 内容直接写到respons缓冲 器 (2)pageContext对象jsp页面的上下文对象，作用如下：page对象与pageContext对象不是一回事1.pageContext是一个域对象setAttribute(String name,Object obj)getAttribute(String name)removeAttrbute(String name) 2.pageContext可以向指定的其他域中存取数据setAttribute(String name,Object obj,int scope)getAttribute(String name,int scope)removeAttrbute(String name,int scope)findAttribute(String name)—依次从pageContext域，request域，session域，application域中获取属性，在某个域中获取后将不在向后寻找 3.可以获得其他8大隐式对象例如： pageContext.getRequest()pageContext.getSession() 四大作用域：page域：当前jsp页面范围request域：一次请求session域：一次会话application域：整个web应用 5．jsp标签（动作）(1)页面包含(动态包含)：&lt;jsp :include page=&quot;a.jsp&quot;/&gt; (2)请求转发：&lt;jsp:forward page=&quot;要转发的资源&quot; /&gt; 动态包含与静态包含的区别：1.静态包含: &lt;%@include file=&quot;fileurl&quot;%&gt;2.动态包含: &lt;jsp :include page=&quot;a.jsp&quot;/&gt;(1)静态包含发生在：JSP—-&gt;java文件阶段。动态包含发生在：执行class文件阶段,动态加入。(2)静态包含：只生成一个java文件，动态包含：生成多个class文件。(3)静态包含不会检查所包含文件中的变化；但是动态包含，每次检查所含文件中的变化，并且可以带参数 请求转发与请求重定向的区别：1.请求转发：request.getRequestDispatcher().forward(req,resp)服务器行为，request.getRequestDispatcher().forward(req,resp);是一次请求，转发后请求对象会保存，地址栏的URL地址不会改变。2.请求重定向：response.sendRedirect()客户端行为，response.sendRedirect(),从本质上讲等同于两次请求，前一次的请求对象不会保存，地址栏的URL地址会改变。]]></content>
      <tags>
        <tag>java</tag>
        <tag>jsp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Session]]></title>
    <url>%2F2017%2F03%2F25%2F2017-03-25-session%2F</url>
    <content type="text"><![CDATA[SessionSession技术是将数据存储在服务器端的技术，会为每个客户端都创建一块内存空间存储客户的数据，但客户端需要每次都携带一个标识ID去服务器中寻找属于自己的内存空间。所以说Session的实现是基于Cookie，Session需要借助于Cookie存储客户的唯一性标识JSESSIONID Session的使用1．获得Session对象1HttpSession session = request.getSession(); 此方法会获得专属于当前会话的Session对象，如果服务器端没有该会话的Session对象会创建一个新的Session返回，如果已经有了属于该会话的Session直接将已有的Session返回（实质就是根据JSESSIONID判断该客户端是否在服务器上已经存在 session了） 2．向session中存取数据（session也是一个域对象）Session也是存储数据的区域对象，所以session对象也具有如下三个方法：123session.setAttribute(String name,Object obj);session.getAttribute(String name);session.removeAttribute(String name); 3．Session对象的生命周期创建：第一次执行request.getSession()时创建销毁：1.服务器(非正常)关闭时2.session过期/失效（默认30分钟,从不操作服务器端的资源开始计时）,可以在工程的web.xml中进行配置123&lt;session-config&gt; &lt;session-timeout&gt;30&lt;/session-timeout&gt;&lt;/session-config&gt; 3.手动销毁session1session.invalidate();]]></content>
      <tags>
        <tag>java</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cookie]]></title>
    <url>%2F2017%2F03%2F22%2F2017-03-22-cookie%2F</url>
    <content type="text"><![CDATA[CookieCookie技术是将用户的数据存储到客户端本地的技术优点：数据存储在客户端本地，减少服务器端的存储的压力缺点：安全性存在问题 Cookie的使用1．服务器端向客户端发送一个Cookie(1)创建Cookie：1Cookie cookie = new Cookie(String cookieName,String cookieValue); cookie会以响应头的形式发送给客户端注意：Cookie中不能存储中文(2)设置Cookie在客户端的持久化时间：1cookie.setMaxAge(int seconds); 注意：如果不设置持久化时间，cookie会存储在浏览器的内存中，浏览器关闭cookie信息销毁（会话级别的cookie），如果设置持久化时间，cookie信息会被持久化到浏览器的磁盘文件里cookie.setMaxAge(10*60);设置cookie信息在浏览器的磁盘文件中存储的时间是10分钟，过期浏览器会自动删除该cookie信息 (3)设置Cookie的路径：1cookie.setPath(String path); 注意：如果不设置携带路径，那么该cookie信息会在访问产生该cookie的web资源所在的路径都携带cookie信息示例： 123cookie.setPath("/WEB16"); // 代表访问WEB16应用中的任何资源都携带cookiecookie.setPath("/WEB16/cookieServlet"); // 代表访问WEB16中的cookieServlet时才携带cookie信息 (4)向客户端发送cookie： 1response.addCookie(Cookie cookie); (5)删除客户端的cookie：如果想删除客户端的已经存储的cookie信息，那么就使用同名同路径的持久化时间为0的cookie进行覆盖即可 12Cookie cookie = new Cookie("key", null);cookie.setMaxAge(0); 2．服务器端接受客户端携带的Cookiecookie信息是以请求头的方式发送到服务器端的(1)通过request获得所有的Cookie：1Cookie[] cookies = request.getCookies(); (2)遍历Cookie数组，通过Cookie的名称获得我们想要的Cookie12345for(Cookie cookie : cookies)&#123; if(cookie.getName().equal(cookieName))&#123; String cookieValue = cookie.getValue(); &#125;&#125;]]></content>
      <tags>
        <tag>java</tag>
        <tag>cookie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDBC]]></title>
    <url>%2F2017%2F03%2F16%2F2017-03-16-jdbc%2F</url>
    <content type="text"><![CDATA[JDBC技术1.简介JDBC对Java程序员而言是API，对实现与数据库连接的服务提供商而言是接口模型。作为API，JDBC为程序开发提供标准的接口，并为数据库厂商及第三方中间件厂商实现与数据库的连接提供了标准方法。简单地说，JDBC 可做三件事：与数据库建立连接、发送 操作数据库的语句并处理结果。 2.执行过程第一步：Class.forName()加载数据库连接驱动；第二步：DriverManager.getConnection()获取数据连接对象;第三步：根据 SQL 获取 sql 会话对象，有 2 种方式 Statement、PreparedStatement ;第四步：执行 SQL 处理结果集，执行 SQL 前如果有参数值就设置参数值 setXXX();第五步：关闭结果集、关闭会话、关闭连接。下列代码段给出了JDBC执行过程的基本示例：123456789Class.forName("com.mysql.jdbc.Driver");Connection con = DriverManager.getConnection("jdbc:odbc:wombat","login","password");Statement stmt = con.createStatement();ResultSet rs = stmt.executeQuery("SELECT a, b, c FROM Table1");while (rs.next()) &#123; int x = rs.getInt("a"); String s = rs.getString("b"); float f = rs.getFloat("c");&#125; 3.相关知识点PreparedStatement的优点1、PreparedStatement 接口继承 Statement，PreparedStatement 实例包含已编译的 SQL 语句，所以其执行速度要快于 Statement 对象。2、作为Statement的子类，PreparedStatement继承了Statement的所有功能。三种方法 execute、 executeQuery 和 executeUpdate 已被更改以使之不再需要参数。3、在 JDBC 应用中,在任何时候都不要使用 Statement，原因如下：一、代码的可读性和可维护性.Statement 需要不断地拼接，而 PreparedStatement 不会。二、PreparedStatement 尽最大可能提高性能.DB 有缓存机制，相同的预编译语句再次被调用不会再次需要编译。三、最重要的一点是极大地提高了安全性.Statement 容易被 SQL 注入，而 reparedStatementc 传入的内容不会和 sql 语句发生任何匹配关系。 关系型数据库中连接池的机制前提：为数据库连接建立一个缓冲池。1：从连接池获取或创建可用连接2：使用完毕之后，把连接返回给连接池3：在系统关闭前，断开所有连接并释放连接占用的系统资源4：能够处理无效连接，限制连接池中的连接总数不低于或者不超过某个限定值。 事物处理Connection类中提供了3个事务处理方法: setAutoCommit(Boolean autoCommit):设置是否自动提交事务,默认为自动提交,即为true,通过设置false禁止自动提交事务; commit():提交事务; rollback():回滚事务]]></content>
      <tags>
        <tag>java</tag>
        <tag>mysql</tag>
        <tag>jdbc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android ContentProvider]]></title>
    <url>%2F2017%2F03%2F12%2F2017-03-12-android-contentprovider%2F</url>
    <content type="text"><![CDATA[1 ContentProviderContentProvider在android中的作用是可以通过ContentProvider把应用中的数据共享给其他应用访问，其他应用可以通过ContentProvider对应用中的数据进行增、删、改、查。使用ContentProvider对外共享数据的好处是，统一了数据的访问方式，它实际上是对SQLiteOpenHelper的进一步封装，通过Uri映射来判断选择需要曹组哦数据库中的哪个表，并且进行增、删、改、查处理。]]></content>
      <tags>
        <tag>java</tag>
        <tag>android</tag>
        <tag>ContentProvider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Broadcast]]></title>
    <url>%2F2017%2F02%2F26%2F2017-02-26-android-broadcast%2F</url>
    <content type="text"><![CDATA[1 Broadcast(广播)Broadcast是一种广泛运用的、在应用程序之间传输信息的机制，一个广播可以有任意个接收者。广播机制是一个典型的发布——订阅模式，最大的特点就是发送方不关心接收方是否接到数据，也不关心是如何处理数据的，通过这样的形式来达到接、收双方的完全解耦合。 Android中的广播使用了设计模式中的观察者模式：基于消息的发布 / 订阅事件模型 Android广播机制包含3个基本要素，分别是用于发送广播的Broadcast、接收广播的BroadcastReceiver以及用于传递信息的Intent。Android广播可分为普通广播、有序广播、本地广播和Sticky。 1.1 普通广播 普通广播是完全异步的，通过Context的sendBroadcast()函数发送，优点：消息传递的效率比较高。缺点：receivers(接收器)的执行顺序不确定，接收者不能讲处理结果传递给下一个接收者，并且无法终止广播Intent的传播，直到没有与之匹配的广播接收器为止。1、首先定义一个广播接收器，如下：123456public class HelloBroadcastReceiver extends BroadcaseReceiver &#123; @Override public void onReceive(Context context, Intent intent) &#123; Toast.makeText(context, "hello", Toast.LENGTH_LONG).show(); &#125;&#125; 2、注册广播，可以通过AndroidManifest.xml静态注册或者代码动态注册。 AndroidManifest.xml静态注册：&lt;receiver android:name=&quot;.broadcast.HelloBroadcastReceiver&quot;&gt;&lt;/receiver&gt; 代码动态注册：123private void registerHelloBroadcast() &#123; registerReceiver(new HelloBroadcastReceiver(), new IntentFilter(HELLO_ACTION));&#125; 如果是在Activity或者Fragment中动态测试，那么不要忘了在执行onDestory时注销该广播。 3、发送广播123private void sendNormalBroadcast() &#123; sendBroadcast(new Intent(HELLO_ACTION));&#125; 然后就会调用HelloBroadcastReceiver的onReceive函数，在该函数中执行相关操作即可。 1.2 有序广播有序广播通过Context.sendOrderedBroadcast()来发送，所有的广播接收器按照优先级依次执行，广播接收器的优先级通过AndroidManifest.xml中的receiver的intent-filter中的android:priority属性来设置，数值越大优先级越高。当广播接收器接收到广播后，可以使用setResult()函数来将结果传给下一个广播接收器接收，然后通过getResult()函数来取得上个广播接收器返回的结果，并可以用abortBroadcast()函数来让系统丢弃该广播，使该广播不再传送到别的广播接收器。 1.3 本地广播之前的广播都是全局的，所有应用程序都可以接收到，这样就会带来安全隐患。LocalBroadcastManager能够实现限于应用内的广播，只是进程内使用，提高程序的安全性。 1.4 sticky广播sticky广播通过Context.sendStickyBroadcast()函数来发送，用此函数发送的广播会一直滞留，当有匹配此广播的广播接收器被注册后，该广播接收器就会收到此条广播。使用此函数发送广播时，需要获得BROADCAST_STICKY权限： &lt;uses-permission android:name=&quot;android.permission.BROADCAST_STICKY&quot; /&gt; sendStickyBroadcast只保留最后一条广播，并且一直保留下去，这样即使已经有广播接收器处理了该广播，当再有匹配的广播接收器被注册时，此广播仍会被接收。如果你只想处理一遍该广播，可以通过removeStickyBroadcast函数实现。]]></content>
      <tags>
        <tag>java</tag>
        <tag>android</tag>
        <tag>Broadcast</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java核心知识体系]]></title>
    <url>%2F2017%2F02%2F13%2F2017-02-13-java-core-knowledge%2F</url>
    <content type="text"><![CDATA[JVMJava集合Java多线程并发Java基础数据结构Java算法加密算法Spring原理数据库网络日志微服务负载均衡一致性算法分布式缓存ZookeeperKafkaRabbitMQNetty与RPCHBaseMongoDBCassandra设计模式HadoopSparkStormYarn机器学习云计算]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Service]]></title>
    <url>%2F2017%2F02%2F12%2F2017-02-12-android-service%2F</url>
    <content type="text"><![CDATA[1 Service与AIDLService是Android中实现程序后台运行的解决方案，适合用于那些不需要和用户交互而且还要求长期运行的任务。但不要被“后台”二字所迷惑，Service默认并不会运行在子线程中，它也不运行在一个独立的进程中，它同样执行在UI线程中，因此，不要在Service中执行耗时的操作，除非你在Service中创建了子线程来完成耗时操作。 Service的运行不依赖于任何用户界面，即使程序被切换到后台或者用户打开了另一个应用程序，Service仍然能够保持正常运行，这也正是Service的使用场景。当某个应用程序进程被杀掉时，所有依赖于该进程的Service也会停止运行。 1.1 普通ServiceService的生命周期只有三个，分别为onCreate、onStartCommand和onDestory。一旦在项目的任何位置调用了Context的startService()函数，相应的服务就会启动起来，首次创建时会调用onCreate函数，然后回调onStartCommand()函数。服务启动之后会一直保持运行状态，直到stopService()或stopSelf()函数被调用。虽然每调用一次startService()函数，onStartCommand()就会执行一次，但实际上每个服务都只会存在一个实例。所以不管你调用了多少次startService()函数，只需调用一个stopService()或stopSelf()函数，服务就会被停止。 与Activity一样，Service也需要在AndroidManifest.xml中进行注册。 1.2 IntentServiceIntentService将用户的请求执行在一个子线程中，用户只需要覆写onHandleIntent函数，并且在该函数中完成自己的耗时操作即可。需要注意的是，在任务执行完毕之后IntentService会调用stopSelf自我销毁，因此，它适用于完成一些短期的耗时任务。 1.3 运行在前台的Service将Service运行在前台不仅不会被系统无情地回收，它还会在通知栏显示一条消息，下拉状态栏后可以看到更加详细的信息。例如，墨迹天气在前台运行了一个Service，并且在Service中定时更新通知栏上的天气信息。 1.4 AIDLAndroid Interface Definition Language(Android接口定义语言)，同行用于进程间通信（Android系统中的进程之间不能共享内存）。 Note: Using AIDL is necessary only if you allow clients from different applications to access your service for IPC and want to handle multithreading in your service. If you do not need to perform concurrent IPC across different applications, you should create your interface by implementing a Binder or, if you want to perform IPC, but do not need to handle multithreading, implement your interface using a Messenger. Regardless, be sure that you understand Bound Services before implementing an AIDL. “只有当你允许来自不同的客户端访问你的服务并且需要处理多线程问题时你才必须使用AIDL”，其他情况下你都可以选择其他方法，如使用Messager，也能跨进程通讯。可见AIDL是处理多线程、多客户端并发访问的。而Messager是单线程处理。]]></content>
      <tags>
        <tag>java</tag>
        <tag>android</tag>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Activity]]></title>
    <url>%2F2017%2F02%2F02%2F2017-02-02-android-activity%2F</url>
    <content type="text"><![CDATA[1 Activity1.1 Activity的生命周期1.onCreate() onCreate()方法会在Activity第一次被创建时调用，通常会在这个函数中完成Activity的初始化操作，如设置布局、初始化视图、绑定事件等。 2.onStart() 这个函数在Activity可见之前被调用。 3.onResume() 这个函数在Activity变为可见时被调用，执行完onResume之后，Activity就会请求AMS渲染它所管理的视图。此时的Activity一定位于返回栈的栈顶，并且处于运行状态。可见的、有焦点的。 4.onPause() 这个函数在Activity失去焦点，从可见变为不完全可见时调用。 5.onStop() 这个函数在Activity完全不可见时调用。 6.onDestroy() 这个函数在Activity被销毁之前调用，之后Activity的状态变为销毁状态。在这个函数里释放内存。 7.onRestart() 这个函数在Activity由停止状态重新变为运行状态之前调用，下一个调用onStart()。 ###1.2 Activity的构成 ### PhoneWindow→DecorView→DefultLayout→ViewGroup:mContentParent→用户自己的xml布局 1.3 Activity的4种启动模式1.standard 在这种模式下启动的Activity可以被多次实例化，每启动一个Activity都会在栈顶创建一个新的实例。实际开发中，闹钟程序通常使采用这种模式。如果Activity是一个非常耗资源的类，那么将会使应用消耗更多的系统资源。 2.singleTop singleTop模式启动Activity时，首先会判断要启动Acitity实例是否位于栈顶，如果位于栈顶则直接复用，否则与standard模式相同，创建一个新的实例。实际开发中，浏览器的书签通常采用这种模式。 3.singleTask singleTask模式可以保证一个任务栈中只能有一个该Activity实例。每次启动该Activity时，首先会判断该Acitivity是否存在任务栈中，如果已存在，系统会销毁该Activity之上的所有Activity实例，最终让该Activity实例位于栈顶。如果任务栈中没有该Activity实例，会新创建一个实例并放在栈顶。在实际开发中，浏览器主界面通常采用这种模式。 4.singleInstance 设置为singleInstance模式的Activity会启动一个独立的任务栈来管理Activity实例，并且这个任务栈中有且只有一个实例。如果要启动的Activity已存在，无论当前Activity位于哪个程序哪个任务栈中，系统都会把Activity所在的任务栈转移到前台，从而使Activity显示。实际开发中，来电界面通常采用这种模式。]]></content>
      <tags>
        <tag>java</tag>
        <tag>android</tag>
        <tag>activity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android四大组件]]></title>
    <url>%2F2017%2F01%2F28%2F2017-01-28-android-components%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>java</tag>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[入门训练 Fibonacci数列]]></title>
    <url>%2F2017%2F01%2F18%2F2017-01-18-introductory-training-fibonacci-sequence%2F</url>
    <content type="text"><![CDATA[问题描述Fibonacci数列的递推公式为：Fn=Fn-1+Fn-2，其中F1=F2=1。 当n比较大时，Fn也非常大，现在我们想知道，Fn除以10007的余数是多少。 输入格式输入包含一个整数n。 输出格式输出一行，包含一个整数，表示Fn除以10007的余数。 说明：在本题中，答案是要求Fn除以10007的余数，因此我们只要能算出这个余数即可，而不需要先计算出Fn的准确值，再将计算的结果除以10007取余数，直接计算余数往往比先算出原数再取余简单。 样例输入10 样例输出55 样例输入22 样例输出7704 数据规模与约定1 &lt;= n &lt;= 1,000,000 提交序号 1209677提交时间 2017-01-18 19:05:30评测结果 正确得分 100CPU使用 250ms内存使用 21.17MB试题名称 入门训练 Fibonacci数列语言 JAVA源代码12345678910111213141516171819202122import java.util.Scanner;public class Main &#123; public static void main(String[] args) &#123; int a1,a2; a1=a2=1; int sum=0,temp;//sum是保存余数的变量 ，temp是为了方便交换数据 long n;//因为n&gt;=1 and n&lt;=1000000 long i; Scanner scanner = new Scanner(System.in); n = scanner.nextLong(); for(i=1;i&lt;=n;i++) &#123; sum=a1 % 10007; temp=a2; a2=(a1+a2) % 10007; a1=temp; &#125; System.out.println(sum); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery]]></title>
    <url>%2F2016%2F08%2F26%2F2016-08-26-jquery%2F</url>
    <content type="text"><![CDATA[JQuery 是什么? javascript 的代码框架。 有什么用? 简化代码，提高效率。 核心 write less do more , 写得更少，做的更多。 load12345678910&lt;a href="" onclick="load()"&gt;使用JQuery执行load方法&lt;/a&gt;有两次刷新， 先走 onClick的方法，取到数据回来之后，赋值显示。 接着 走 href=""的路径，但是这个属性没有给值，所以会把当前的页面重新再刷新一次。所以导致看不见值。//找到这个元素， 去执行加载的动作， 加载/day16/DemoServlet02 ， 得到的数据，赋值显示$("#aaa").load("/day16/DemoServlet02" , function(responseText , statusTXT , xhr) &#123; //找到id为text01的元素， 设置它的value属性值为 responseText 对应的值 $("#aaa").val(responseText); &#125;); Get123$.get("/day16/DemoServlet02" , function(data ,status) &#123; $("#div01").text(data); &#125;); 赋值显示 val(“aa”); 只能放那些标签带有value属性 html(“aa”); —写html代码 text(“aa”); 其实没有什么区别，如果想针对这分数据做html样式处理，那么只能用html() load &amp; get load 123$("#元素id").load(url地址);$("#div1").load(serlvet); ---&gt; 使用的get请求，回来赋值的时候， 使用text（）;去赋值 get语法格式 ： 1$.get(URL,callback); 使用案例：123$.get("/day16/DemoServlet02" , function(data ,status) &#123; $("#div01").text(data);&#125;); post语法格式：1$.post(URL,data,callback); 123456function post() &#123; $.post("/day16/DemoServlet02", &#123;name:"zhangsan",age:18&#125;,function(data,status) &#123; //想要放数据到div里面去。 --- 找到div $("#div01").html(data); &#125;);&#125; 使用JQuery去实现校验用户名1234567891011121314151617function checkUserName() &#123; //1. 获取输入框的内容 var name = $("#name").val(); //2. 发送请求 $.post("/day16/CheckUserNameServlet" , &#123;name:name&#125; , function(data , status)&#123; //alert(data); if(data == 1)&#123;//用户名存在 //alert("用户名存在"); $("#span01").html("&lt;font color='red'&gt;用户名已被注册&lt;/font&gt;"); &#125;else&#123; //alert("用户名可用"); $("#span01").html("&lt;font color='green'&gt;用户名可以使用&lt;/font&gt;"); &#125; &#125; ); //3. 输出响应的数据到页面上。&#125; 实现百度搜索提示搭建环境 定义首页 123456789&lt;body&gt; &lt;center&gt; &lt;h2&gt;百度&lt;/h2&gt; &lt;input type="text" name="word" id="word" style="width: 600px ; height: 50px ;font-size: 20px;"&gt; &lt;input type="button" value="百度一下" style="height: 55px ; width: 100px ; "&gt; &lt;div id="div01" style="position:relative; left : -54px; width: 600px; height: 200px ; border: 1px solid blue; display: none"&gt;&lt;/div&gt; &lt;/center&gt;&lt;/body&gt; 定义数据库 捕获键盘弹起12345$(function()&#123; $("#word").keyup(function() &#123; alert("键盘弹起了.."); &#125;)&#125;); JS请求123456789101112131415161718192021$(function()&#123; $("#word").keyup(function() &#123; //2。 获取输入框的值 //var word = $("#word").val(); //this 对应就是执行这个方法的那个对象， $("#word") var word = $(this).val(); if(word == "")&#123; $("#div01").hide(); &#125;else&#123; //3. 请求数据。 $.post("find",&#123;word:word&#125; ,function(data , status)&#123; //alert(data); $("#div01").show(); $("#div01").html(data); &#125;); &#125; &#125;)&#125;); Servlet代码123456789101112131415161718192021222324252627protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding("utf-8"); try &#123; //1. 先获取参数 String word = request.getParameter("word"); System.out.println("word="+word); //2. 让dao执行查询 WordsDao dao = new WordsDaoImpl(); List&lt;WordBean&gt; list = dao.findWords(word); for (WordBean wordBean : list) &#123; System.out.println("==="+wordBean.toString()); &#125; request.setAttribute("list", list); //3. 返回数据 response.setContentType("text/html;charset=utf-8"); //response.getWriter().write("数据是："); request.getRequestDispatcher("list.jsp").forward(request, response); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125; list.jsp12345678910111213&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c"%&gt;​ &lt;table style="width: 100%"&gt; &lt;c:forEach items="$&#123;list &#125;" var="wordBean"&gt; &lt;tr&gt; &lt;td&gt;$&#123;wordBean.words&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt;&lt;/table&gt; 使用JQuery实现 省市联动环境准备 准备数据库 2 。 准备页面1234567891011121314151617&lt;script type="text/javascript" src="js/jquery-1.11.3.min.js"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="js/city.js"&gt;&lt;/script&gt;&lt;/head&gt; &lt;body&gt; 省份: &lt;select name="province" id ="province"&gt; &lt;option value="" &gt;-请选择 - &lt;option value="1" &gt;广东 &lt;option value="2" &gt;湖南 &lt;option value="3" &gt;湖北 &lt;option value="4" &gt;四川 &lt;/select&gt; 城市: &lt;select name="city" id="city"&gt; &lt;option value="" &gt;-请选择 - &lt;/select&gt; &lt;/body&gt; XStream的使用1234567891011//3. 返回数据。手动拼 ---&gt; XStream 转化 bean对象成 xml XStream xStream = new XStream(); //想把id做成属性 xStream.useAttributeFor(CityBean.class, "id"); //设置别名 xStream.alias("city", CityBean.class); //转化一个对象成xml字符串 String xml = xStream.toXML(list); JS代码12345678910111213141516171819202122232425262728293031323334353637383940$(function() &#123; //1。找到省份的元素 $("#province").change(function() &#123; //2. 一旦里面的值发生了改变，那么就去请求该省份的城市数据 //$("#province").varl(); var pid = $(this).val(); /*&lt;list&gt; &lt;city&gt; &lt;id&gt;1&lt;id&gt; &lt;pid&gt;1&lt;/pid&gt; &lt;cname&gt;深圳&lt;/cname&gt; &lt;/city&gt; &lt;city &gt; &lt;id&gt;2&lt;id&gt; &lt;pid&gt;1&lt;/pid&gt; &lt;cname&gt;东莞&lt;/cname&gt; &lt;/city&gt; &lt;/list&gt;*/ $.post( "CityServlet",&#123;pid:pid&#125; ,function(data,status)&#123; //alert("回来数据了:"+data); //先清空以前的值： $("#city").html("&lt;option value='' &gt;-请选择-") //遍历： //从data数据里面找出所有的city ， 然后遍历所有的city。 //遍历一个city，就执行一次function方法 $(data).find("city").each(function() &#123; //遍历出来的每一个city，取它的孩子。 id , cname var id = $(this).children("id").text(); var cname = $(this).children("cname").text(); $("#city").append("&lt;option value='"+id+"' &gt;"+cname) &#125;); &#125; ); &#125;);&#125;); 服务器和客户端数据传输的方式 xml 123456789101112&lt;list&gt; &lt;city&gt; &lt;id&gt;1&lt;id&gt; &lt;pid&gt;1&lt;/pid&gt; &lt;cname&gt;深圳&lt;/cname&gt; &lt;/city&gt; &lt;city &gt; &lt;id&gt;2&lt;id&gt; &lt;pid&gt;1&lt;/pid&gt; &lt;cname&gt;东莞&lt;/cname&gt; &lt;/city&gt;&lt;/list&gt; json 阅读性更好 、 容量更小。 {“name”:”aaa” , “age”:19} 把javaBean 转化成 json数据123456//3. 把list ---&gt; json数据//JSONArray ---&gt; 变成数组 ， 集合 []//JSONObject ---&gt; 变成简单的数据 &#123; name : zhangsan , age:18&#125;JSONArray jsonArray = JSONArray.fromObject(list);String json = jsonArray.toString(); 使用json格式数据显示省市联动效果serlvet代码： 123456789101112131415161718192021222324252627protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; try &#123; //1. 获取参数 int pid = Integer.parseInt(request.getParameter("pid")); //2 找出所有的城市 CityDao dao = new CityDaoImpl(); List&lt;CityBean&gt; list = dao.findCity(pid); //3. 把list ---&gt; json数据 //JSONArray ---&gt; 变成数组 ， 集合 [] //JSONObject ---&gt; 变成简单的数据 &#123; name : zhangsan , age:18&#125; JSONArray jsonArray = JSONArray.fromObject(list); String json = jsonArray.toString(); response.setContentType("text/html;charset=utf-8"); response.getWriter().write(json); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;;&#125; js代码 123456789101112131415161718192021222324252627282930313233$(function() &#123; //1。找到省份的元素 $("#province").change(function() &#123; //2. 一旦里面的值发生了改变，那么就去请求该省份的城市数据 //$("#province").varl(); var pid = $(this).val(); /*[ &#123; "cname": "深圳", "id": 1, "pid": 1 &#125;, &#123; "cname": "东莞", "id": 2, "pid": 1 &#125; ... ]*/ $.post( "CityServlet02",&#123;pid:pid&#125; ,function(data,status)&#123; //先清空 $("#city").html("&lt;option value='' &gt;-请选择-"); //再遍历，追加 $(data).each(function(index , c) &#123; $("#city").append("&lt;option value='"+c.id+"' &gt;"+c.cname) &#125;); &#125;,"json" ); &#125;);&#125;); 总结JQuery发送get请求 发送post请求 都要求带数据 + 获取数据＋ 放置到元素上。 --------------------------------------- 1. 服务器返回xml数据 2. 服务器返回json数据]]></content>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax]]></title>
    <url>%2F2016%2F08%2F16%2F2016-08-16-ajax%2F</url>
    <content type="text"><![CDATA[Ajax 是什么? “Asynchronous Javascript And XML”（异步JavaScript和XML）， 并不是新的技术，只是把原有的技术，整合到一起而已。 1.使用CSS和XHTML来表示。 2.使用DOM模型来交互和动态显示。 3.使用XMLHttpRequest来和服务器进行异步通信。 4.使用javascript来绑定和调用。 有什么用? 咱们的网页如果想要刷新局部内容。 那么需要重新载入整个网页。用户体验不是很好。 就是为了解决局部刷新的问题。 保持其他部分不动，只刷新某些地方。 数据请求 Get1.创建对象1234567891011121314151617function ajaxFunction()&#123; var xmlHttp; try &#123; // Firefox, Opera 8.0+, Safari xmlHttp=new XMLHttpRequest(); &#125; catch (e)&#123; try&#123;// Internet Explorer xmlHttp=new ActiveXObject("Msxml2.XMLHTTP"); &#125; catch (e)&#123; try&#123; xmlHttp=new ActiveXObject("Microsoft.XMLHTTP"); &#125; catch (e)&#123;&#125; &#125; &#125; return xmlHttp; &#125; 发送请求 12345678910111213141516171819202122232425262728293031323334353637383940414243//执行get请求function get() &#123; //1. 创建xmlhttprequest 对象 var request = ajaxFunction(); //2. 发送请求。 // http://localhost:8080/day16/demo01.jsp //http://localhost:8080/day16/DemoServlet01 /* 参数一： 请求类型 GET or POST 参数二： 请求的路径 参数三： 是否异步， true or false */ request.open("GET" ,"/day16/DemoServlet01" ,true ); request.send();&#125;如果发送请求的同时，还想获取数据，那么代码如下//执行get请求function get() &#123; //1. 创建xmlhttprequest 对象 var request = ajaxFunction(); //2. 发送请求 request.open("GET" ,"/day16/DemoServlet01?name=aa&amp;age=18" ,true ); //3. 获取响应数据 注册监听的意思。 一会准备的状态发生了改变，那么就执行 = 号右边的方法 request.onreadystatechange = function()&#123; //前半段表示 已经能够正常处理。 再判断状态码是否是200 if(request.readyState == 4 &amp;&amp; request.status == 200)&#123; //弹出响应的信息 alert(request.responseText); &#125; &#125; request.send();&#125; 数据请求 Post1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;script type="text/javascript"&gt; //1. 创建对象function ajaxFunction()&#123; var xmlHttp; try&#123; // Firefox, Opera 8.0+, Safari xmlHttp=new XMLHttpRequest(); &#125; catch (e)&#123; try&#123;// Internet Explorer xmlHttp=new ActiveXObject("Msxml2.XMLHTTP"); &#125; catch (e)&#123; try&#123; xmlHttp=new ActiveXObject("Microsoft.XMLHTTP"); &#125; catch (e)&#123;&#125; &#125; &#125; return xmlHttp; &#125;function post() &#123; //1. 创建对象 var request = ajaxFunction(); //2. 发送请求 request.open( "POST", "/day16/DemoServlet01", true ); //如果不带数据，写这行就可以了 //request.send(); //如果想带数据，就写下面的两行 //如果使用的是post方式带数据，那么 这里要添加头， 说明提交的数据类型是一个经过url编码的form表单数据 request.setRequestHeader("Content-type","application/x-www-form-urlencoded"); //带数据过去 ， 在send方法里面写表单数据。 request.send("name=aobama&amp;age=19");&#125;&lt;/script&gt;需要获取数据function post() &#123; //1. 创建对象 var request = ajaxFunction(); //2. 发送请求 request.open( "POST", "/day16/DemoServlet01", true ); //想获取服务器传送过来的数据， 加一个状态的监听。 request.onreadystatechange=function()&#123; if(request.readyState==4 &amp;&amp; request.status == 200)&#123; alert("post："+request.responseText); &#125; &#125; //如果使用的是post方式带数据，那么 这里要添加头， 说明提交的数据类型是一个经过url编码的form表单数据 request.setRequestHeader("Content-type","application/x-www-form-urlencoded"); //带数据过去 ， 在send方法里面写表单数据。 request.send("name=aobama&amp;age=19");&#125; 校验用户名是否可用1. 搭建环境 页面准备 1234567891011121314151617181920212223&lt;body&gt; &lt;table border="1" width="500"&gt; &lt;tr&gt; &lt;td&gt;用户名:&lt;/td&gt; &lt;td&gt;&lt;input type="text" name="name" id="name" onblur="checkUserName()"&gt;&lt;span id="span01"&gt;&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;密码&lt;/td&gt; &lt;td&gt;&lt;input type="text" name=""&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;邮箱&lt;/td&gt; &lt;td&gt;&lt;input type="text" name=""&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;简介&lt;/td&gt; &lt;td&gt;&lt;input type="text" name=""&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan="2"&gt;&lt;input type="submit" value="注册"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt; 数据库准备 2. Servlet代码1234567891011121314151617181920212223protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; try &#123; request.setCharacterEncoding("UTF-8"); //1. 检测是否存在 String name = request.getParameter("name"); System.out.println("name="+name); UserDao dao = new UserDaomImpl(); boolean isExist = dao.checkUserName(name); //2. 通知页面，到底有还是没有。 if(isExist)&#123; response.getWriter().println(1); //存在用户名 &#125;else&#123; response.getWriter().println(2); //不存在该用户名 &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125; 3. Dao代码12345678910111213141516public class UserDaomImpl implements UserDao&#123; @Override public boolean checkUserName(String username) throws SQLException &#123; QueryRunner runner = new QueryRunner(JDBCUtil02.getDataSource()); String sql = "select count(*) from t_user where username =?"; runner.query(sql, new ScalarHandler(), username); Long result = (Long) runner.query(sql, new ScalarHandler(), username); return result &gt; 0 ; &#125;&#125; jsp页面显示​123456789101112131415161718192021222324252627282930function checkUserName() &#123; //获取输入框的值 document 整个网页 var name = document.getElementById("name").value; // value value() val val() //1. 创建对象 var request = ajaxFunction(); //2. 发送请求 request.open("POST" ,"/day16/CheckUserNameServlet" , true ); //注册状态改变监听，获取服务器传送过来的数据 request.onreadystatechange = function()&#123; if(request.readyState == 4 &amp;&amp; request.status == 200)&#123; //alert(request.responseText); var data = request.responseText; if(data == 1)&#123; //alert("用户名已存在"); document.getElementById("span01").innerHTML = "&lt;font color='red'&gt;用户名已存在!&lt;/font&gt;"; &#125;else&#123; document.getElementById("span01").innerHTML = "&lt;font color='green'&gt;用户名可用!&lt;/font&gt;"; //alert("用户名未存在"); &#125; &#125; &#125; request.setRequestHeader("Content-type","application/x-www-form-urlencoded"); request.send("name="+name);&#125; 总结Ajax发送get请求 发送post请求 都要求带数据 + 获取数据＋ 放置到元素上。]]></content>
      <tags>
        <tag>ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七大查找算法汇总]]></title>
    <url>%2F2016%2F08%2F08%2F2016-08-08-search-algorithm%2F</url>
    <content type="text"><![CDATA[1.顺序查找2.二分查找3.插值查找4.斐波那契查找5.树表查找6.分块查找7.哈希查找 查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。本文简单概括性的介绍了常见的七种查找算法，说是七种，其实二分查找、插值查找以及斐波那契查找都可以归为一类——插值查找。插值查找和斐波那契查找是在二分查找的基础上的优化查找算法。查找定义：根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。查找算法分类：（1）静态查找和动态查找； 注：静态或者动态都是针对查找表而言的。动态表指查找表中有删除和插入操作的表。（2）无序查找和有序查找。 无序查找：被查找数列有序无序均可； 有序查找：被查找数列必须为有序数列。平均查找长度（Average Search Length，ASL）：需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度。对于含有n个数据元素的查找表，查找成功的平均查找长度为：ASL = Pi*Ci的和。Pi：查找表中第i个数据元素的概率。Ci：找到第i个数据元素时已经比较过的次数。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七、哈希查找]]></title>
    <url>%2F2016%2F08%2F04%2F2016-08-04-hash-search%2F</url>
    <content type="text"><![CDATA[哈希查找什么是哈希表（Hash）？我们使用一个下标范围比较大的数组来存储元素。可以设计一个函数（哈希函数， 也叫做散列函数），使得每个元素的关键字都与一个函数值（即数组下标）相对应，于是用这个数组单元来存储这个元素；也可以简单的理解为，按照关键字为每一个元素”分类”，然后将这个元素存储在相应”类”所对应的地方。但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素，却计算出了相同的函数值，这样就产生了”冲突”，换句话说，就是把不同的元素分在了相同的”类”之中。后面我们将看到一种解决”冲突”的简便做法。 总的来说，”直接定址”与”解决冲突”是哈希表的两大特点。 什么是哈希函数？哈希函数的规则是：通过某种转换关系，使关键字适度的分散到指定大小的的顺序结构中，越分散，则以后查找的时间复杂度越小，空间复杂度越高。 算法思想：哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。 算法流程： 1）用给定的哈希函数构造哈希表； 2）根据选择的冲突处理方法解决地址冲突； 常见的解决冲突的方法：拉链法和线性探测法。详细的介绍可以参见：浅谈算法和数据结构: 十一 哈希表。 3）在哈希表的基础上执行哈希查找。 哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 复杂度分析： 单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(n)（注意，在查找之前我们需要构建相应的Hash表）。 使用Hash，我们付出了什么？我们在实际编程中存储一个大规模的数据，最先想到的存储结构可能就是map，也就是我们常说的KV pair，经常使用Python的博友可能更有这种体会。使用map的好处就是，我们在后续处理数据处理时，可以根据数据的key快速的查找到对应的value值。map的本质就是Hash表，那我们在获取了超高查找效率的基础上，我们付出了什么？ Hash是一种典型以空间换时间的算法，比如原来一个长度为100的数组，对其查找，只需要遍历且匹配相应记录即可，从空间复杂度上来看，假如数组存储的是byte类型数据，那么该数组占用100byte空间。现在我们采用Hash算法，我们前面说的Hash必须有一个规则，约束键与存储位置的关系，那么就需要一个固定长度的hash表，此时，仍然是100byte的数组，假设我们需要的100byte用来记录键与位置的关系，那么总的空间为200byte,而且用于记录规则的表大小会根据规则，大小可能是不定的。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六、分块查找]]></title>
    <url>%2F2016%2F07%2F25%2F2016-07-25-block-search%2F</url>
    <content type="text"><![CDATA[分块查找分块查找又称索引顺序查找，它是顺序查找的一种改进方法。 算法思想：将n个数据元素”按块有序”划分为m块（m ≤ n）。每一块中的结点不必有序，但块与块之间必须”按块有序”；即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字；而第2块中任一元素又都必须小于第3块中的任一元素，…… 算法流程：step1 先选取各块中的最大关键字构成一个索引表；step2 查找分两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五、树表查找]]></title>
    <url>%2F2016%2F07%2F18%2F2016-07-18-tree-table-search%2F</url>
    <content type="text"><![CDATA[树表查找 最简单的树表查找算法——二叉树查找算法。 基本思想：二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后在就行和每个节点的父节点比较大小，查找最适合的范围。 这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树。 二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树： 1）若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 2）若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 3）任意节点的左、右子树也分别为二叉查找树。 二叉查找树性质：对二叉查找树进行中序遍历，即可得到有序的数列。 复杂度分析：它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四、斐波那契查找]]></title>
    <url>%2F2016%2F07%2F12%2F2016-07-12-fibonacci-search%2F</url>
    <content type="text"><![CDATA[在介绍斐波那契查找算法之前，我们先介绍一下很它紧密相连并且大家都熟知的一个概念——黄金分割。 黄金比例又称黄金分割，是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1:0.618或1.618:1。 0.618被公认为最具有审美意义的比例数字，这个数值的作用不仅仅体现在诸如绘画、雕塑、音乐、建筑等艺术领域，而且在管理、工程设计等方面也有着不可忽视的作用。因此被称为黄金分割。 大家记不记得斐波那契数列：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89…….（从第三个数开始，后边每一个数都是前两个数的和）。然后我们会发现，随着斐波那契数列的递增，前后两个数的比值会越来越接近0.618，利用这个特性，我们就可以将黄金比例运用到查找技术中。斐波那契搜索就是在二分查找的基础上根据斐波那契数列进行分割的。在斐波那契数列找一个等于略大于查找表中元素个数的数F[n]，将原查找表扩展为长度为Fn，完成后进行斐波那契分割，即F[n]个元素分割为前半部分F[n-1]个元素，后半部分F[n-2]个元素，找出要查找的元素在那一部分并递归，直到找到。基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素比较，比较结果分三种情况： 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1; 3）&lt;，high=mid-1。 斐波那契查找与折半查找很相似，他是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契数小1，及n=F(k)-1; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1),比较结果也分为三种 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1,k-=2; 说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为n-(F(k-1))= Fk-1-F(k-1)=Fk-F(k-1)-1=F(k-2)-1个，所以可以递归的应用斐波那契查找。 3）&lt;，high=mid-1,k-=1。 说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归 的应用斐波那契查找。 复杂度分析：最坏情况下，时间复杂度为O(log2n)，且其期望复杂度也为O(log2n)。C++代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// 斐波那契查找.cpp#include "stdafx.h"#include &lt;memory&gt;#include &lt;iostream&gt;using namespace std;const int max_size=20;//斐波那契数组的长度/*构造一个斐波那契数组*/void Fibonacci(int * F)&#123; F[0]=0; F[1]=1; for(int i=2;i&lt;max_size;++i) F[i]=F[i-1]+F[i-2];&#125;/*定义斐波那契查找法*/ int FibonacciSearch(int *a, int n, int key) //a为要查找的数组,n为要查找的数组长度,key为要查找的关键字&#123; int low=0; int high=n-1; int F[max_size]; Fibonacci(F);//构造一个斐波那契数组F int k=0; while(n&gt;F[k]-1)//计算n位于斐波那契数列的位置 ++k; int * temp;//将数组a扩展到F[k]-1的长度 temp=new int [F[k]-1]; memcpy(temp,a,n*sizeof(int)); for(int i=n;i&lt;F[k]-1;++i) temp[i]=a[n-1]; while(low&lt;=high) &#123; int mid=low+F[k-1]-1; if(key&lt;temp[mid]) &#123; high=mid-1; k-=1; &#125; else if(key&gt;temp[mid]) &#123; low=mid+1; k-=2; &#125; else &#123; if(mid&lt;n) return mid; //若相等则说明mid即为查找到的位置 else return n-1; //若mid&gt;=n则说明是扩展的数值,返回n-1 &#125; &#125; delete [] temp; return -1;&#125;int main()&#123; int a[] = &#123;0,16,24,35,47,59,62,73,88,99&#125;; int key=100; int index=FibonacciSearch(a,sizeof(a)/sizeof(int),key); cout&lt;&lt;key&lt;&lt;" is located at:"&lt;&lt;index; return 0;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三、插值查找]]></title>
    <url>%2F2016%2F07%2F06%2F2016-07-06-interpolation-search%2F</url>
    <content type="text"><![CDATA[插值查找在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。同样的，比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下：mid=(low+high)/2, 即mid=low+1/2(high-low);通过类比，我们可以将查找的点改进为如下：mid=low+(key-a[low])/(a[high]-a[low])(high-low)，也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 基本思想：基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为O(log2(log2n))。 Java代码实现12345678910111213//插值查找public int InsertionSearch(int a[], int value, int low, int high) &#123; int mid = low+(value-a[low])/(a[high]-a[low])*(high-low); if(a[mid]==value) &#123; return mid; &#125; if(a[mid]&gt;value) &#123; return InsertionSearch(a, value, low, mid-1); &#125; if(a[mid]&lt;value) &#123; return InsertionSearch(a, value, mid+1, high); &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二、二分查找]]></title>
    <url>%2F2016%2F06%2F30%2F2016-06-30-binary-search%2F</url>
    <content type="text"><![CDATA[2.二分查找说明：元素必须是有序的，如果是无序的则要先进行排序操作。 基本思想：也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下，关键词比较次数为log2(n+1)，且期望时间复杂度为O(log2n)； 注：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。——《大话数据结构》 Java代码实现12345678910111213141516171819202122232425262728293031323334//二分查找 版本1 循环public int BinarySearch1(int a[], int value, int n) &#123; int low, high, mid; low = 0; high = n-1; while(low&lt;=high) &#123; mid = (low+high)/2; if(a[mid]==value) &#123; return mid; &#125; if(a[mid]&gt;value) &#123; high = mid-1; &#125; if(a[mid]&lt;value) &#123; low = mid+1; &#125; &#125; return -1;&#125;//二分查找，递归版本public int BinarySearch2(int a[], int value, int low, int high)&#123; int mid = low+(high-low)/2; if(a[mid]==value) &#123; return mid; &#125; if(a[mid]&gt;value) &#123; return BinarySearch2(a, value, low, mid-1); &#125; if(a[mid]&lt;value) &#123; return BinarySearch2(a, value, mid+1, high); &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一、顺序查找]]></title>
    <url>%2F2016%2F06%2F24%2F2016-06-24-in-order-to-find-search%2F</url>
    <content type="text"><![CDATA[顺序查找说明：顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想：顺序查找也称为线形查找，属于无序查找算法。从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 复杂度分析：查找成功时的平均查找长度为：（假设每个数据元素的概率相等） ASL = 1/n(1+2+3+…+n) = (n+1)/2 ; 当查找不成功时，需要n+1次比较，时间复杂度为O(n);所以，顺序查找的时间复杂度为O(n)。Java代码实现12345678910//顺序查找public int SequenceSearch(int a[], int value, int n) &#123; int i; for(i=0; i&lt;n; i++) &#123; if(a[i]==value) &#123; return i; &#125; &#125; return -1;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[八大排序算法汇总]]></title>
    <url>%2F2016%2F06%2F20%2F2016-06-20-sorting-algorithm%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[八、基数排序]]></title>
    <url>%2F2016%2F06%2F18%2F2016-06-18-radix-sort%2F</url>
    <content type="text"><![CDATA[基数排序(Radix Sort或Bin Sort)1.基本思想BinSort想法非常简单，首先创建数组A[MaxValue]；然后将每个数放到相应的位置上（例如17放在下标17的数组位置）；最后遍历数组，即为排序后的结果。 2.图示①问题：当序列中存在较大值时，BinSort 的排序方法会浪费大量的空间开销。②思想：基数排序是在BinSort的基础上，通过基数的限制来减少空间的开销。 3.过程（1）首先确定基数为10，数组的长度也就是10.每个数34都会在这10个数中寻找自己的位置。（2）不同于BinSort会直接将数34放在数组的下标34处，基数排序是将34分开为3和4，第一轮排序根据最末位放在数组的下标4处，第二轮排序根据倒数第二位放在数组的下标3处，然后遍历数组即可。 4.Java代码实现1234567891011121314151617181920212223242526272829303132public static void RadixSort(int A[],int temp[],int n,int k,int r,int cnt[])&#123; //A:原数组 //temp:临时数组 //n:序列的数字个数 //k:最大的位数2 //r:基数10 //cnt:存储bin[i]的个数 for(int i=0 , rtok=1; i&lt;k ; i++ ,rtok = rtok*r)&#123; //初始化 for(int j=0;j&lt;r;j++)&#123; cnt[j] = 0; &#125; //计算每个箱子的数字个数 for(int j=0;j&lt;n;j++)&#123; cnt[(A[j]/rtok)%r]++; &#125; //cnt[j]的个数修改为前j个箱子一共有几个数字 for(int j=1;j&lt;r;j++)&#123; cnt[j] = cnt[j-1] + cnt[j]; &#125; for(int j = n-1;j&gt;=0;j--)&#123; //重点理解 cnt[(A[j]/rtok)%r]--; temp[cnt[(A[j]/rtok)%r]] = A[j]; &#125; for(int j=0;j&lt;n;j++)&#123; A[j] = temp[j]; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七、归并排序]]></title>
    <url>%2F2016%2F06%2F12%2F2016-06-12-merge-sort%2F</url>
    <content type="text"><![CDATA[归并排序(Merge Sort)1.基本思想归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法的一个非常典型的应用。首先考虑下如何将2个有序数列合并。这个非常简单，只要从比较2个数列的第一个数，谁小就先取谁，取了后就在对应数列中删除这个数。然后再进行比较，如果有数列为空，那直接将另一个数列的数据依次取出即可。1234567891011121314151617181920//将有序数组a[]和b[]合并到c[]中void MemeryArray(int a[], int n, int b[], int m, int c[])&#123; int i, j, k; i = j = k = 0; while (i &lt; n &amp;&amp; j &lt; m) &#123; if (a[i] &lt; b[j]) c[k++] = a[i++]; else c[k++] = b[j++]; &#125; while (i &lt; n) c[k++] = a[i++]; while (j &lt; m) c[k++] = b[j++];&#125; 解决了上面的合并有序数列问题，再来看归并排序，其的基本思路就是将数组分成2组A，B，如果这2组组内的数据都是有序的，那么就可以很方便的将这2组数据进行排序。如何让这2组组内数据有序了？可以将A，B组各自再分成2组。依次类推，当分出来的小组只有1个数据时，可以认为这个小组组内已经达到了有序，然后再合并相邻的2个小组就可以了。这样通过先递归的分解数列，再合并数列就完成了归并排序。 2.过程 3.平均时间复杂度 O(n㏒n)归并排序的效率是比较高的，设数列长为n，将数列分开成小数列一共要㏒n步，每步都是一个合并有序数列的过程，时间复杂度可以记为O(n)，故一共为O(n㏒n)。 4.Java代码实现123456789101112131415161718192021222324252627282930313233343536373839404142public static void merge_sort(int a[],int first,int last,int temp[])&#123; if(first &lt; last)&#123; int middle = (first + last)/2; merge_sort(a,first,middle,temp);//左半部分排好序 merge_sort(a,middle+1,last,temp);//右半部分排好序 mergeArray(a,first,middle,last,temp); //合并左右部分 &#125;&#125;//合并 ：将两个序列a[first-middle],a[middle+1-end]合并public static void mergeArray(int a[],int first,int middle,int end,int temp[])&#123; int i = first; int m = middle; int j = middle+1; int n = end; int k = 0; while(i&lt;=m &amp;&amp; j&lt;=n)&#123; if(a[i] &lt;= a[j])&#123; temp[k] = a[i]; k++; i++; &#125;else&#123; temp[k] = a[j]; k++; j++; &#125; &#125; while(i&lt;=m)&#123; temp[k] = a[i]; k++; i++; &#125; while(j&lt;=n)&#123; temp[k] = a[j]; k++; j++; &#125; for(int ii=0;ii&lt;k;ii++)&#123; a[first + ii] = temp[ii]; &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六、快速排序]]></title>
    <url>%2F2016%2F06%2F06%2F2016-06-06-quick-sort%2F</url>
    <content type="text"><![CDATA[快速排序(Quick Sort)1.基本思想 （分治）1.先从数列中取出一个数作为key值；2.将比这个数小的数全部放在它的左边，大于或等于它的数全部放在它的右边；3.对左右两个小数列重复第二步，直至各区间只有1个数。 2.辅助理解 挖坑填数1.初始时 i = 0; j = 9; key=72由于已经将a[0]中的数保存到key中，可以理解成在数组a[0]上挖了个坑，可以将其它数据填充到这来。从j开始向前找一个比key小的数。当j=8，符合条件，a[0] = a[8] ; i++ ; 将a[8]挖出再填到上一个坑a[0]中。这样一个坑a[0]就被搞定了，但又形成了一个新坑a[8]，这怎么办了？简单，再找数字来填a[8]这个坑。这次从i开始向后找一个大于key的数，当i=3，符合条件，a[8] = a[3] ; j– ; 将a[3]挖出再填到上一个坑中。数组：72 - 6 - 57 - 88 - 60 - 42 - 83 - 73 - 48 - 85 0 1 2 3 4 5 6 7 8 92.此时 i = 3; j = 7; key=72再重复上面的步骤，先从后向前找，再从前向后找。从j开始向前找，当j=5，符合条件，将a[5]挖出填到上一个坑中，a[3] = a[5]; i++;从i开始向后找，当i=5时，由于i==j退出。此时，i = j = 5，而a[5]刚好又是上次挖的坑，因此将key填入a[5]。数组：48 - 6 - 57 - 88 - 60 - 42 - 83 - 73 - 88 - 85 0 1 2 3 4 5 6 7 8 93.可以看出a[5]前面的数字都小于它，a[5]后面的数字都大于它。因此再对a[0…4]和a[6…9]这二个子区间重复上述步骤就可以了。数组：48 - 6 - 57 - 42 - 60 - 72 - 83 - 73 - 88 - 85 0 1 2 3 4 5 6 7 8 9 3.平均时间复杂度：O(n㏒n)4.Java代码实现12345678910111213141516171819202122232425262728public static void quickSort(int a[],int l,int r)&#123; if(l&gt;=r) return; int i = l; int j = r; int key = a[l];//选择第一个数为key while(i&lt;j)&#123; while(i&lt;j &amp;&amp; a[j]&gt;=key)//从右向左找第一个小于key的值 j--; if(i&lt;j)&#123; a[i] = a[j]; i++; &#125; while(i&lt;j &amp;&amp; a[i]&lt;key)//从左向右找第一个大于key的值 i++; if(i&lt;j)&#123; a[j] = a[i]; j--; &#125; &#125; //i == j a[i] = key; quickSort(a, l, i-1);//递归调用 quickSort(a, i+1, r);//递归调用&#125; key值的选取可以有多种形式，例如中间数或者随机数，分别会对算法的复杂度产生不同的影响。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五、冒泡排序]]></title>
    <url>%2F2016%2F05%2F28%2F2016-05-28-bubble-sort%2F</url>
    <content type="text"><![CDATA[冒泡排序(Bubble Sort)1.基本思想两个数比较大小，较大的数下沉，较小的数冒起来。 2.过程 比较相邻的两个数据，如果第二个数小，就交换位置。 从后向前两两比较，一直到比较最前两个数据。最终最小数被交换到起始的位置，这样第一个最小数的位置就排好了。 继续重复上述过程，依次将第2.3…n-1个最小数排好位置。3.平均时间复杂度 O(n²)4.Java代码实现1234567891011121314public static void BubbleSort(int [] arr)&#123; int temp;//临时变量 for(int i=0; i&lt;arr.length-1; i++)&#123; //表示趟数，一共arr.length-1次。 for(int j=arr.length-1; j&gt;i; j--)&#123; if(arr[j] &lt; arr[j-1])&#123; temp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = temp; &#125; &#125; &#125;&#125; 5.优化 针对问题：数据的顺序排好之后，冒泡算法仍然会继续进行下一轮的比较，直到arr.length-1次，后面的比较没有意义的。 方案：设置标志位flag，如果发生了交换flag设置为true；如果没有交换就设置为false。这样当一轮比较结束后如果flag仍为false，即：这一轮没有发生交换，说明数据的顺序已经排好，没有必要继续进行下去。 12345678910111213141516171819public static void BubbleSort1(int [] arr)&#123; int temp;//临时变量 boolean flag;//是否交换的标志 for(int i=0; i&lt;arr.length-1; i++)&#123; //表示趟数，一共arr.length-1次。 flag = false; for(int j=arr.length-1; j&gt;i; j--)&#123; if(arr[j] &lt; arr[j-1])&#123; temp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = temp; flag = true; &#125; &#125; if(!flag) break; &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四、堆排序]]></title>
    <url>%2F2016%2F05%2F23%2F2016-05-23-heap-sort%2F</url>
    <content type="text"><![CDATA[堆排序(Heap Sort)1.基本思想 ①图示： （88,85,83,73,72,60,57,48,42,6） 2.平均时间复杂度：O(n㏒n)由于每次重新恢复堆的时间复杂度为O(n㏒n)，共n - 1次重新恢复堆操作，再加上前面建立堆时n / 2次向下调整，每次调整时间复杂度也为O(n㏒n)。二次操作时间相加还是O(n㏒n)。 3.Java代码实现1234567891011121314151617181920212223242526272829303132333435363738394041//构建最小堆public static void MakeMinHeap(int a[], int n)&#123; for(int i=(n-1)/2 ; i&gt;=0 ; i--)&#123; MinHeapFixdown(a,i,n); &#125;&#125;//从i节点开始调整,n为节点总数 从0开始计算 i节点的子节点为 2*i+1, 2*i+2 public static void MinHeapFixdown(int a[],int i,int n)&#123; int j = 2*i+1; //子节点 int temp = 0; while(j&lt;n)&#123; //在左右子节点中寻找最小的 if(j+1&lt;n &amp;&amp; a[j+1]&lt;a[j])&#123; j++; &#125; if(a[i] &lt;= a[j]) break; //较大节点下移 temp = a[i]; a[i] = a[j]; a[j] = temp; i = j; j = 2*i+1; &#125;&#125;public static void MinHeap_Sort(int a[],int n)&#123; int temp = 0; MakeMinHeap(a,n); for(int i=n-1;i&gt;0;i--)&#123; temp = a[0]; a[0] = a[i]; a[i] = temp; MinHeapFixdown(a,0,i); &#125; &#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三、选择排序]]></title>
    <url>%2F2016%2F05%2F18%2F2016-05-18-selection-sort%2F</url>
    <content type="text"><![CDATA[选择排序(Selction Sort)1.基本思想在长度为N的无序数组中，第一次遍历n-1个数，找到最小的数值与第一个元素交换；第二次遍历n-2个数，找到最小的数值与第二个元素交换；。。。第n-1次遍历，找到最小的数值与第n-1个元素交换，排序完成。 2.过程 3.平均时间复杂度 O(n²)4.Java代码实现1234567891011121314151617public static void select_sort(int array[],int lenth)&#123; for(int i=0;i&lt;lenth-1;i++)&#123; int minIndex = i; for(int j=i+1;j&lt;lenth;j++)&#123; if(array[j]&lt;array[minIndex])&#123; minIndex = j; &#125; &#125; if(minIndex != i)&#123; int temp = array[i]; array[i] = array[minIndex]; array[minIndex] = temp; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二、希尔排序]]></title>
    <url>%2F2016%2F05%2F12%2F2016-05-12-shell-sort%2F</url>
    <content type="text"><![CDATA[希尔排序(Shell Sort)1.前言数据序列1： 13-17-20-42-28 利用插入排序，13-17-20-28-42. Number of swap:1;数据序列2： 13-17-20-42-14 利用插入排序，13-14-17-20-42. Number of swap:3;如果数据序列基本有序，使用插入排序会更加高效。 2.基本思想在要排序的一组数中，根据某一增量分为若干子序列，并对子序列分别进行插入排序。然后逐渐将增量减小,并重复上述过程。直至增量为1,此时数据序列基本有序,最后进行插入排序。 3.过程 4.平均时间复杂度 O(n^1.3)5.Java代码实现1234567891011121314151617181920212223242526272829public static void shellSort(int array[],int lenth)&#123; int temp = 0; int incre = lenth; while(true)&#123; incre = incre/2; for(int k = 0;k&lt;incre;k++)&#123; //根据增量分为若干子序列 for(int i=k+incre;i&lt;lenth;i+=incre)&#123; for(int j=i;j&gt;k;j-=incre)&#123; if(array[j]&lt;array[j-incre])&#123; temp = array[j-incre]; array[j-incre] = array[j]; array[j] = temp; &#125;else&#123; break; &#125; &#125; &#125; &#125; if(incre == 1)&#123; break; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一、插入排序]]></title>
    <url>%2F2016%2F05%2F04%2F2016-05-04-insertion-sort%2F</url>
    <content type="text"><![CDATA[插入排序(Insertion Sort)1.基本思想在要排序的一组数中，假定前n-1个数已经排好序，现在将第n个数插到前面的有序数列中，使得这n个数也是排好顺序的。如此反复循环，直到全部排好顺序。算法适用于少量数据的排序，时间复杂度为O(n^2)，是稳定的排序方法。 2.过程 3.平均时间复杂度 O(n²)4.Java代码实现12345678910111213141516public static void insert_sort(int array[],int lenth)&#123; int temp; for(int i=0;i&lt;lenth-1;i++)&#123; for(int j=i+1;j&gt;0;j--)&#123; if(array[j] &lt; array[j-1])&#123; temp = array[j-1]; array[j-1] = array[j]; array[j] = temp; &#125;else&#123; //不需要交换 break; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F02%2F22%2F2016-02-22-hello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Next</tag>
      </tags>
  </entry>
</search>
